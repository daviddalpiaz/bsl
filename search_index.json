[["index.html", "Basics of Statistical Learning Welcome 0.1 Who? 0.2 What? 0.3 Why? 0.4 Where? 0.5 When? 0.6 How?", " Basics of Statistical Learning David Dalpiaz Welcome Welcome to Basics of Statistical Learning! What a boring title! The title was chosen to mirror that of the University of Illinois at Urbana-Champaign course STAT 432 - Basics of Statistical Learning. That title was chosen to meet certain University course naming conventions, hence the boring title. A more appropriate title would be a broad introduction to machine learning from the perspective of a statistician who uses R1 and emphasizes practice over theory. This is more descriptive, still boring, and way too many words. Anyway, this “book” will be referred to as BSL for short. This chapter will outline the who, what, when, where, why, and how2 of this book, but not necessarily in that order. 0.1 Who? 0.1.1 Readers This book is targeted at advanced undergraduate or first year MS students in Statistics who have no prior machine learning experience. While both will be discussed in great detail, previous experience with both statistical modeling and R are assumed. In other words, this books is for students in STAT 4323. If you are reading this book but are not involved in STAT 432, we assume: a semester of calculus based probability and statistics familiarity with linear algebra enough understanding of linear models and R to be able to use R’s formula syntax to specify models 0.1.2 Author This text was written by David Dalpiaz4. 0.1.3 Acknowledgements The following is a (likely incomplete) list of helpful contributors. This book was also influenced by the helpful contributors to R4SL. Jae-Ho Lee - STAT 432, Fall 2019 W. Jonas Reger - STAT 432, Spring 2020 Please see the CONTRIBUTING document on GitHub for details on interacting with this project. Pull requests encouraged! 0.2 What? Well, this is a book. But you already knew that. More specifically, this is a book for use in STAT 432. But if you are reading this chapter, you’re either not in STAT 432, or new to STAT 432, so that isn’t really helpful. This is a book about machine learning. But this is too vague a description. It is probably most useful to describe the desired outcome as a result of reading this book. In a single sentence: After engaging with BSL, readers should feel comfortable training predictive models and evaluating their use as part of larger systems or data anlyses. This sentence is both too specific and too general, so some additional comments about what will and will not be discussed in this text: An ability to train models will be emphasized over the ability to understand models at a deep theoretical level. This is not to say that theory will be completely ignored, but some theory will be sacrificed for practicality. Theory5 will be explored especially when it motivates use in practice. Evaluation of models is emphasized as the author takes the position6 that in practice it is more important to know if your model works than how your model works. Rather than making an attempt to illustrate all possible modeling techniques7, a small set of techniques are emphasized: linear models, nearest neighbors, and decision trees. These will initially serve as examples for theory discussions, but will then become the building blocks for more complex techniques such as lasso, ridge, and random forests. While the set of models discussed will be limited8, the emphasis on an ability to train and evaluate these models should allow a reader to train and evaluate any model in a predictive context, provided it is implemented in a statistical computing environment9. For a better understanding of the specific topics covered, please see the next chapter which serves as an overview of the text. To be clear: This book is not an exhaustive treatment of machine learning. If this is your first brush with machine learning, hopefully it is not your last! 0.3 Why? Why does this book exists? That is a very good question, especially given the existence of An Introduction to Statistical Learning10, the immensely popular book11 by James, Witten, Hastie, and Tibshirani. The author of this text believes ISL is a great text12, so much so that he would suggest that any readers of BSL also read all of ISL13. Despite this, a book that was more inline with the content and goals of STAT 43214 was conceived by the author, so here we are. Why does STAT 432 exist? Short answer: to add a course on machine learning to the undergraduate Statistics curriculum at the University of Illinois. The long story is long, but two individuals deserve credit for their work in the background: Ehsan Bokhari for introducing the author to ISL and suggesting that it would make a good foundation for an undergraduate course. Jeff Douglas for actually getting the pilot version of STAT 432 off the ground15. 0.4 Where? Currently, this text is used exclusively16 for STAT 43217 at the University of Illinois at Urbana-Champaign. The text can be accessed from https://statisticallearning.org/. 0.5 When? This book was last updated on: 2021-04-19.18 0.6 How? Knowing a bit about how this book was built will help readers better interact with the text. 0.6.1 Build Tools This book is authored using Bookdown19, built using Travis-CI, and hosted via GitHub pages. Details of this setup can be found by browser the relevant GitHub repository.20 Users that are familiar with these tools, most importantly GitHub, are encouraged to contribute. As noted above, please see the CONTRIBUTING document on GitHub for details on interacting with this project. 0.6.2 Active Development This “book” is under active development. Literally every element of the book is subject to change, at any moment. This text, BSL, is the successor to R4SL, an unfinished work that began as a supplement to Introduction to Statistical Learning, but was never finished. (In some sense, this book is just a fresh start due to the author wanting to change the presentation of the material. The author is seriously worried that he will encounter the second-system effect.21 Because this book is written with a course in mind, that is actively being taught, often out of convenience the text will speak directly to the students of that course. Thus, be aware that any references to a “course” are a reference to STAT 432 @ UIUC. Since this book is under active development you may encounter errors ranging from typos, to broken code, to poorly explained topics. If you do, please let us know! Better yet, fix the issue yourself!22 If you are familiar with R Markdown and GitHub, pull requests are highly encouraged!. This process is partially automated by the edit button in the top-left corner of the html version. If your suggestion or fix becomes part of the book, you will be added to the acknowledgments in this chapter this chapter. We’ll also link to your GitHub account, or personal website upon request. If you’re not familiar with version control systems feel free to email the author, dalpiaz2 AT illinois DOT edu.23 See additional details in the Acknowledgments section above. While development is taking place, you may see “TODO” items scattered throughout the text. These are mostly notes for internal use, but give the reader some idea of what development is still to come. 0.6.3 Packages The following will install all R packages needed to follow along with the text. install.packages( c( &quot;tidyverse&quot;, &quot;kableExtra&quot;, &quot;GGally&quot;, &quot;ISLR&quot;, &quot;klaR&quot;, &quot;rpart&quot;, &quot;rpart.plot&quot;, &quot;caret&quot;, &quot;ellipse&quot;, &quot;gbm&quot;, &quot;cvms&quot;, &quot;palmerpenguins&quot;, &quot;glmnet&quot;, &quot;broom&quot; ) ) remotes::install_github(&quot;coatless/ucidata&quot;) 0.6.4 License This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License R Core Team, R: A Language and Environment for Statistical Computing (Vienna, Austria: R Foundation for Statistical Computing, 2016), https://www.R-project.org/↩︎ Wikipedia: Five Ws↩︎ STAT 432 is also cross-listed as ASRM 451, but we will exclusively refer to STAT 432 for simplicity.↩︎ He does not enjoy writing about himself↩︎ Theory here is ill defined. Loosely, “theory” is activities that are closer to writing theorem-proof mathematics while “practice” is more akin to using built-in statistical computing tools in a language like R.↩︎ This is not a unique opinion.↩︎ This is impossible.↩︎ Students often ask if we will cover support vector machines or deep learning or insert latest buzzword model here. The author believes this is because students consider these to be “cool” methods. One of the goals of this text is to make machine learning seems as uncool as possible. The hope would be for readers to understand something like an SVM to be “just another method” which also needs to be evaluated. The author believes deep learning is useful, but would clutter the presentation because of the additional background and computing that would need to be introduced. Follow-up learning of deep learning is encouraged after reading BSL. Hopefully, by reading BSL, getting up to speed using deep learning will be made easier.↩︎ Also provided the user reads the documentation.↩︎ Gareth James et al., An Introduction to Statistical Learning, vol. 112 (Springer, 2013), http://faculty.marshall.usc.edu/gareth-james/ISL/↩︎ This book is generally referred to as ISL.↩︎ He has spent so much time referencing ISL that he found and suggested a typo fix.↩︎ One of the biggest strengths of ISL is its readability.↩︎ The biggest differences are: Assumed reader background, overall structure, and R code usage and style.↩︎ Jeff taught the first proto-version of STAT 432 as a topics course, but then allowed the author to take over teaching and development while he worked to get the course fully approved.↩︎ If you are using this text elsewhere, that’s great! Please let the author know!↩︎ https://stat432.org/↩︎ The author has no idea what else to write in this section, but the last updated date seems like useful information.↩︎ Yihui Xie, Bookdown: Authoring Books and Technical Documents with R Markdown, 2020, https://github.com/rstudio/bookdown↩︎ https://github.com/daviddalpiaz/bsl↩︎ Wikipedia: Second-System Effect↩︎ Yihui Xie: You Do Not Need to Tell Me I Have A Typo in My Documentation↩︎ But also consider using this opportunity to learn a bit about version control!↩︎ "],["ml-overview.html", "Chapter 1 Machine Learning Overview 1.1 What is Machine Learning? 1.2 Machine Learning Tasks 1.3 Open Questions", " Chapter 1 Machine Learning Overview This is a book about machine learning, so let’s try to define machine learning in this chapter. Specifically, we’ll discuss: What is machine learning? The difference between supervised learning and unsupervised learning. The difference between classification and regression. This chapter is currently under construction. While it is being developed, the following links to the STAT 432 course notes. Notes: Machine Learning Overview 1.1 What is Machine Learning? Machine learning (ML) is about learning functions from data.24 That’s it. Really. Pretty boring, right?25 To quickly address some buzzwords that come up when discussing machine learning: Deep learning is a subset of machine learning. Artificial Intelligence (AI) overlaps machine learning but has much loftier goals. In general, if someone claims to be using AI, they are not. They’re probably using function learning! For example, we will learn logistic regression in this course. People in marketing might call that AI! Someone who understands ML will simply call it function learning. Don’t buy the hype! We don’t need to call simple methods AI to make them effective.26 Machine learning is not data science. Data science sometimes uses machine learning. Does big data exist? If it does, I would bet a lot of money that you haven’t seen it, and probably won’t see it that often. Analytics is just a fancy word for doing data analysis. Machine learning can be used in analyses! When it is, it is often called “Predictive Analytics.” What makes machine learning interesting are the uses of these learned functions. We could develop functions that have applications in a wide variety of fields. In medicine, we could develop a function that helps detect skin cancer. Input: Pixels from an image of mole Output: A probability that the mole is cancerous In sport analytics, we could develop a function that helps determine player salary. Input: Lifetime statistics of an NBA player Output: An estimate of player’s salary In meteorology, we could develop a function to predict the weather. Input: Historic weather data in Champaign, IL Output: A probability of rain tomorrow in Champaign, IL In political science we could develop a function that predicts the mood of the president. Input: The text of a tweet from President Donald Trump Output: A prediction of Donald’s mood (happy, sad, angry, etc) In urban planning we could develop a function that predicts the rental prices of Airbnbs. Input: The attributes of the location for rent Output: An estimate of the rent of the property How do we learn these functions? By looking at many previous examples, that is, data! Again, we will learn functions from data. That’s what we’re here to do. 1.2 Machine Learning Tasks When doing machine learning, we will classify our tasks into one of two categories, supervised or unsupervised learning.27 Within these two broad categories of ML tasks, we will define some specifics. 1.2.1 Supervised Learning In supervised learning, we want to “predict” a specific response variable. (The response variable might also be called the target or outcome variable.) In the following examples, this is the y variable. Supervised learning tasks are called regression if the response variable is numeric. If a supervised learning tasks has a categorical response, it is called classification. 1.2.1.1 Regression In the regression task, we want to predict numeric response variables. The non-response variables, which we will call the feature variables, or simply features can be either categorical or numeric.28 x1 x2 x3 y A -0.66 0.48 14.09 A 1.55 0.97 2.92 A -1.19 -0.81 15.00 A 0.15 0.28 9.29 B -1.09 -0.16 17.57 B 1.61 1.94 2.12 B 0.04 1.72 8.92 A 1.31 0.36 4.40 C 0.98 0.30 4.40 C 0.88 -0.39 4.52 With the data above, our goal would be to learn a function that takes as input values of the three features (x1, x2, and x3) and returns a prediction (best guess) for the true (but usually unknown) value of the response y. For example, we could obtain some “new” data that does not contain the response. x1 x2 x3 B -0.85 -2.41 We would then pass this data to our function, which would return a prediction of the value of y. Stated mathematically, our prediction will often be an estimate of the conditional mean of \\(Y\\), given values of the \\(\\boldsymbol{X}\\) variables. \\[ \\mu(\\boldsymbol{x}) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] \\] In other words, we want to learn this function, \\(\\mu(\\boldsymbol{x})\\). Much more on this later.29 1.2.1.2 Classification Classification is similar to regression, except it considers categorical response variables. x1 x2 x3 y Q 0.46 5.42 B Q 0.72 0.83 C Q 0.93 5.93 B Q 0.26 5.68 A P 0.46 0.49 B P 0.94 3.09 B P 0.98 2.34 C P 0.12 5.43 C Q 0.47 2.68 B P 0.56 5.02 B As before we want to learn a function from this data using the same inputs, except this time, we want it to output one of A, B, or C for predictions of the y variable. Again, consider some new data: x1 x2 x3 P 0.96 5.33 While ultimately we would like our function to return one of A, B, or C, what we actually would like is an intermediate return of probabilities that y is A, B, or C. In other words, we are attempting to estimate the conditional probability that \\(Y\\) is each of the possible categories, given values of the \\(\\boldsymbol{X}\\) values. \\[ p_k(\\boldsymbol{x}) = P\\left[ Y = k \\mid \\boldsymbol{X} = \\boldsymbol{x} \\right] \\] We want to learn this function, \\(p_k(\\boldsymbol{x})\\). Much more on this later. 1.2.2 Unsupervised Learning Unsupervised learning is a very broad task that is rather difficult to define. Essentially, it is learning without a response variable. To get a better idea about what unsupervised learning is, consider some specific tasks. x1 x2 x3 x4 x5 2.74 0.46 5.42 4.43 2.28 2.81 0.72 0.83 4.87 2.61 0.86 0.93 5.93 2.33 0.22 2.49 0.26 5.68 4.11 5.84 1.93 0.46 0.49 0.02 2.59 -8.44 -9.06 -6.91 -5.00 -4.25 -7.79 -9.02 -7.66 -9.96 -4.67 -9.60 -9.88 -4.57 -8.75 -6.16 -8.03 -9.53 -7.32 -4.56 -4.17 -7.88 -9.44 -4.98 -6.33 -6.29 1.2.2.1 Clustering Clustering is essentially the task of grouping the observations of a dataset. In the above data, can you see an obvious grouping? (Hint: Compare the first five observations to the second five observations.) In general, we try to group observations that are similar. 1.2.2.2 Density Estimation Density estimation tries to do exactly what the name implies, estimate the density. In this case, the joint density of \\(X_1, X_2, X_3, X_4, X_5\\). In other words, we would like to learn the function that generated this data.30 1.2.2.3 Outlier Detection Consider some new data: x1 x2 x3 x4 x5 67 66.68 66.26 69.49 70 Was this data generated by the same process as the data above? If we don’t believe so, we would call it an outlier. 1.3 Open Questions The two previous sections were probably more confusing than helpful. But of course, because we haven’t started learning yet! Hopefully, you are currently pondering one very specific question: How do we learn functions from data? That’s what this text will be about! We will spend a lot of time on this question. It is what us statisticians call fitting a model. On some level the answer is: look at a bunch of old data before predicting on new data. While we will dedicate a large amount of time to answering this question, sometimes, some of the details might go unanswered. Since this is an introductory text, we can only go so far. However, as long as we answer another question, this will be OK. How do we evaluate how well learned functions work? This text places a high priority on being able to do machine learning, specifically do machine learning in R. You can actually do a lot of machine learning without fully understanding how the learning is taking place. That makes the evaluation of ML models extremely important. This is a purposefully narrow view of machine learning. Obviously there’s a lot more to ML, but the author believes this statement will help readers understand that the methods learned in this text are simply tools that must be evaluated as a part of a larger analysis.↩︎ An alternative title of this book could be: ML is boring but useful.↩︎ There are certainly people who do legitamtely work on AI, but the strong statement is made here to try to downplay the hype.↩︎ There are technically other tasks such as reinforcement learning and semi-supervised learning, but they are outside the scope of this text. To understand these advanced tasks, you should first learn the basics!↩︎ These variables are often called “predictor” variables, but we find this nomenclature to be needlessly confusing.↩︎ For now, just understand that we are able to make a “best guess” for a new observation.↩︎ You could take the position that this is the only machine learning task, and all other tasks are subset of this task. We’ll hold off on explaining this for a while.↩︎ "],["linear-regression.html", "Chapter 2 Linear Regression 2.1 R Setup and Source 2.2 Explanation versus Prediction 2.3 Task Setup 2.4 Mathematical Setup 2.5 Linear Regression Models 2.6 Using lm() 2.7 The predict() Function 2.8 Data Splitting 2.9 Regression Metrics 2.10 Example: “Simple” Simulated Data 2.11 Example: Diamonds Data 2.12 Example: Credit Card Data", " Chapter 2 Linear Regression This chapter will discuss linear regression models, but for a very specific purpose: using linear regression models to make predictions. Viewed this way, linear regression will be our first example of a supervised learning algorithm. Specifically, we will discuss: The regression function and estimating conditional means. Using the lm() and predict() functions in R. Data splits used in evaluation of model performance for machine learning tasks. Metrics for evaluating models used for the regression task. This chapter will be the most action packed as we will establish a framework that will be recycled throughout the rest of the text. This chapter is currently under construction. While it is being developed, the following links to the STAT 432 course notes. Notes: Linear Regression 2.1 R Setup and Source library(tibble) # data frame printing library(dplyr) # data manipulation library(knitr) # creating tables library(kableExtra) # styling tables Additionally, objects from ggplot2, GGally, and ISLR are accessed. Recall that the Welcome chapter contains directions for installing all necessary packages for following along with the text. The R Markdown source is provided as some code, mostly for creating plots, has been suppressed from the rendered document that you are currently reading. R Markdown Source: linear-regression.Rmd 2.2 Explanation versus Prediction Before we even begin to discuss regression, we make a strong declaration: this is not a text about general statistical inference. We will focus our efforts on a narrow sub-goal of inference: making predictions. We will only make a passing attempt to explain why our models make the predictions they do and it is very possible that there will be zero causal claims in this book. While it would certainly be nice (but extremely difficult) to uncover explanations for predictions or causal relationships, our focus will be on finding predictive relationships and checking their performance so as not to clutter the presentation. Suppose (although it is likely untrue) that there is a strong correlation between wearing a wrist watch and car accidents. That is, we can see in some data that car drivers who wear wrist watches get into more traffic accidents. Also, assume that it is the case that wrist watches actually do not cause accidents, which seems like a reasonable assumption. There is only a correlation, but this is the result of confounding variables.31 Depending on your frame of reference, you should view this information in very different ways. Suppose you are a car insurance company. This is great news! You can now more accurately predict the number of accidents of your policy holders if you know whether or not your policy holders wear a wrist watch. For the sake of understanding how much your company will need to pay out in a year, you don’t care what causes accidents, you just want to be able to predict (estimate) the number of accidents. Suppose you are a car driver. As a driver, you want to stay safe. That is, you want to do things that decrease accidents. In this framing, you care about things that cause accidents, not things that predict accidents. In other words, this correlation information should not lead to you throwing away your wrist watch. Disclaimer: Extremely high correlation should not simply be ignored. For example, there is a very high correlation between smoking and lung cancer.32 However, this strong correlation is not proof that smoking causes lung cancer. Instead, additional study is needed to rule out confounders, establish mechanistic relationships, and more. 2.3 Task Setup We now introduce the regression task. Regression is a subset of a broader machine learning tasks called supervised learning, which also include classification.33 Stated simply, the regression tasks seeks to estimate (predict) a numeric quantity. For example: Estimating the salary of a baseball player given statistics about their previous year’s performance. Estimating the price of a home for sale given the attributes of the home such as square footage, location, and number of bathrooms. Estimating the credit score of a bank customer, given demographic information and recent transaction history. Estimating the number of downloads of a podcast episode given its length, genre, and time of day released. Each of these quantities is some numeric value. The goal of the regression task is to estimate (predict) these quantities when they are unknown through the use of additional, possibly correlated quantities, for example the offensive and defensive statistics of a baseball player, or the location and attributes of a home. 2.4 Mathematical Setup To get a better grasp of what regression is, we move to defining the task mathematically. Consider a random variable \\(Y\\) which represents a response (or outcome or target) variable, and \\(p\\) feature variables \\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\).34 In the most common regression setup, we assume that the response variable \\(Y\\) is some function of the features, plus some random noise. \\[ Y = f(\\boldsymbol{X}) + \\epsilon \\] We call \\(f(\\boldsymbol{X})\\) the signal. This \\(f\\) is the function that we would like to learn. We call \\(\\epsilon\\) the noise. We do not want to learn this, which we risk doing if we overfit. (More on this later.) So our goal will be to find some \\(f\\) such that \\(f(\\boldsymbol{X})\\) is close to \\(Y\\). But how do we define close? There are many ways but we will start with, and most often consider, squared error loss. Specifically, we define a loss function, \\[ L(Y, f(\\boldsymbol{X})) \\triangleq \\left(Y - f(\\boldsymbol{X})\\right) ^ 2 \\] Now we can clarify the goal of regression, which is to minimize the above loss, on average. We call this the risk of estimating \\(Y\\) using \\(f(\\boldsymbol{X})\\). \\[ R(Y, f(\\boldsymbol{X})) \\triangleq \\mathbb{E}[L(Y, f(\\boldsymbol{X}))] = \\mathbb{E}_{\\boldsymbol{X}, Y}[(Y - f(\\boldsymbol{X})) ^ 2] \\] Before attempting to minimize the risk, we first re-write the risk after conditioning on \\(\\boldsymbol{X}\\). \\[ \\mathbb{E}_{\\boldsymbol{X}, Y} \\left[ (Y - f(\\boldsymbol{X})) ^ 2 \\right] = \\mathbb{E}_{\\boldsymbol{X}} \\mathbb{E}_{Y \\mid \\boldsymbol{X}} \\left[ ( Y - f(\\boldsymbol{X}) ) ^ 2 \\mid \\boldsymbol{X} = \\boldsymbol{x} \\right] \\] Minimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to \\(Y \\mid \\boldsymbol{X}\\), essentially minimizing the risk pointwise, for each \\(\\boldsymbol{x}\\). It turns out, that the risk is minimized by the conditional mean of \\(Y\\) given \\(\\boldsymbol{X}\\), \\[ \\mu(\\boldsymbol{x}) \\triangleq \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] \\] which we call the regression function.35 This is not a “learned” function. This is the function we would like to learn in order to minimize the squared error loss on average. \\(f\\) is any function, \\(\\mu\\) is the function that would minimize squared error loss on average if we knew it, but we will need to learn it form the data. Note that \\(\\boldsymbol{x}\\) represents (potential) realized values of the random variables \\(\\boldsymbol{X}\\). \\[ \\boldsymbol{x} = (x_1, x_2, \\ldots, x_p) \\] We can now state the goal of the regression task: we want to estimate the regression function. How do we do that? 2.5 Linear Regression Models What do linear regression models do? They estimate the conditional mean of \\(Y\\) given \\(\\boldsymbol{X}\\)!36 Consider the following probability model \\[ Y = 1 - 2x - 3x ^ 2 + 5x ^ 3 + \\epsilon \\] where \\(\\epsilon \\sim \\text{N}(0, \\sigma^2)\\). Alternatively we could write \\[ Y \\mid X \\sim \\text{N}(1 - 2x - 3x ^ 2 + 5x ^ 3, \\sigma^2) \\] This perhaps makes it clearer that \\[ \\mu(x) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] = 1 - 2x - 3x ^ 2 + 5x ^ 3 \\] What do linear models do? More specifically than before, linear regression models estimate the conditional mean of \\(Y\\) given \\(\\boldsymbol{X}\\) by assuming this conditional mean is a linear combination of the feature variables. Suppose for a moment that we did not know the above true probability model, or even more specifically the regression function. Instead, all we had was some data, \\((x_i, y_i)\\) for \\(i = 1, 2, \\ldots, n\\). x y -0.47 -0.06 -0.26 1.72 0.15 1.39 0.82 0.68 -0.60 -0.27 0.80 1.55 0.89 0.76 0.32 -0.40 0.26 -1.85 -0.88 -1.85 How do we fit (or “train” in ML language) a linear model with this data? In order words, how to be learn the regression function from this data with a linear regression model? First, we need to make assumptions about the form of the regression function, up to, but not including some unknown parameters. Consider three possible linear models, in particular, three possible regression functions. Degree 1 Polynomial \\[ \\mu(x) = \\beta_0 + \\beta_1 x \\] Degree 3 Polynomial \\[ \\mu(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] Degree 9 Polynomial \\[ \\mu(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\ldots + \\beta_9 x^9 \\] These are chosen mostly arbitrarily for illustrative purposes which we’ll see in a moment. So how do we actually fit these models, that is train them, with the given data. We have a couple of options: Maximum Likelihood or Least Squares! In this case, they actually produce the same result, so we use least squares for simplicity of explanation. To fit the degree 3 polynomial using least squares, we minimize \\[ \\sum_{i = 1}^{n}\\left(y_i - (\\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3)\\right) ^ 2 \\] Skipping the details of the minimization, we would acquire \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), \\(\\hat{\\beta}_2\\), and \\(\\hat{\\beta}_3\\) which are estimates of \\({\\beta}_0\\), \\({\\beta}_1\\), \\({\\beta}_2\\), and \\({\\beta}_3\\). Taken together, we would have \\[ \\hat{\\mu}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2^2 + \\hat{\\beta}_3 x_3^3 \\] which is then an estimate of \\(\\mu(x)\\). While in this case, it will almost certainly not be the case that \\(\\hat{\\beta}_0 = 1\\) or \\(\\hat{\\beta}_1 = -2\\) or \\(\\hat{\\beta}_2 = -3\\) or \\(\\hat{\\beta}_3 = 5\\), which are the true values of the \\(\\beta\\) coefficients, they are at least reasonable estimates. As a bit of an aside, note that in this case, it is sort of ambiguous as to whether there is one feature, \\(x\\), which is seen in the data, or three features \\(x\\), \\(x^2\\), and \\(x^3\\), which are seen in the model. The truth is sort of in the middle. The data has a single feature, but through feature engineering, we have created two additional features for fitting the model. Note that when using R, you do not need to modify the data to do this, instead you should use R’s formula syntax to specify this feature engineering when fitting the model. More on this when we discuss the lm() function in R. We introduce this somewhat confusing notion early so we can emphasize that linear models are about linear combinations of features, not necessarily linear relationships. Although, linear models are very good at learning linear relationships. Suppose instead we had assumed that \\[ \\mu(x) = \\beta_0 + \\beta_1 x \\] This model is obviously flawed as it doesn’t contain enough terms to capture the true regression function. (Later we will say this model is not “flexible” enough.) Or, suppose we had assumed \\[ \\mu(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\ldots + \\beta_9 x^9 \\] This model is also flawed, but for a different reason. (Later we will say this model is too “flexible.”) After using least squares, we will obtain some \\(\\hat{\\beta}_9\\) even though there is not a 9th degree term in the true regression function! Let’s take a look at this visually. Here we see the three models fit to the data above. The dashed black curve is the true mean function, that is the true mean of \\(Y\\) given \\(x\\), and the solid colored curves are the estimated mean functions. Now we ask the question: which of these models is best? Given these pictures, there are two criteria that we could consider. How close is the estimated regression (mean) function to the data? (Degree 9 is best! There is no error!) How close is the estimated regression (mean) function to the true regression (mean) function? (Degree 3 is best.) From the presentation here, it’s probably clear that the latter is actually what matters. We can demonstrate this by generating some “new” data. These plots match the plots above, except newly simulated data is shown. (The regression functions were still estimated with the original data.) Note that the degree 3 polynomial matches the data about the same as before. The degree 9 polynomial now correctly predicts none of the new data and makes some huge errors. We will define these concepts more generally later, but for now we note that: The Degree 9 Polynomial is overfitting. It performs well on the data used to fit the model, but poorly on new data. The Degree 1 Polynomial is underfitting. It performs poorly on the data used to fit the model and poorly on new data. There’s a bit of a problem though! In practice, we don’t know the true mean function, and we don’t have the magical ability to simulate new data! Yikes! After we discuss a bit about how to fit these models in R, we’ll return to this issue.37 2.6 Using lm() Before we continue, let’s consider a different data generating process. We first define this data generating process as an R function. gen_mlr_data = function(sample_size = 250) { x1 = round(runif(n = sample_size), 2) x2 = round(runif(n = sample_size), 2) x3 = round(runif(n = sample_size), 2) x4 = factor(sample(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), size = sample_size, replace = TRUE)) x5 = round(runif(n = sample_size), 2) x6 = round(runif(n = sample_size), 2) y = 2 + x1 + sin(x2) + 3 * x3 ^ 2 + 3 * (x4 == &quot;B&quot;) - 2 * (x4 == &quot;C&quot;) + rnorm(n = sample_size, mean = 0, sd = 0.5) tibble(y, x1, x2, x3, x4, x5, x6) } We then run the function and store the data that is returned. set.seed(42) sim_mlr_data = gen_mlr_data() We then inspect the data. head(sim_mlr_data) ## # A tibble: 6 x 7 ## y x1 x2 x3 x4 x5 x6 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.85 0.91 0.33 0.14 A 0.53 0.24 ## 2 6.22 0.94 0.19 0.18 B 0.7 0.51 ## 3 6.71 0.29 0.27 0.52 B 0.05 0.51 ## 4 7.84 0.83 0.53 0.81 B 0.92 0.76 ## 5 2.75 0.64 0.02 0.12 A 0.03 0.27 ## 6 4.60 0.52 0.8 0.89 A 0.78 0.69 Note that we see only numeric (dbl or int) and factor (fctr) variables. For now, we will require that data contains only these types, and in particular, we will coerce any categorical variables to be factors. Mathematically, this data was generated from the probability model \\[ Y \\mid \\boldsymbol{X} \\sim \\text{N}(2 + 1\\cdot x_1 + 1 \\cdot \\sin(x_2) + 3 \\cdot x_3^3 + 3 \\cdot x_{4B} -2 \\cdot x_{4C}, \\sigma^2 = 0.25) \\] where \\(x_{4B}\\) is a dummy variable which takes the value 1 when \\(x_4 = \\text{B}\\) and 0 otherwise \\(x_{4C}\\) is a dummy variable which takes the value 1 when \\(x_4 = \\text{C}\\) and 0 otherwise In particular, the true mean function is \\[ \\mu(\\boldsymbol{x}) = 2 + 1\\cdot x_1 + 1 \\cdot \\sin(x_2) + 3 \\cdot x_3^3 + 3 \\cdot x_{4B} -2 \\cdot x_{4C} \\] Now, finally, let’s fit some models it R to this data! To do so, we will use one of the most important functions in R, the lm() function. Let’s specify the form of some assumed mean functions of models that we would like to fit. Model 1 or mod_1 in R \\[ \\mu_1(\\boldsymbol{x}) = \\beta_0 + \\beta_1 x_1 \\] Model 2 or mod_2 in R \\[ \\mu_2(\\boldsymbol{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] Model 3 or mod_3 in R \\[ \\mu_3(\\boldsymbol{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_{4B} x_{4B} +\\beta_{4C} x_{4C} + \\beta_5 x_5 + \\beta_6 x_6 \\] Model 4 or mod_4 in R \\[ \\mu_4(\\boldsymbol{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 \\sin(x_2) + \\beta_3 x_3^3 + \\beta_{4B} x_{4B} + \\beta_{4C} x_{4C} \\] Now, finally, R! mod_1 = lm(y ~ x1, data = sim_mlr_data) coef(mod_1) ## (Intercept) x1 ## 3.7834423 0.9530758 Nothing too interesting here about fitting Model 1. We see that the coef() function returns estimates of the \\(\\beta_0\\) and \\(\\beta_1\\) parameters defined above. mod_2 = lm(y ~ x1 + x2, data = sim_mlr_data) coef(mod_2) ## (Intercept) x1 x2 ## 3.8747999 0.9400654 -0.1802538 Again, Model 2 isn’t too interesting. We see that the coef() function returns estimate of the \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\) parameters defined above. mod_3 = lm(y ~ ., data = sim_mlr_data) coef(mod_3) ## (Intercept) x1 x2 x3 x4B x4C ## 1.71015079 0.76017877 0.77637360 3.00479841 3.06812204 -1.93068734 ## x5 x6 ## -0.12248770 -0.04797294 Now, Model 3, we see a couple interesting things. First, the formula syntax y ~ . fits a model with y as the response, and all other variables in the sim_mlr_data data frame (tibble) as features. Also note: we did not manually create the needed dummy variables! R did this for us! levels(sim_mlr_data$x4) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; Because x4 is a factor variable, R uses the first level, A, as the reference level, and then creates dummy variables for the remaining levels. Cool! mod_4 = lm(y ~ x1 + I(sin(x2)) + I(x3 ^ 3) + x4, data = sim_mlr_data) coef(mod_4) ## (Intercept) x1 I(sin(x2)) I(x3^3) x4B x4C ## 2.3435702 0.8176247 0.9159963 3.0446314 3.0369950 -1.9421931 Our last model, mod_4 is the most interesting. It makes use of the inhibit function, I(). This allows for on-the-fly feature engineering based on available features. We’re creating new features via R’s formula syntax as we fit the model. To see why this is necessary, consider the following: lm(y ~ (x1 + x2) ^ 2, data = sim_mlr_data) ## ## Call: ## lm(formula = y ~ (x1 + x2)^2, data = sim_mlr_data) ## ## Coefficients: ## (Intercept) x1 x2 x1:x2 ## 4.1800 0.3353 -0.8259 1.3130 This created an interaction term! That means the ^ operator has different uses depending on the context. In specifying a formula, it has a particular use, in this case specifying an interaction term, and all lower order terms. However, inside of I() it will be used for exponentiation. For details, use ?I and ?formula. These are complex R topics, but it will help to start to learn them.38 For the first half of this book, we will always keep the data mostly untouched, and rely heavily on the use of R’s formula syntax. If you are ever interested in what’s happening under the hood when you use the formula syntax, and you recall the linear algebra necessary to perform linear regression, the model.matrix() function will be useful. head(model.matrix(mod_4)) ## (Intercept) x1 I(sin(x2)) I(x3^3) x4B x4C ## 1 1 0.91 0.32404303 0.002744 0 0 ## 2 1 0.94 0.18885889 0.005832 1 0 ## 3 1 0.29 0.26673144 0.140608 1 0 ## 4 1 0.83 0.50553334 0.531441 1 0 ## 5 1 0.64 0.01999867 0.001728 0 0 ## 6 1 0.52 0.71735609 0.704969 0 0 X = model.matrix(mod_4) y = sim_mlr_data$y solve((t(X) %*% X)) %*% t(X) %*% y ## [,1] ## (Intercept) 2.3435702 ## x1 0.8176247 ## I(sin(x2)) 0.9159963 ## I(x3^3) 3.0446314 ## x4B 3.0369950 ## x4C -1.9421931 Back to talking about mod_4. Recall that we had assumed that \\[ \\mu_4(\\boldsymbol{x}) = \\beta_0 + \\beta_1 x_1 + \\beta_2 \\sin(x_2) + \\beta_3 x_3^3 + \\beta_{4B} x_{4B} + \\beta_{4C} x_{4C} \\] Also recall that the true mean function is \\[ \\mu(\\boldsymbol{x}) = 2 + 1\\cdot x_1 + 1 \\cdot \\sin(x_2) + 3 \\cdot x_3^3 + 3 \\cdot x_{4B} -2 \\cdot x_{4C} \\] Because we know this, we can investigate how well our model is performing. We know the true values of the parameters, in this case \\(\\beta_0 = 2\\) \\(\\beta_1 = 1\\) \\(\\beta_2 = 1\\) \\(\\beta_3 = 3\\) \\(\\beta_{4B} = 3\\) \\(\\beta_{4C} = -2\\) \\(\\beta_5 = 0\\) (\\(x_5\\) is not used in the true mean function.) \\(\\beta_6 = 0\\) (\\(x_6\\) is not used in the true mean function.) We also have the estimated coefficients from mod_4. coef(mod_4) ## (Intercept) x1 I(sin(x2)) I(x3^3) x4B x4C ## 2.3435702 0.8176247 0.9159963 3.0446314 3.0369950 -1.9421931 \\(\\hat{\\beta}_0 = 2.344\\) \\(\\hat{\\beta}_1 = 0.818\\) \\(\\hat{\\beta}_2 = 0.916\\) \\(\\hat{\\beta}_3 = 3.045\\) \\(\\hat{\\beta}_{4B} = 3.037\\) \\(\\hat{\\beta}_{4C} = -1.942\\) \\(\\hat{\\beta}_5 = 0\\) (We assumed \\(x_5\\) is not used in the true mean function.) \\(\\hat{\\beta}_6 = 0\\) (We assumed \\(x_6\\) is not used in the true mean function.) Our estimated regression (mean) function is then \\[ \\hat{\\mu}_4(\\boldsymbol{x}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 \\sin(x_2) + \\hat{\\beta}_3 x_3^3 + \\hat{\\beta}_{4B} x_{4B} + \\hat{\\beta}_{4C} x_{4C} \\] Perfect? No. Pretty good? Maybe. However, in reality, this is not a check that we can perform! We still need an evaluation strategy that doesn’t depend on knowing the true model! Note that the other models are “bad” in this case because they are either missing features (mod_1 and mod_2) or they are both missing features and contain unnecessary features (mod_3). 2.7 The predict() Function We stated previously that fitting a linear regression model means that we are learning the regression (mean) function. Now that we fit and stored some models, how do we access these estimated regression (mean) functions? The predict() function! The predict() function will be the workhorse of STAT 432. Let’s see how to use it with models fit using the lm() function. set.seed(3) new_obs = gen_mlr_data(sample_size = 1) new_obs ## # A tibble: 1 x 7 ## y x1 x2 x3 x4 x5 x6 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.751 0.17 0.81 0.38 C 0.6 0.6 Suppose we wanted to estimate the mean of \\(Y\\) when \\(x_1 = 0.17\\) \\(x_2 = 0.81\\) \\(x_3 = 0.38\\) \\(x_4 = \\text{C}\\) \\(x_5 = 0.38\\) \\(x_6 = 0.38\\) In other words, we want to estimate \\[ \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\texttt{new_obs}] = \\mathbb{E}[Y \\mid X_1 = 0.17, X_2 = 0.81, X_3 = 0.38, X_4 = \\text{C}, X_5 = 0.6, X_6 = 0.6] \\] The predict() function to the rescue! predict(mod_1, new_obs) ## 1 ## 3.945465 What’s being returned here? \\[ \\hat{\\mu}_1(\\texttt{new_obs}) = \\hat{\\mathbb{E}}[Y \\mid \\boldsymbol{X} = \\texttt{new_obs}] = 3.9454652 \\] The predict function, together with a trained model, is the estimated regression (mean) function! Supply a different trained model, then you get that estimated regression (mean) function. predict(mod_4, new_obs) ## 1 ## 1.370883 What’s being returned here? \\[ \\hat{\\mu}_4(\\texttt{new_obs}) = \\hat{\\mathbb{E}}[Y \\mid \\boldsymbol{X} = \\texttt{new_obs}] = 1.3708827 \\] We could compare these two estimates of the conditional mean of \\(Y\\) to the true value of y observed in the observation. More on that in the next section. If given an entire dataset, instead of a single observation, predict() returns the estimated conditional mean of each observation. set.seed(9) some_more_data = gen_mlr_data(sample_size = 10) predict(mod_4, some_more_data) ## 1 2 3 4 5 6 7 8 ## 7.8896349 5.4061018 1.3788387 0.8560024 6.6246872 8.2203544 3.2140060 3.5738889 ## 9 10 ## 5.9928135 8.4908895 Neat! A warning: Do not name the second argument to the predict function. This will cause issues because sometimes the name of that argument is newdata, as it is here, but sometimes it is data. If you use the wrong name, bad things will happen. It is safer to simply never name this argument. (However, in general, arguments after the first should be named. The predict() function is the exception.) 2.8 Data Splitting Note: Many readers will have possibly seen some machine learning previously. For now, please pretend that you have never heard of or seen cross-validation. Cross-validation will clutter the initial introduction of many concepts. We will return to and formalize it later. OK. So now we can fit models, and make predictions (create estimates of the conditional mean of \\(Y\\) given values of the features), how do we evaluate how well our models perform, without knowing the true model? First, let’s state a somewhat specific goal. We would like to train models that generalize well, that is, perform well on “new” or “unseen” data that was not used to train the model.39 To accomplish this goal, we’ll just “create” a dataset that isn’t used to train the model! To create it, we will just split it off. (We’ll actually do so twice.) First, denote the entire available data as \\(\\mathcal{D}\\) which contains \\(n\\) observations of the response variable \\(y\\) and \\(p\\) feature variables \\(\\boldsymbol{x}_i = (x_{i1}, x_{i2}, \\ldots, x_{ip})\\). \\[ \\mathcal{D} = \\{ (\\boldsymbol{x}_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i = 1, 2, \\ldots n \\} \\] We first split this data into a train and test set. We will discuss these two datasets ad nauseam, but let’s set two rules right now.40 You can do whatever you would like with the training data. However, it is best used to train, evaluate, and select models. Do not, ever, for any reason, fit a model using test data! Additionally, you should not select models using test data. In STAT 432, we will only use test data to provide a final estimate of the generalization error of a chosen model. (Much more on this along the way.) Again, do not, ever, for any reason, fit a model using test data! I repeat: Do not, ever, for any reason, fit a model using test data! (You’ve been warned.) To perform this split, we will randomly select some observations for the train (trn) set, the remainder will be used for the test (tst) set. \\[ \\mathcal{D} = \\mathcal{D}_{\\texttt{trn}} \\cup \\mathcal{D}_{\\texttt{tst}} \\] As a general guiding heuristic, use 80% of the data for training, 20% for testing. In addition to the train-test split, we will further split the train data into estimation and validation sets. These are somewhat confusing terms, developed for STAT 432, but hear us out.41 To perform this split, we will randomly select some observations (from the train set) for the estimation (est) set, the remainder will be used for the validation (val) set. \\[ \\mathcal{D}_{\\texttt{trn}} = \\mathcal{D}_{\\texttt{est}} \\cup \\mathcal{D}_{\\texttt{val}} \\] Again, use 80% of the data for estimation, 20% for validation.42 The need for this second split might not become super clear until later on, but the general idea is this: Fit a bunch of candidate models to the estimation data. (Think of this as the data to estimate the model parameters. That’s how we chose the name.) Using these candidate models, evaluate how well they perform using the validation data. After evaluating and picking a single model, re-fit this model to the entire training dataset. Provide an estimate of how well this model performs using the test data. At this point it will likely be unclear why we cannot use the same data set for selecting a model, and evaluating its performance, but we aren’t ready for that discussion yet. For now, just follow the rules while you think about why we’re worried about this. Now that we have data for estimation, and validation, we need some metrics for evaluating these models. 2.9 Regression Metrics If our goal is to “predict” then we want small errors. In general there are two types of errors we consider: Squared Errors: \\(\\left(y_i - \\hat{\\mu}(\\boldsymbol{x}_i)\\right) ^2\\) Absolute Errors: \\(|y_i - \\hat{\\mu}(\\boldsymbol{x}_i)|\\) In both cases, we will want to consider the average errors made. We define two metrics. Root Mean Square Error (RMSE) \\[ \\text{rmse}\\left(\\hat{f}_{\\texttt{set_f}}, \\mathcal{D}_{\\texttt{set_D}} \\right) = \\sqrt{\\frac{1}{n_{\\texttt{set_D}}}\\displaystyle\\sum_{i \\in {\\texttt{set_D}}}^{}\\left(y_i - \\hat{f}_{\\texttt{set_f}}({x}_i)\\right)^2} \\] Mean Absolute Error (MAE) \\[ \\text{mae}\\left(\\hat{f}_{\\texttt{set_f}}, \\mathcal{D}_{\\texttt{set_D}} \\right) = \\frac{1}{n_{\\texttt{set_D}}}\\displaystyle\\sum_{i \\in {\\texttt{set_D}}}^{}\\left|y_i - \\hat{f}_{\\texttt{set_f}}({x}_i)\\right| \\] \\(\\hat{f}_{\\texttt{set_f}}\\) is a function \\(f\\) estimated using a model fit to some dataset \\(\\texttt{set_f}\\). The \\((x_i, y_i)\\) are data from dataset \\(\\mathcal{D}_{\\texttt{set_D}}\\). For both, smaller is better. (Less error on average.) In both, we note both the data that the model was fit to, as well as the data the model is evaluated on. Depending on the data used for these different sets, we “define” different metrics. For example, for RMSE, we have: Train RMSE: Evaluate a model fit to estimation data, using estimation data. Note that this metric is only used for illustrative or diagnostic purposes. Do not use this metric to select a model or evaluate its performance. \\[ \\text{RMSE}_{\\texttt{trn}} = \\text{rmse}\\left(\\hat{f}_{\\texttt{est}}, \\mathcal{D}_{\\texttt{est}}\\right) = \\sqrt{\\frac{1}{n_{\\texttt{est}}}\\displaystyle\\sum_{i \\in {\\texttt{est}}}^{}\\left(y_i - \\hat{f}_{\\texttt{est}}({x}_i)\\right)^2} \\] Validation RMSE: Evaluate a model fit to estimation data, using validation data. This metric will often be used to select a model. \\[ \\text{RMSE}_{\\texttt{val}} = \\text{rmse}\\left(\\hat{f}_{\\texttt{est}}, \\mathcal{D}_{\\texttt{val}}\\right) = \\sqrt{\\frac{1}{n_{\\texttt{val}}}\\displaystyle\\sum_{i \\in {\\texttt{val}}}^{}\\left(y_i - \\hat{f}_{\\texttt{est}}({x}_i)\\right)^2} \\] Test RMSE: Evaluate a model fit to training data, using test data. This metric will be used to quantify the error of a chosen model. \\[ \\text{RMSE}_{\\texttt{tst}} = \\text{rmse}\\left(\\hat{f}_{\\texttt{trn}}, \\mathcal{D}_{\\texttt{tst}}\\right) = \\sqrt{\\frac{1}{n_{\\texttt{tst}}}\\displaystyle\\sum_{i \\in {\\texttt{tst}}}^{}\\left(y_i - \\hat{f}_{\\texttt{trn}}({x}_i)\\right)^2} \\] For the rest of this chapter, we will largely ignore train error. It’s a bit confusing, since it doesn’t use the full training data! However, think of training error this way: training error evaluates how well a model performs on the data used to fit the model. (This is the general concept behind “training error.” Others might simply call the “estimation” set the training set. We use “estimation” so that we can reserve “train” for the full training dataset, not just the subset use to initially fit the model.) Let’s return to the sim_mlr_data data and apply these splits and metrics to this data. # test-train split mlr_trn_idx = sample(nrow(sim_mlr_data), size = 0.8 * nrow(sim_mlr_data)) mlr_trn = sim_mlr_data[mlr_trn_idx, ] mlr_tst = sim_mlr_data[-mlr_trn_idx, ] Here we randomly select 80% of the rows of the full data, and store these indices as mlr_trn_idx. We then create the mlr_trn and mlr_tst datasets by either selecting or anti-selecting these rows from the original dataset. # estimation-validation split mlr_est_idx = sample(nrow(mlr_trn), size = 0.8 * nrow(mlr_trn)) mlr_est = mlr_trn[mlr_est_idx, ] mlr_val = mlr_trn[-mlr_est_idx, ] We then repeat the process from above within the train data. Now, let’s compare mod_3 and mod_4. To do so, we first fit both models to the estimation data. mod_3_est = lm(y ~ ., data = mlr_est) mod_4_est = lm(y ~ x1 + I(sin(x2)) + I(x3 ^ 3) + x4, data = mlr_est) We then calculate the validation error for both. Because we will do it so often, we go ahead and write a function to calculate RMSE, given vectors of the actual values (from the data used to evaluate) and the predictions from the model. calc_rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } # calculate validation RMSE, model 3 calc_rmse(actual = mlr_val$y, predicted = predict(mod_3_est, mlr_val)) ## [1] 0.5788282 # calculate validation RMSE, model 4 calc_rmse(actual = mlr_val$y, predicted = predict(mod_4_est, mlr_val)) ## [1] 0.5452852 Here we see that mod_4_est achieves a lower validation error, so we move forward with this model.43 We then refit to the full train data, then evaluate on test. mod_4_trn = lm(y ~ x1 + I(sin(x2)) + I(x3 ^ 3) + x4, data = mlr_trn) # calculate test RMSE, model 4 calc_rmse(actual = mlr_tst$y, predicted = predict(mod_4_trn, mlr_tst)) ## [1] 0.538057 We ignore the validation metrics. (We already used them for selecting a model.) This test RMSE is our estimate of how well our selected model will perform on unseen data, on average, in a squared error sense. Note that for selecting a model there is no difference between MSE and RMSE, but for the sake of understanding, RMSE has preferential units, the same units as the response variables. (Whereas MSE has units squared.) We will always report RMSE. 2.9.1 Graphical Evaluation In addition to numeric evaluations, we can evaluate a regression model graphically, in particular with a predicted versus actual plot. plot( x = mlr_tst$y, y = predict(mod_4_trn, mlr_tst), pch = 20, col = &quot;darkgrey&quot;, xlim = c(-1, 10), ylim = c(-1, 10), main = &quot;Predicted vs Actual, Model 4, Test Data&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot; ) abline(a = 0, b = 1, lwd = 2) grid() The closer to the line the better. Also, the less of a pattern the better. In other words, this plot will help diagnose if our model is making similar sized errors for all predictions, or if there are systematic differences. It can also help identify large errors. Sometimes, errors can be on average small, but include some huge errors. In some settings, this may be extremely undesirable. This might get you thinking about “checking the assumptions” of a linear model. Assessing things like: normality, constant variance, etc. Note that while these are nice things to have, we aren’t really concerned with these things. If we care how well our model predicts, then we will directly evaluate how well it predicts. Least squares is least squares. It minimizes errors. It doesn’t care about model assumptions. 2.10 Example: “Simple” Simulated Data Let’s return to our initial dataset with a single feature \\(x\\). This time we’ll generate more data, and then split it. # define regression function cubic_mean = function(x) { 1 - 2 * x - 3 * x ^ 2 + 5 * x ^ 3 } # define full data generating process gen_slr_data = function(sample_size = 100, mu) { x = runif(n = sample_size, min = -1, max = 1) y = mu(x) + rnorm(n = sample_size) tibble(x, y) } # simulate entire dataset set.seed(3) sim_slr_data = gen_slr_data(sample_size = 100, mu = cubic_mean) # test-train split slr_trn_idx = sample(nrow(sim_slr_data), size = 0.8 * nrow(sim_slr_data)) slr_trn = sim_slr_data[slr_trn_idx, ] slr_tst = sim_slr_data[-slr_trn_idx, ] # estimation-validation split slr_est_idx = sample(nrow(slr_trn), size = 0.8 * nrow(slr_trn)) slr_est = slr_trn[slr_est_idx, ] slr_val = slr_trn[-slr_est_idx, ] # check data head(slr_trn, n = 10) ## # A tibble: 10 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.573 -1.18 ## 2 0.807 0.576 ## 3 0.272 -0.973 ## 4 -0.813 -1.78 ## 5 -0.161 0.833 ## 6 0.736 1.07 ## 7 -0.242 2.97 ## 8 0.520 -1.64 ## 9 -0.664 0.269 ## 10 -0.777 -2.02 This time let’s evaluate nine different models. Polynomial models from degree 1 to 9. We fit each model to the estimation data, and store the results in a list. poly_mod_est_list = list( poly_mod_1_est = lm(y ~ poly(x, degree = 1), data = slr_est), poly_mod_2_est = lm(y ~ poly(x, degree = 2), data = slr_est), poly_mod_3_est = lm(y ~ poly(x, degree = 3), data = slr_est), poly_mod_4_est = lm(y ~ poly(x, degree = 4), data = slr_est), poly_mod_5_est = lm(y ~ poly(x, degree = 5), data = slr_est), poly_mod_6_est = lm(y ~ poly(x, degree = 6), data = slr_est), poly_mod_7_est = lm(y ~ poly(x, degree = 7), data = slr_est), poly_mod_8_est = lm(y ~ poly(x, degree = 8), data = slr_est), poly_mod_9_est = lm(y ~ poly(x, degree = 9), data = slr_est) ) So, for example, to access the third model, we would use poly_mod_est_list[[3]] ## ## Call: ## lm(formula = y ~ poly(x, degree = 3), data = slr_est) ## ## Coefficients: ## (Intercept) poly(x, degree = 3)1 poly(x, degree = 3)2 ## -0.2058 5.3030 -7.4306 ## poly(x, degree = 3)3 ## 6.7638 But let’s back up. That code was terrible to write. Too much repeated code.44 First, we see that we are repeatedly fitting models where the only differences is the degree of the polynomial. Let’s write a function that takes as input the degree of the polynomial, and then fits the model with a polynomial of that degree, to the estimation data.45 fit_poly_mod_to_est_data = function(d) { lm(y ~ poly(x, degree = d), data = slr_est) } Now, we just need to go about the business of “repeating” this process for d from 1 to 9. Your first instinct might be a for loop, but fight that instinct. poly_mod_est_list = lapply(1:9, fit_poly_mod_to_est_data) This accomplishes the same task, but is much cleaner! poly_mod_est_list[[3]] ## ## Call: ## lm(formula = y ~ poly(x, degree = d), data = slr_est) ## ## Coefficients: ## (Intercept) poly(x, degree = d)1 poly(x, degree = d)2 ## -0.2058 5.3030 -7.4306 ## poly(x, degree = d)3 ## 6.7638 We’ll use the various *apply() functions throughout this text. A bit more on them later. We also may quickly introduce an alternative system, which is the use of the map() function (and its associated functions) from the purrr package. # make predictions on the estimation data with each model poly_mod_est_pred = lapply(poly_mod_est_list, predict, slr_est) # make predictions on the validation data with each model poly_mod_val_pred = lapply(poly_mod_est_list, predict, slr_val) If instead we wanted to return a numeric vector, we would use, sapply(). Let’s use this to calculate train and validation RMSE. # calculate train RMSE slr_est_rmse = sapply(poly_mod_est_pred, calc_rmse, actual = slr_est$y) # calculate validation RMSE slr_val_rmse = sapply(poly_mod_val_pred, calc_rmse, actual = slr_val$y) slr_est_rmse ## [1] 1.5748180 1.2717458 0.9500069 0.9480786 0.9302359 0.9187948 0.9151668 ## [8] 0.9120942 0.9117093 slr_val_rmse ## [1] 1.6584930 1.2791685 0.9574010 0.9729928 1.0104449 1.0505615 1.0617693 ## [8] 1.0953461 1.0968283 Note that training error goes down46 as degree goes up. Validation error goes down, then starts creeping up. This is a pattern that we’ll keep an eye out for. Later, we will explain this phenomenon. which.min(slr_val_rmse) ## [1] 3 The model with polynomial degree 3 has the lowest validation error47, so we move forward with this model. We re-fit to the full train dataset, then evaluate on the test set one last time. poly_mod_3_trn = lm(y ~ poly(x, degree = 3), data = slr_trn) calc_rmse(actual = slr_tst$y, predicted = predict(poly_mod_3_trn, slr_tst)) ## [1] 0.7198306 Note: There are hints here that this process is a bit unstable. See if you can figure out why. Hint: See what happens when you change the seed to generate or split the data. We’ll return to this issue when we introduce cross-validation, but for now, we’ll pretend we didn’t notice. We’ll round out this chapter with three “real” data examples. 2.11 Example: Diamonds Data For this example, we use (a subset of) the diamonds data from the ggplot2 package. # load (subset of) data set.seed(42) dmnd = ggplot2::diamonds[sample(nrow(ggplot2::diamonds), size = 5000), ] # data prep dmnd = dmnd %&gt;% mutate(cut = factor(cut, ordered = FALSE), color = factor(color, ordered = FALSE), clarity = factor(clarity, ordered = FALSE)) %&gt;% select(-price, everything()) # test-train split dmnd_trn_idx = sample(nrow(dmnd), size = 0.8 * nrow(dmnd)) dmnd_trn = dmnd[dmnd_trn_idx, ] dmnd_tst = dmnd[-dmnd_trn_idx, ] # estimation-validation split dmnd_est_idx = sample(nrow(dmnd_trn), size = 0.8 * nrow(dmnd_trn)) dmnd_est = dmnd_trn[dmnd_est_idx, ] dmnd_val = dmnd_trn[-dmnd_est_idx, ] The code above loads the data, then performs a test-train split, then additionally an estimation-validation split. We then look at the train data. That is we do not even look at the test data. # check data head(dmnd_trn, n = 10) ## # A tibble: 10 x 10 ## carat cut color clarity depth table x y z price ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.5 Premium H SI1 59 59 5.22 5.18 3.07 1156 ## 2 1.01 Ideal G SI2 63.2 57 6.33 6.28 3.99 4038 ## 3 0.62 Very Good D SI1 61.3 58 5.47 5.49 3.36 1949 ## 4 0.41 Ideal D VS2 62.4 54 4.78 4.74 2.97 1076 ## 5 0.31 Ideal G IF 61.6 54 4.36 4.4 2.7 853 ## 6 1.08 Ideal I SI1 62.6 53.9 6.51 6.56 4.09 5049 ## 7 0.52 Very Good G VS2 62.4 60 5.14 5.18 3.22 1423 ## 8 1.01 Premium F SI2 60.9 60 6.45 6.42 3.91 3297 ## 9 0.57 Ideal H VS1 61.7 54 5.33 5.36 3.3 1554 ## 10 0.34 Ideal H VS2 62.5 54 4.54 4.49 2.82 689 Our goal here will be to build a model to predict the price of a diamond given it’s characteristics. Let’s create a few EDA plots. Note that these plots do not contain the test data. If they did, we would be using the test data to influence model building and selection, a big no-no. Let’s consider four possible models, each of which we fit to the estimation data. dmnd_mod_list = list( dmnd_mod_1_est = lm(price ~ carat, data = dmnd_est), dmnd_mod_2_est = lm(price ~ carat + x + y + z, data = dmnd_est), dmnd_mod_3_est = lm(price ~ poly(carat, degree = 2) + x + y + z, data = dmnd_est), dmnd_mod_4_est = lm(price ~ poly(carat, degree = 2) + . - carat, data = dmnd_est) ) Now, let’s calculate the validation RMSE of each. dmnd_mod_val_pred = lapply(dmnd_mod_list, predict, dmnd_val) sapply(dmnd_mod_val_pred, calc_rmse, actual = dmnd_val$price) ## dmnd_mod_1_est dmnd_mod_2_est dmnd_mod_3_est dmnd_mod_4_est ## 1583.558 1517.080 1634.396 1350.659 It looks like model dmnd_mod_4_est achieves the lowest validation error. We re-fit this model, then report the test RMSE. dmnd_mod_4_trn = lm(price ~ poly(carat, degree = 2) + . - carat, data = dmnd_trn) calc_rmse(actual = dmnd_tst$price, predicted = predict(dmnd_mod_4_trn, dmnd_tst)) ## [1] 1094.916 So, on average, this model is “wrong” by about $1000 dollars. However, less-so when it is a low cost diamond, more so with high priced diamonds, as we can see in the plot below. plot( x = dmnd_tst$price, y = predict(dmnd_mod_4_trn, dmnd_tst), pch = 20, col = &quot;darkgrey&quot;, xlim = c(0, 25000), ylim = c(0, 25000), main = &quot;Diamonds: Predicted vs Actual, Model 4, Test Data&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot; ) abline(a = 0, b = 1, lwd = 2) grid() Some things to consider: Could you use the predicted versus actual plot to assist in selecting a model with the validation data? Note that the model we have chosen is not necessarily the “best” model. It is simply the model with the lowest validation RMSE. This is currently a very simplistic analysis. Does this plot suggest any issues with this model? (Hint: Note the range of predicted values.) Can you improve this model? Would a log transform of price help? 2.12 Example: Credit Card Data Suppose you work for a small local bank, perhaps a credit union, that has a credit card product offering. For years, you relied on credit agencies to provide a rating of your customer’s credit, however, this costs your bank money. One day, you realize that it might be possible to reverse engineer your customers’ (and thus potential customers) credit rating based on the credit ratings that you have already purchased, as well as the demographic and credit card information that you already have, such as age, education level, credit limit, etc.48 So long as you can estimate customers’ credit ratings with a reasonable error, you could stop buying the ratings from an outside agency. Effectively, you will have created your own rating. # load data, coerce to tibble crdt = as_tibble(ISLR::Credit) To perform this analysis, we will use the Credit data form the ISLR package. Note: this is not real data. It has been simulated. # data prep crdt = crdt %&gt;% select(-ID) %&gt;% select(-Rating, everything()) We remove the ID variable as it should have no predictive power. We also move the Rating variable to the last column with a clever dplyr trick. This is in no way necessary, but is useful in creating some plots. # test-train split set.seed(1) crdt_trn_idx = sample(nrow(crdt), size = 0.8 * nrow(crdt)) crdt_trn = crdt[crdt_trn_idx, ] crdt_tst = crdt[-crdt_trn_idx, ] # estimation-validation split crdt_est_idx = sample(nrow(crdt_trn), size = 0.8 * nrow(crdt_trn)) crdt_est = crdt_trn[crdt_est_idx, ] crdt_val = crdt_trn[-crdt_est_idx, ] After train-test and estimation-validation splitting the data, we look at the train data. # check data head(crdt_trn, n = 10) ## # A tibble: 10 x 11 ## Income Limit Cards Age Education Gender Student Married Ethnicity Balance ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 183. 13913 4 98 17 &quot; Male&quot; No Yes Caucasian 1999 ## 2 35.7 2880 2 35 15 &quot; Male&quot; No No African A… 0 ## 3 123. 8376 2 89 17 &quot; Male&quot; Yes No African A… 1259 ## 4 20.8 2672 1 70 18 &quot;Femal… No No African A… 0 ## 5 39.1 5565 4 48 18 &quot;Femal… No Yes Caucasian 772 ## 6 36.5 3806 2 52 13 &quot; Male&quot; No No African A… 188 ## 7 45.1 3762 3 80 8 &quot; Male&quot; No Yes Caucasian 70 ## 8 43.5 2906 4 69 11 &quot; Male&quot; No No Caucasian 0 ## 9 23.1 3476 2 50 15 &quot;Femal… No No Caucasian 209 ## 10 53.2 4943 2 46 16 &quot;Femal… No Yes Asian 382 ## # … with 1 more variable: Rating &lt;int&gt; To get a better “look” at the data, consider running the following: skimr::skim(crdt_trn) str(crdt_trn) View(crdt_trn) We also create a pairs plot. We immediately notice three variables that have a strong correlation with Rating: Income, Limit, and Balance. Based on this, we evaluate five candidate models.49 crdt_mod_list = list( crdt_mod_0_est = lm(Rating ~ 1, data = crdt_est), crdt_mod_1_est = lm(Rating ~ Limit, data = crdt_est), crdt_mod_2_est = lm(Rating ~ Limit + Income + Balance, data = crdt_est), crdt_mod_3_est = lm(Rating ~ ., data = crdt_est), crdt_mod_4_est = step(lm(Rating ~ . ^ 2, data = crdt_est), trace = FALSE) ) crdt_mod_val_pred = lapply(crdt_mod_list, predict, crdt_val) sapply(crdt_mod_val_pred, calc_rmse, actual = crdt_val$Rating) ## crdt_mod_0_est crdt_mod_1_est crdt_mod_2_est crdt_mod_3_est crdt_mod_4_est ## 140.080591 12.244099 12.333767 9.890607 11.575484 From these results, it appears that the additive model, including all terms performs best. We move forward with this model. final_credit_model = lm(Rating ~ ., data = crdt_trn) sqrt(mean((predict(final_credit_model, crdt_tst) - crdt_tst$Rating) ^ 2)) ## [1] 10.47727 It seems that on average, this model errors by about 10 credit points. range(crdt_trn$Rating) ## [1] 93 982 sd(crdt_trn$Rating) ## [1] 157.5897 Given the range of possible ratings, this seem pretty good! What do you think? plot( x = crdt_tst$Rating, y = predict(final_credit_model, crdt_tst), pch = 20, col = &quot;darkgrey&quot;, main = &quot;Credit: Predicted vs Actual, Test Data&quot;, xlab = &quot;Actual&quot;, ylab = &quot;Predicted&quot; ) abline(a = 0, b = 1, lwd = 2) grid() The predicted versus actual plot almost looks too good to be true! Wow!50 In summary, if this data were real, we might have an interesting result! Do note, that both this example and the previous should not be considered data analyses, but instead, examples that reinforce how to use the validation and test sets. As part of a true analysis, we will need to be much more careful about many of our decision. After putting down some additional foundation, we’ll move towards these ideas in this text. One possible critique of what we’ve done here: It’s possible we should not have used the Limit variable. Why? Because (and we’re guessing here, as this is not real data) it’s possible that we would have to acquire this information along with the Rating information. Let’s assume this is true. We need to first make a distinction between train time and test time. Train time: The portion of the ML process where you are creating models. (Anything you do before make predictions on truly new data.) Test time: The portion of the ML process where you are making predictions in the real world. The issue here is that the models you create during training should only include features which you will have access to at test time. Otherwise, you won’t actually be able to make a prediction! Hint: If you’re wearing a wrist watch, you probably have the need to be at certain locations at certain times. That is, you’re probably more likely to be in a hurry!↩︎ Fun fact: RA Fisher, the most famous statistician, did not believe that smoking caused cancer. It’s actually a part of a larger fasinating story.↩︎ We will return later to discuss supervised learning in general after getting through some specifics of regression and classification.↩︎ Features are also called covariates or predictors but we find the “predictors” nomenclature to be problematic when discussing prediction tasks. We will attempt to consistently use “features.”↩︎ Note that using a different loss function will result in a different regression function. For example, if we used absolute loss, we would then have a regression function that is the conditional median. This particular regression function is related to Quantile Regression. Perhaps more on this later.↩︎ That’s convenient isn’t it?↩︎ Spoiler: Don’t fit the model to all the available data. Pretend the data you didn’t use is “new” when you evaluate models.↩︎ For some additional reading on R’s formula syntax, the following two blog posts by Max Kuhn are good reads: The R Formula Method: The Good Parts and The R Formula Method: The Bad Parts.↩︎ However, we will be assuming that this data is generated using the same process as the original data. It is important to keep this in mind in practice.↩︎ We’re ignoring some nuance by adhering to these rules, but unless you have a very good reason to break them, it’s best to follow them.↩︎ The hope is that these terms will make the transition to using cross-validation much easier.↩︎ There is a trade-off here. More data for estimation gives better estimates. More data for validation gives a better sense of errors on new data.↩︎ We note that there isn’t actually a huge difference between these two, an idea we will return to later.↩︎ Wikipedia: Don’t Repeat Yourself↩︎ This function could be made more general if also supplied an argument for data, but we’re keeping things simple for now.↩︎ More specifically, it never increases.↩︎ This shouldn’t be too surprising given the way the data was generated!↩︎ We make no comment on the legality or ethics of this idea. Consider these before using at your own risk.↩︎ You might be wondering, aren’t there about a million different candidate models we could consider if we included things like engineered variables and interactions? Yup! Because of this, we’ll look at some variable selection techniques, as well as some algorithms that avoid this issue to a certain extent.↩︎ Perhaps not surprising since this data was simulated.↩︎ "],["nonparametric-regression.html", "Chapter 3 Nonparametric Regression 3.1 R Setup and Source 3.2 Mathematical Setup 3.3 k-Nearest Neighbors 3.4 Decision Trees 3.5 Example: Credit Card Data", " Chapter 3 Nonparametric Regression In this chapter, we will continue to explore models for making predictions, but now we will introduce nonparametric models that will contrast the parametric models that we have used previously. Specifically, we will discuss: How to use k-nearest neighbors for regression through the use of the knnreg() function from the caret package How to use decision trees for regression through the use of the rpart() function from the rpart package. How “making predictions” can be thought of as estimating the regression function, that is, the conditional mean of the response given values of the features. The difference between parametric and nonparametric methods. The difference between model parameters and tuning parameters methods. How these nonparametric methods deal with categorical variables and interactions. We will also hint at, but delay for one more chapter a detailed discussion of: What is model flexibility? What is overfitting and how do we avoid it? This chapter is currently under construction. While it is being developed, the following links to the STAT 432 course notes. Notes: Nonparametric Regression 3.1 R Setup and Source library(tibble) # data frame printing library(dplyr) # data manipulation library(caret) # fitting knn library(rpart) # fitting trees library(rpart.plot) # plotting trees library(knitr) # creating tables library(kableExtra) # styling tables Additionally, objects from ISLR are accessed. Recall that the Welcome chapter contains directions for installing all necessary packages for following along with the text. The R Markdown source is provided as some code, mostly for creating plots, has been suppressed from the rendered document that you are currently reading. R Markdown Source: nonparametric-regression.Rmd 3.2 Mathematical Setup Let’s return to the setup we defined in the previous chapter. Consider a random variable \\(Y\\) which represents a response variable, and \\(p\\) feature variables \\(\\boldsymbol{X} = (X_1, X_2, \\ldots, X_p)\\). We assume that the response variable \\(Y\\) is some function of the features, plus some random noise. \\[ Y = f(\\boldsymbol{X}) + \\epsilon \\] Our goal is to find some \\(f\\) such that \\(f(\\boldsymbol{X})\\) is close to \\(Y\\). More specifically we want to minimize the risk under squared error loss. \\[ \\mathbb{E}_{\\boldsymbol{X}, Y} \\left[ (Y - f(\\boldsymbol{X})) ^ 2 \\right] = \\mathbb{E}_{\\boldsymbol{X}} \\mathbb{E}_{Y \\mid \\boldsymbol{X}} \\left[ ( Y - f(\\boldsymbol{X}) ) ^ 2 \\mid \\boldsymbol{X} = \\boldsymbol{x} \\right] \\] We saw last chapter that this risk is minimized by the conditional mean of \\(Y\\) given \\(\\boldsymbol{X}\\), \\[ \\mu(\\boldsymbol{x}) \\triangleq \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] \\] which we called the regression function. Our goal then is to estimate this regression function. Let’s return to the example from last chapter where we know the true probability model. \\[ Y = 1 - 2x - 3x ^ 2 + 5x ^ 3 + \\epsilon \\] where \\(\\epsilon \\sim \\text{N}(0, \\sigma^2)\\). Recall that this implies that the regression function is \\[ \\mu(x) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] = 1 - 2x - 3x ^ 2 + 5x ^ 3 \\] Let’s also return to pretending that we do not actually know this information, but instead have some data, \\((x_i, y_i)\\) for \\(i = 1, 2, \\ldots, n\\). x y -0.4689827 0.7174461 -0.2557522 1.2154439 0.1457067 1.6041985 0.8164156 0.9096322 -0.5966361 0.6573128 0.7967794 0.9500528 0.8893505 1.1477361 0.3215956 0.2874057 0.2582281 -1.6197576 -0.8764275 -2.2977242 -0.5880509 0.0658105 -0.6468865 -0.4708965 0.3740457 -1.3769103 -0.2317926 0.7619832 0.5396828 0.2507367 -0.0046015 2.3678186 0.4352370 -0.1293181 0.9838122 1.2774803 -0.2399296 1.1842963 0.5548904 -1.5562874 0.8694105 -0.1356129 -0.5757150 -0.1913002 0.3033475 0.1975021 -0.7488898 -0.1847245 -0.4655587 1.5395212 -0.2277718 1.0762960 -0.9732193 -4.7573427 -0.2352241 1.9363453 0.7393817 0.4588894 -0.3193020 0.4812168 We simulated a bit more data than last time to make the “pattern” clearer to recognize. Recall that when we used a linear model, we first need to make an assumption about the form of the regression function. For example, we could assume that \\[ \\mu(x) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 \\] which is fit in R using the lm() function lm(y ~ x + I(x ^ 2) + I(x ^ 3), data = sim_slr_data) ## ## Call: ## lm(formula = y ~ x + I(x^2) + I(x^3), data = sim_slr_data) ## ## Coefficients: ## (Intercept) x I(x^2) I(x^3) ## 0.8397 -2.7257 -2.3752 6.0906 Notice that what is returned are (maximum likelihood or least squares) estimates of the unknown \\(\\beta\\) coefficients. That is, the “learning” that takes place with a linear models is “learning” the values of the coefficients. For this reason, we call linear regression models parametric models. They have unknown model parameters, in this case the \\(\\beta\\) coefficients that must be learned from the data. The form of the regression function is assumed. What if we don’t want to make an assumption about the form of the regression function? While in this case, you might look at the plot and arrive at a reasonable guess of assuming a third order polynomial, what if it isn’t so clear? What if you have 100 features? Making strong assumptions might not work well. Enter nonparametric models. We will consider two examples: k-nearest neighbors and decision trees. 3.3 k-Nearest Neighbors We’ll start with k-nearest neighbors which is possibly a more intuitive procedure than linear models.51 If our goal is to estimate the mean function, \\[ \\mu(x) = \\mathbb{E}[Y \\mid \\boldsymbol{X} = \\boldsymbol{x}] \\] the most natural approach would be to use \\[ \\text{average}(\\{ y_i : x_i = x \\}). \\] That is, to estimate the conditional mean at \\(x\\), average the \\(y_i\\) values for each data point where \\(x_i = x\\). While this sounds nice, it has an obvious flaw. For most values of \\(x\\) there will not be any \\(x_i\\) in the data where \\(x_i = x\\)! So what’s the next best thing? Pick values of \\(x_i\\) that are “close” to \\(x\\). \\[ \\text{average}( \\{ y_i : x_i \\text{ equal to (or very close to) x} \\} ). \\] This is the main idea behind many nonparametric approaches. The details often just amount to very specifically defining what “close” means. In the case of k-nearest neighbors we use \\[ \\hat{\\mu}_k(x) = \\frac{1}{k} \\sum_{ \\{i \\ : \\ x_i \\in \\mathcal{N}_k(x, \\mathcal{D}) \\} } y_i \\] as our estimate of the regression function at \\(x\\). While this looks complicated, it is actually very simple. Here, we are using an average of the \\(y_i\\) values of for the \\(k\\) nearest neighbors to \\(x\\). The \\(k\\) “nearest” neighbors are the \\(k\\) data points \\((x_i, y_i)\\) that have \\(x_i\\) values that are nearest to \\(x\\). We can define “nearest” using any distance we like, but unless otherwise noted, we are referring to euclidean distance.52 We are using the notation \\(\\{i \\ : \\ x_i \\in \\mathcal{N}_k(x, \\mathcal{D}) \\}\\) to define the \\(k\\) observations that have \\(x_i\\) values that are nearest to the value \\(x\\) in a dataset \\(\\mathcal{D}\\), in other words, the \\(k\\) nearest neighbors. The plots below begin to illustrate this idea. In the left plot, to estimate the mean of \\(Y\\) at \\(x = -0.5\\) we use the three nearest neighbors, which are highlighted with green. Our estimate is the average of the \\(y_i\\) values of these three points indicated by the black x. In the middle plot, to estimate the mean of \\(Y\\) at \\(x = 0\\) we use the five nearest neighbors, which are highlighted with green. Our estimate is the average of the \\(y_i\\) values of these five points indicated by the black x. In the right plot, to estimate the mean of \\(Y\\) at \\(x = 0.75\\) we use the nine nearest neighbors, which are highlighted with green. Our estimate is the average of the \\(y_i\\) values of these nine points indicated by the black x. You might begin to notice a bit of an issue here. We have to do a new calculation each time we want to estimate the regression function at a different value of \\(x\\)! For this reason, k-nearest neighbors is often said to be “fast to train” and “slow to predict.” Training, is instant. You just memorize the data! Prediction involves finding the distance between the \\(x\\) considered and all \\(x_i\\) in the data!53 So, how then, do we choose the value of the tuning parameter \\(k\\)? We validate! First, let’s take a look at what happens with this data if we consider three different values of \\(k\\). For each plot, the black dashed curve is the true mean function. In the left plot we use \\(k = 25\\). The red “curve” is the estimate of the mean function for each \\(x\\) shown in the plot. In the left plot we use \\(k = 5\\). The blue “curve” is the estimate of the mean function for each \\(x\\) shown in the plot. In the left plot we use \\(k = 1\\). The green “curve” is the estimate of the mean function for each \\(x\\) shown in the plot. Some things to notice here: The left plot with \\(k = 25\\) is performing poorly. The estimated “curve” does not “move” enough. This is an example of an inflexible model. The right plot with \\(k = 1\\) might not perform too well. The estimated “curve” seems to “move” too much. (Notice, that it goes through each point. We’ve fit to the noise.) This is an example of a flexible model. While the middle plot with \\(k = 5\\) is not “perfect” it seems to roughly capture the “motion” of the true regression function. We can begin to see that if we generated new data, this estimated regression function would perform better than the other two. But remember, in practice, we won’t know the true regression function, so we will need to determine how our model performs using only the available data! This \\(k\\), the number of neighbors, is an example of a tuning parameter. Instead of being learned from the data, like model parameters such as the \\(\\beta\\) coefficients in linear regression, a tuning parameter tells us how to learn from data. It is user-specified. To determine the value of \\(k\\) that should be used, many models are fit to the estimation data, then evaluated on the validation. Using the information from the validation data, a value of \\(k\\) is chosen. (More on this in a bit.) Model parameters are “learned” using the same data that was used to fit the model. Tuning parameters are “chosen” using data not used to fit the model. This tuning parameter \\(k\\) also defines the flexibility of the model. In KNN, a small value of \\(k\\) is a flexible model, while a large value of \\(k\\) is inflexible.54 Before moving to an example of tuning a KNN model, we will first introduce decision trees. 3.4 Decision Trees Decision trees are similar to k-nearest neighbors but instead of looking for neighbors, decision trees create neighborhoods. We won’t explore the full details of trees, but just start to understand the basic concepts, as well as learn to fit them in R. Neighborhoods are created via recursive binary partitions. In simpler terms, pick a feature and a possible cutoff value. Data that have a value less than the cutoff for the selected feature are in one neighborhood (the left) and data that have a value greater than the cutoff are in another (the right). Within these two neighborhoods, repeat this procedure until a stopping rule is satisfied. To make a prediction, check which neighborhood a new piece of data would belong to and predict the average of the \\(y_i\\) values of data in that neighborhood. With the data above, which has a single feature \\(x\\), consider three possible cutoffs: -0.5, 0.0, and 0.75. For each plot, the black vertical line defines the neighborhoods. The green horizontal lines are the average of the \\(y_i\\) values for the points in the left neighborhood. The red horizontal lines are the average of the \\(y_i\\) values for the points in the right neighborhood. What makes a cutoff good? Large differences in the average \\(y_i\\) between the two neighborhoods. More formally we want to find a cutoff value that minimizes \\[ \\sum_{i \\in N_L} \\left( y_i - \\hat{\\mu}_{N_L} \\right) ^ 2 + \\sum_{i \\in N_R} \\left(y_i - \\hat{\\mu}_{N_R} \\right) ^ 2 \\] where \\(N_L\\) are the data in the left neighborhood, that is \\(x &lt; c\\) \\(N_R\\) are the data in the right neighborhood, that is \\(x &gt; c\\) \\(\\hat{\\mu}_{N_L}\\) is the mean of the \\(y_i\\) for data in the left neighborhood \\(\\hat{\\mu}_{N_R}\\) is the mean of the \\(y_i\\) for data in the right neighborhood This quantity is the sum of two sum of squared errors, one for the left neighborhood, and one for the right neighborhood. Cutoff Total SSE Left SSE Right SSE -0.50 45.02 21.28 23.74 0.00 58.94 44.68 14.26 0.75 56.71 55.46 1.25 The table above summarizes the results of the three potential splits. We see that (of the splits considered, which are not exhaustive55) the split based on a cutoff of \\(x = -0.50\\) creates the best partitioning of the space. Now let’s consider building a full tree. In the plot above, the true regression function is the dashed black curve, and the solid orange curve is the estimated regression function using a decision tree. We see that there are two splits, which we can visualize as a tree. The above “tree”56 shows the splits that were made. It informs us of the variable used, the cutoff value, and some summary of the resulting neighborhood. In “tree” terminology the resulting neighborhoods are “terminal nodes” of the tree. In contrast, “internal nodes” are neighborhoods that are created, but then further split. The “root node” is the neighborhood contains all observations, before any splitting, and can be seen at the top of the image above. We see that this node represents 100% of the data. The other number, 0.21, is the mean of the response variable, in this case, \\(y_i\\). Looking at a terminal node, for example the bottom left node, we see that 23% of the data is in this node. The average value of the \\(y_i\\) in this node is -1, which can be seen in the plot above. We also see that the first split is based on the \\(x\\) variable, and a cutoff of \\(x = -0.52\\). Note that because there is only one variable here, all splits are based on \\(x\\), but in the future, we will have multiple features that can be split and neighborhoods will no longer be one-dimensional. However, this is hard to plot. Let’s build a bigger, more flexible tree. There are two tuning parameters at play here which we will call by their names in R which we will see soon: cp or the “complexity parameter” as it is called.57 This parameter determines which splits are accepted. A split must improve the performance of the tree by more than cp in order to be used. When we get to R, we will see that the default value is 0.1. minsplit, the minimum number of observations in a node (neighborhood) in order to consider splitting within a neighborhood. There are actually many more possible tuning parameters for trees, possibly differing depending on who wrote the code you’re using. We will limit discussion to these two.58 Note that they effect each other, and they effect other parameters which we are not discussing. The main takeaway should be how they effect model flexibility. First let’s look at what happens for a fixed minsplit by variable cp. We see that as cp decreases, model flexibility increases. We see more splits, because the increase in performance needed to accept a split is smaller as cp is reduced. Now the reverse, fix cp and vary minsplit. We see that as minsplit decreases, model flexibility increases. By allowing splits of neighborhoods with fewer observations, we obtain more splits, which results in a more flexible model. 3.5 Example: Credit Card Data Let’s return to the credit card data from the previous chapter. While last time we used the data to inform a bit of analysis, this time we will simply use the dataset to illustrate some concepts. # load data, coerce to tibble crdt = as_tibble(ISLR::Credit) Again, we are using the Credit data form the ISLR package. Note: this is not real data. It has been simulated. # data prep crdt = crdt %&gt;% select(-ID) %&gt;% select(-Rating, everything()) We remove the ID variable as it should have no predictive power. We also move the Rating variable to the last column with a clever dplyr trick. This is in no way necessary, but is useful in creating some plots. # test-train split set.seed(1) crdt_trn_idx = sample(nrow(crdt), size = 0.8 * nrow(crdt)) crdt_trn = crdt[crdt_trn_idx, ] crdt_tst = crdt[-crdt_trn_idx, ] # estimation-validation split crdt_est_idx = sample(nrow(crdt_trn), size = 0.8 * nrow(crdt_trn)) crdt_est = crdt_trn[crdt_est_idx, ] crdt_val = crdt_trn[-crdt_est_idx, ] After train-test and estimation-validation splitting the data, we look at the train data. # check data head(crdt_trn, n = 10) ## # A tibble: 10 x 11 ## Income Limit Cards Age Education Gender Student Married Ethnicity Balance ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 183. 13913 4 98 17 &quot; Male&quot; No Yes Caucasian 1999 ## 2 35.7 2880 2 35 15 &quot; Male&quot; No No African A… 0 ## 3 123. 8376 2 89 17 &quot; Male&quot; Yes No African A… 1259 ## 4 20.8 2672 1 70 18 &quot;Femal… No No African A… 0 ## 5 39.1 5565 4 48 18 &quot;Femal… No Yes Caucasian 772 ## 6 36.5 3806 2 52 13 &quot; Male&quot; No No African A… 188 ## 7 45.1 3762 3 80 8 &quot; Male&quot; No Yes Caucasian 70 ## 8 43.5 2906 4 69 11 &quot; Male&quot; No No Caucasian 0 ## 9 23.1 3476 2 50 15 &quot;Femal… No No Caucasian 209 ## 10 53.2 4943 2 46 16 &quot;Femal… No Yes Asian 382 ## # … with 1 more variable: Rating &lt;int&gt; Recall that we would like to predict the Rating variable. This time, let’s try to use only demographic information as predictors.59 In particular, let’s focus on Age (numeric), Gender (categorical), and Student (categorical). Let’s fit KNN models with these features, and various values of \\(k\\). To do so, we use the knnreg() function from the caret package.60 Use ?knnreg for documentation and details. crdt_knn_01 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 1) crdt_knn_10 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 10) crdt_knn_25 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 25) Here, we fit three models to the estimation data. We supply the variables that will be used as features as we would with lm(). We also specify how many neighbors to consider via the k argument. But wait a second, what is the distance from non-student to student? From male to female? In other words, how does KNN handle categorical variables? It doesn’t! Like lm() it creates dummy variables under the hood. Note: To this point, and until we specify otherwise, we will always coerce categorical variables to be factor variables in R. We will then let modeling functions such as lm() or knnreg() deal with the creation of dummy variables internally. head(crdt_knn_10$learn$X) ## Age GenderFemale StudentYes ## 1 30 0 0 ## 2 25 0 0 ## 3 44 0 0 ## 4 73 1 0 ## 5 44 0 1 ## 6 71 0 0 Once these dummy variables have been created, we have a numeric \\(X\\) matrix, which makes distance calculations easy.61 For example, the distance between the 3rd and 4th observation here is 29.017. dist(head(crdt_knn_10$learn$X)) ## 1 2 3 4 5 ## 2 5.000000 ## 3 14.000000 19.000000 ## 4 43.011626 48.010416 29.017236 ## 5 14.035669 19.026298 1.000000 29.034462 ## 6 41.000000 46.000000 27.000000 2.236068 27.018512 sqrt(sum((crdt_knn_10$learn$X[3, ] - crdt_knn_10$learn$X[4, ]) ^ 2)) ## [1] 29.01724 What about interactions? Basically, you’d have to create them the same way as you do for linear models. We only mention this to contrast with trees in a bit. OK, so of these three models, which one performs best? (Where for now, “best” is obtaining the lowest validation RMSE.) First, note that we return to the predict() function as we did with lm(). predict(crdt_knn_10, crdt_val[1:5, ]) ## [1] 337.7857 356.0000 295.7692 360.8182 306.8000 This uses the 10-NN (10 nearest neighbors) model to make predictions (estimate the regression function) given the first five observations of the validation data. Note: We did not name the second argument to predict(). Again, you’ve been warned. Now that we know how to use the predict() function, let’s calculate the validation RMSE for each of these models. knn_mod_list = list( crdt_knn_01 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 1), crdt_knn_10 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 10), crdt_knn_25 = knnreg(Rating ~ Age + Gender + Student, data = crdt_est, k = 25) ) knn_val_pred = lapply(knn_mod_list, predict, crdt_val) calc_rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } sapply(knn_val_pred, calc_rmse, crdt_val$Rating) ## crdt_knn_01 crdt_knn_10 crdt_knn_25 ## 182.3469 149.2172 138.6527 So, of these three values of \\(k\\), the model with \\(k = 25\\) achieves the lowest validation RMSE. This process, fitting a number of models with different values of the tuning parameter, in this case \\(k\\), and then finding the “best” tuning parameter value based on performance on the validation data is called tuning. In practice, we would likely consider more values of \\(k\\), but this should illustrate the point. In the next chapter, we will discuss the details of model flexibility and model tuning, and how these concepts are tied together. However, even though we will present some theory behind this relationship, in practice, you must tune and validate your models. There is no theory that will inform you ahead of tuning and validation which model will be the best. By teaching you how to fit KNN models in R and how to calculate validation RMSE, you already have all a set of tools you can use to find a good model. Let’s turn to decision trees which we will fit with the rpart() function from the rpart package. Use ?rpart and ?rpart.control for documentation and details. In particular, ?rpart.control will detail the many tuning parameters of this implementation of decision tree models in R. We’ll start by using default tuning parameters. crdt_tree = rpart(Rating ~ Age + Gender + Student, data = crdt_est) crdt_tree ## n= 256 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 256 6667400.0 357.0781 ## 2) Age&lt; 82.5 242 5865419.0 349.3719 ## 4) Age&gt;=69.5 52 1040678.0 313.0385 * ## 5) Age&lt; 69.5 190 4737307.0 359.3158 ## 10) Age&lt; 38.5 55 700013.2 326.6000 * ## 11) Age&gt;=38.5 135 3954443.0 372.6444 ## 22) Student=Yes 14 180764.4 297.7857 * ## 23) Student=No 121 3686148.0 381.3058 ## 46) Age&gt;=50.5 64 1881299.0 359.2344 ## 92) Age&lt; 53.5 9 48528.0 278.3333 * ## 93) Age&gt;=53.5 55 1764228.0 372.4727 * ## 47) Age&lt; 50.5 57 1738665.0 406.0877 * ## 3) Age&gt;=82.5 14 539190.9 490.2857 * Above we see the resulting tree printed, however, this is difficult to read. Instead, we use the rpart.plot() function from the rpart.plot package to better visualize the tree. rpart.plot(crdt_tree) At each split, the variable used to split is listed together with a condition. If the condition is true for a data point, send it to the left neighborhood. Although the Gender available for creating splits, we only see splits based on Age and Student. This hints at the relative importance of these variables for prediction. More on this much later. Categorical variables are split based on potential categories! This is excellent. This means that trees naturally handle categorical features without needing to convert to numeric under the hood. We see a split that puts students into one neighborhood, and non-students into another. Notice that the splits happen in order. So for example, the third terminal node (with an average rating of 298) is based on splits of: Age &lt; 83 Age &lt; 70 Age &gt; 39 Student = Yes In other words, individuals in this terminal node are students who are between the ages of 39 and 70. (Only 5% of the data is represented here.) This is basically an interaction between Age and Student without any need to directly specify it! What a great feature of trees. To recap: Trees do not make assumptions about the form of the regression function. Trees automatically handle categorical features. Trees naturally incorporate interaction. Now let’s fit another tree that is more flexible by relaxing some tuning parameters. Recall that by default, cp = 0.1 and minsplit = 20. crdt_tree_big = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.0, minsplit = 20) rpart.plot(crdt_tree_big) To make the tree even bigger, we could reduce minsplit, but in practice we mostly consider the cp parameter.62 Since minsplit has been kept the same, but cp was reduced, we see the same splits as the smaller tree, but many additional splits. Now let’s fit a bunch of trees, with different values of cp, for tuning. tree_mod_list = list( crdt_tree_0000 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.000), crdt_tree_0001 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.001), crdt_tree_0010 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.010), crdt_tree_0100 = rpart(Rating ~ Age + Gender + Student, data = crdt_est, cp = 0.100) ) tree_val_pred = lapply(tree_mod_list, predict, crdt_val) sapply(tree_val_pred, calc_rmse, crdt_val$Rating) ## crdt_tree_0000 crdt_tree_0001 crdt_tree_0010 crdt_tree_0100 ## 156.3527 155.4262 151.9081 140.0806 Here we see the least flexible model, with cp = 0.100, performs best. Note that by only using these three features, we are severely limiting our models performance. Let’s quickly assess using all available predictors. crdt_tree_all = rpart(Rating ~ ., data = crdt_est) rpart.plot(crdt_tree_all) Notice that this model only splits based on Limit despite using all features. This should be a big hint about which variables are useful for prediction. calc_rmse( actual = crdt_val$Rating, predicted = predict(crdt_tree_all, crdt_val) ) ## [1] 28.8498 This model performs much better. You should try something similar with the KNN models above. Also, consider comparing this result to results from last chapter using linear models. Notice that we’ve been using that trusty predict() function here again. predict(crdt_tree_all, crdt_val[1:5, ]) ## 1 2 3 4 5 ## 292.8182 467.5152 467.5152 467.5152 772.4000 What does this code do? It estimates the mean Rating given the feature information (the “x” values) from the first five observations from the validation data using a decision tree model with default tuning parameters. Hopefully a theme is emerging. We chose to start with linear regression because most students in STAT 432 should already be familiar.↩︎ The usual distance when you hear distance. That is, unless you drive a taxicab.↩︎ For this reason, KNN is often not used in practice, but it is very useful learning tool.↩︎ Many texts use the term complex instead of flexible. We feel this is confusing as complex is often associated with difficult. KNN with \\(k = 1\\) is actually a very simple model to understand, but it is very flexible as defined here.↩︎ To exhaust all possible splits of a variable, we would need to consider the midpoint between each of the order statistics of the variable. To exhaust all possible splits, we would need to do this for each of the feature variables.↩︎ It’s really an upside tree isn’t it?↩︎ Flexibility parameter would be a better name.↩︎ The rpart function in R would allow us to use others, but we will always just leave their values as the default values.↩︎ There is a question of whether or not we should use these variables. For example, should men and women be given different ratings when all other variables are the same? Using the Gender variable allows for this to happen. Also, you might think, just don’t use the Gender variable. Unfortunately, it’s not that easy. There is an increasingly popular field of study centered around these ideas called machine learning fairness.↩︎ There are many other KNN functions in R. However, the operation and syntax of knnreg() better matches other functions we will use in this course.↩︎ Wait. Doesn’t this sort of create an arbitrary distance between the categories? Why \\(0\\) and \\(1\\) and not \\(-42\\) and \\(51\\)? Good question. This hints at the notion of pre-processing. We’re going to hold off on this for now, but, often when performing k-nearest neighbors, you should try scaling all of the features to have mean \\(0\\) and variance \\(1\\).↩︎ If you are taking STAT 432, we will occasionally modify the minsplit parameter on quizzes.↩︎ "],["bias-variance-tradeoff.html", "Chapter 4 The Bias–Variance Tradeoff 4.1 R Setup and Source 4.2 The Regression Setup 4.3 Reducible and Irreducible Error 4.4 Bias-Variance Decomposition 4.5 Using Simulation to Estimate Bias and Variance 4.6 Estimating Expected Prediction Error 4.7 Model Flexibility", " Chapter 4 The Bias–Variance Tradeoff This chapter will begin to dig into some theoretical details of estimating regression functions, in particular how the bias-variance tradeoff helps explain the relationship between model flexibility and the errors a model makes. Specifically, we will discuss: The definitions and relationship between bias, variance, and mean squared error. The relationship between model flexibility and training error. The relationship between model flexibility and validation error. Don’t fret if this presentation seems overwhelming. Next chapter we will review some general concepts about regression before moving on to classification. 4.1 R Setup and Source library(tibble) # data frame printing library(dplyr) # data manipulation library(caret) # fitting knn library(rpart) # fitting trees library(rpart.plot) # plotting trees Recall that the Welcome chapter contains directions for installing all necessary packages for following along with the text. The R Markdown source is provided as some code, mostly for creating plots, has been suppressed from the rendered document that you are currently reading. R Markdown Source: bias-variance-tradeoff.Rmd 4.2 The Regression Setup Consider the general regression setup where we are given a random pair \\((X, Y) \\in \\mathbb{R}^p \\times \\mathbb{R}\\). We would like to “predict” \\(Y\\) with some function of \\(X\\), say, \\(f(X)\\). To clarify what we mean by “predict,” we specify that we would like \\(f(X)\\) to be “close” to \\(Y\\). To further clarify what we mean by “close,” we define the squared error loss of estimating \\(Y\\) using \\(f(X)\\). \\[ L(Y, f(X)) \\triangleq (Y - f(X)) ^ 2 \\] Now we can clarify the goal of regression, which is to minimize the above loss, on average. We call this the risk of estimating \\(Y\\) using \\(f(X)\\). \\[ R(Y, f(X)) \\triangleq \\mathbb{E}[L(Y, f(X))] = \\mathbb{E}_{X, Y}[(Y - f(X)) ^ 2] \\] Before attempting to minimize the risk, we first re-write the risk after conditioning on \\(X\\). \\[ \\mathbb{E}_{X, Y} \\left[ (Y - f(X)) ^ 2 \\right] = \\mathbb{E}_{X} \\mathbb{E}_{Y \\mid X} \\left[ ( Y - f(X) ) ^ 2 \\mid X = x \\right] \\] Minimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to \\(Y \\mid X\\), essentially minimizing the risk pointwise, for each \\(x\\). It turns out, that the risk is minimized by setting \\(f(x)\\) to be equal the conditional mean of \\(Y\\) given \\(X\\), \\[ f(x) = \\mathbb{E}(Y \\mid X = x) \\] which we call the regression function.63 Note that the choice of squared error loss is somewhat arbitrary. Suppose instead we chose absolute error loss. \\[ L(Y, f(X)) \\triangleq | Y - f(X) | \\] The risk would then be minimized setting \\(f(x)\\) equal to the conditional median. \\[ f(x) = \\text{median}(Y \\mid X = x) \\] Despite this possibility, our preference will still be for squared error loss. The reasons for this are numerous, including: historical, ease of optimization, and protecting against large deviations. Now, given data \\(\\mathcal{D} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}\\), our goal becomes finding some \\(\\hat{f}\\) that is a good estimate of the regression function \\(f\\). We’ll see that this amounts to minimizing what we call the reducible error. 4.3 Reducible and Irreducible Error Suppose that we obtain some \\(\\hat{f}\\), how well does it estimate \\(f\\)? We define the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\). A good \\(\\hat{f}\\) will have a low expected prediction error. \\[ \\text{EPE}\\left(Y, \\hat{f}(X)\\right) \\triangleq \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ \\left( Y - \\hat{f}(X) \\right)^2 \\right] \\] This expectation is over \\(X\\), \\(Y\\), and also \\(\\mathcal{D}\\). The estimate \\(\\hat{f}\\) is actually random depending on the data, \\(\\mathcal{D}\\), used to estimate \\(\\hat{f}\\). We could actually write \\(\\hat{f}(X, \\mathcal{D})\\) to make this dependence explicit, but our notation will become cumbersome enough as it is. Like before, we’ll condition on \\(X\\). This results in the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\). \\[ \\text{EPE}\\left(Y, \\hat{f}(x)\\right) = \\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[ \\left(Y - \\hat{f}(X) \\right)^2 \\mid X = x \\right] = \\underbrace{\\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]}_\\textrm{reducible error} + \\underbrace{\\mathbb{V}_{Y \\mid X} \\left[ Y \\mid X = x \\right]}_\\textrm{irreducible error} \\] A number of things to note here: The expected prediction error is for a random \\(Y\\) given a fixed \\(x\\) and a random \\(\\hat{f}\\). As such, the expectation is over \\(Y \\mid X\\) and \\(\\mathcal{D}\\). Our estimated function \\(\\hat{f}\\) is random depending on the data, \\(\\mathcal{D}\\), which is used to perform the estimation. The expected prediction error of predicting \\(Y\\) using \\(\\hat{f}(X)\\) when \\(X = x\\) has been decomposed into two errors: The reducible error, which is the expected squared error loss of estimation \\(f(x)\\) using \\(\\hat{f}(x)\\) at a fixed point \\(x\\). The only thing that is random here is \\(\\mathcal{D}\\), the data used to obtain \\(\\hat{f}\\). (Both \\(f\\) and \\(x\\) are fixed.) We’ll often call this reducible error the mean squared error of estimating \\(f(x)\\) using \\(\\hat{f}\\) at a fixed point \\(x\\). \\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) \\triangleq \\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right]\\] The irreducible error. This is simply the variance of \\(Y\\) given that \\(X = x\\), essentially noise that we do not want to learn. This is also called the Bayes error. As the name suggests, the reducible error is the error that we have some control over. But how do we control this error? 4.4 Bias-Variance Decomposition After decomposing the expected prediction error into reducible and irreducible error, we can further decompose the reducible error. Recall the definition of the bias of an estimator. \\[ \\text{bias}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left[\\hat{\\theta}\\right] - \\theta \\] Also recall the definition of the variance of an estimator. \\[ \\mathbb{V}(\\hat{\\theta}) = \\text{var}(\\hat{\\theta}) \\triangleq \\mathbb{E}\\left [ ( \\hat{\\theta} -\\mathbb{E}\\left[\\hat{\\theta}\\right] )^2 \\right] \\] Using this, we further decompose the reducible error (mean squared error) into bias squared and variance. \\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) = \\mathbb{E}_{\\mathcal{D}} \\left[ \\left(f(x) - \\hat{f}(x) \\right)^2 \\right] = \\underbrace{\\left(f(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}(x) \\right)} + \\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}(x) - \\mathbb{E} \\left[ \\hat{f}(x) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}(x) \\right)} \\] This is actually a common fact in estimation theory, but we have stated it here specifically for estimation of some regression function \\(f\\) using \\(\\hat{f}\\) at some point \\(x\\). \\[ \\text{MSE}\\left(f(x), \\hat{f}(x)\\right) = \\text{bias}^2 \\left(\\hat{f}(x) \\right) + \\text{var} \\left(\\hat{f}(x) \\right) \\] In a perfect world, we would be able to find some \\(\\hat{f}\\) which is unbiased, that is \\(\\text{bias}\\left(\\hat{f}(x) \\right) = 0\\), which also has low variance. In practice, this isn’t always possible. It turns out, there is a bias-variance tradeoff. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. Flexible models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance. In the context of regression, models are biased when: Parametric: The form of the model does not incorporate all the necessary variables, or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic. Non-parametric: The model provides too much smoothing. In the context of regression, models are variable when: Parametric: The form of the model incorporates too many variables, or the form of the relationship is too flexible. For example, a parametric model assumes a cubic relationship, but the true relationship is linear. Non-parametric: The model does not provide enough smoothing. It is very, “wiggly.” So for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate flexibility for the data. Recall that when fitting models, we’ve seen that train RMSE decreases as model flexibility is increasing. (Technically it is non-increasing.) For validation RMSE, we expect to see a U-shaped curve. Importantly, validation RMSE decreases, until a certain flexibility, then begins to increase. Now we can understand why this is happening. The expected test RMSE is essentially the expected prediction error, which we now known decomposes into (squared) bias, variance, and the irreducible Bayes error. The following plots show three examples of this. The three plots show three examples of the bias-variance tradeoff. In the left panel, the variance influences the expected prediction error more than the bias. In the right panel, the opposite is true. The middle panel is somewhat neutral. In all cases, the difference between the Bayes error (the horizontal dashed grey line) and the expected prediction error (the solid black curve) is exactly the mean squared error, which is the sum of the squared bias (blue curve) and variance (orange curve). The vertical line indicates the flexibility that minimizes the prediction error. To summarize, if we assume that irreducible error can be written as \\[ \\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2 \\] then we can write the full decomposition of the expected prediction error of predicting \\(Y\\) using \\(\\hat{f}\\) when \\(X = x\\) as \\[ \\text{EPE}\\left(Y, \\hat{f}(x)\\right) = \\underbrace{\\text{bias}^2\\left(\\hat{f}(x)\\right) + \\text{var}\\left(\\hat{f}(x)\\right)}_\\textrm{reducible error} + \\sigma^2. \\] As model flexibility increases, bias decreases, while variance increases. By understanding the tradeoff between bias and variance, we can manipulate model flexibility to find a model that will predict well on unseen observations. Tying this all together, the above image shows how we “expect” training and validation error to behavior in relation to model flexibility.64 In practice, we won’t always see such a nice “curve” in the validation error, but we expect to see the general trends. 4.5 Using Simulation to Estimate Bias and Variance We will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function \\(f(x) = x^2\\). f = function(x) { x ^ 2 } More specifically, we’d like to predict an observation, \\(Y\\), given that \\(X = x\\) by using \\(\\hat{f}(x)\\) where \\[ \\mathbb{E}[Y \\mid X = x] = f(x) = x^2 \\] and \\[ \\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2. \\] Alternatively, we could write this as \\[ Y = f(X) + \\epsilon \\] where \\(\\mathbb{E}[\\epsilon] = 0\\) and \\(\\mathbb{V}[\\epsilon] = \\sigma ^ 2\\). In this formulation, we call \\(f(X)\\) the signal and \\(\\epsilon\\) the noise. To carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following R code. gen_sim_data = function(f, sample_size = 100) { x = runif(n = sample_size, min = 0, max = 1) y = rnorm(n = sample_size, mean = f(x), sd = 0.3) tibble(x, y) } Also note that if you prefer to think of this situation using the \\(Y = f(X) + \\epsilon\\) formulation, the following code represents the same data generating process. gen_sim_data = function(f, sample_size = 100) { x = runif(n = sample_size, min = 0, max = 1) eps = rnorm(n = sample_size, mean = 0, sd = 0.75) y = f(x) + eps tibble(x, y) } To completely specify the data generating process, we have made more model assumptions than simply \\(\\mathbb{E}[Y \\mid X = x] = x^2\\) and \\(\\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2\\). In particular, The \\(x_i\\) in \\(\\mathcal{D}\\) are sampled from a uniform distribution over \\([0, 1]\\). The \\(x_i\\) and \\(\\epsilon\\) are independent. The \\(y_i\\) in \\(\\mathcal{D}\\) are sampled from the conditional normal distribution. \\[ Y \\mid X \\sim N(f(x), \\sigma^2) \\] Using this setup, we will generate datasets, \\(\\mathcal{D}\\), with a sample size \\(n = 100\\) and fit four models. \\[ \\begin{aligned} \\texttt{predict(fit0, x)} &amp;= \\hat{f}_0(x) = \\hat{\\beta}_0\\\\ \\texttt{predict(fit1, x)} &amp;= \\hat{f}_1(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\\\ \\texttt{predict(fit2, x)} &amp;= \\hat{f}_2(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 \\\\ \\texttt{predict(fit9, x)} &amp;= \\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9 \\end{aligned} \\] To get a sense of the data and these four models, we generate one simulated dataset, and fit the four models. set.seed(1) sim_data = gen_sim_data(f) fit_0 = lm(y ~ 1, data = sim_data) fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data) fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data) fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data) Note that technically we’re being lazy and using orthogonal polynomials, but the fitted values are the same, so this makes no difference for our purposes. Plotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild. The following three plots were created using three additional simulated datasets. The zero predictor and ninth degree polynomial were fit to each. This plot should make clear the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance. While the ninth degree model doesn’t appear to be correct for any of these three simulations, we’ll see that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average isn’t the only goal we’re after, since in practice, we’ll only have a single dataset. This is why we’d also like our models to exhibit low variance. We could have also fit \\(k\\)-nearest neighbors models to these three datasets. Here we see that when \\(k = 100\\) we have a biased model with very low variance.65 When \\(k = 5\\), we again have a highly variable model. These two sets of plots reinforce our intuition about the bias-variance tradeoff. Flexible models (ninth degree polynomial and \\(k\\) = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and \\(k = 100\\)) are very biased, but have extremely low variance. We will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates of \\(f(x)\\) given by these four models at the point \\(x = 0.90\\). We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult. set.seed(1) n_sims = 250 n_models = 4 x = data.frame(x = 0.90) # fixed point at which we make predictions predictions = matrix(0, nrow = n_sims, ncol = n_models) for (sim in 1:n_sims) { # simulate new, random, training data # this is the only random portion of the bias, var, and mse calculations # this allows us to calculate the expectation over D sim_data = gen_sim_data(f) # fit models fit_0 = lm(y ~ 1, data = sim_data) fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data) fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data) fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data) # get predictions predictions[sim, 1] = predict(fit_0, x) predictions[sim, 2] = predict(fit_1, x) predictions[sim, 3] = predict(fit_2, x) predictions[sim, 4] = predict(fit_9, x) } Note that this is one of many ways we could have accomplished this task using R. For example we could have used a combination of replicate() and *apply() functions. Alternatively, we could have used a tidyverse approach, which likely would have used some combination of dplyr, tidyr, and purrr. Our approach, which would be considered a base R approach, was chosen to make it as clear as possible what is being done. The tidyverse approach is rapidly gaining popularity in the R community, but might make it more difficult to see what is happening here, unless you are already familiar with that approach. Also of note, while it may seem like the output stored in predictions would meet the definition of tidy data given by Hadley Wickham since each row represents a simulation, it actually falls slightly short. For our data to be tidy, a row should store the simulation number, the model, and the resulting prediction. We’ve actually already aggregated one level above this. Our observational unit is a simulation (with four predictions), but for tidy data, it should be a single prediction. The above plot shows the predictions for each of the 250 simulations of each of the four models of different polynomial degrees. The truth, \\(f(x = 0.90) = (0.9)^2 = 0.81\\), is given by the solid black horizontal line. Two things are immediately clear: As flexibility increases, bias decreases. The mean of a model’s predictions is closer to the truth. As flexibility increases, variance increases. The variance about the mean of a model’s predictions increases. The goal of this simulation study is to show that the following holds true for each of the four models. \\[ \\text{MSE}\\left(f(0.90), \\hat{f}_k(0.90)\\right) = \\underbrace{\\left(\\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] - f(0.90) \\right)^2}_{\\text{bias}^2 \\left(\\hat{f}_k(0.90) \\right)} + \\underbrace{\\mathbb{E} \\left[ \\left( \\hat{f}_k(0.90) - \\mathbb{E} \\left[ \\hat{f}_k(0.90) \\right] \\right)^2 \\right]}_{\\text{var} \\left(\\hat{f}_k(0.90) \\right)} \\] We’ll use the empirical results of our simulations to estimate these quantities. (Yes, we’re using estimation to justify facts about estimation.) Note that we’ve actually used a rather small number of simulations. In practice we should use more, but for the sake of computation time, we’ve performed just enough simulations to obtain the desired results. (Since we’re estimating estimation, the bigger the sample size, the better.) To estimate the mean squared error of our predictions, we’ll use \\[ \\widehat{\\text{MSE}}\\left(f(0.90), \\hat{f}_k(0.90)\\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(f(0.90) - \\hat{f}_k^{[i]}(0.90) \\right)^2 \\] where \\(\\hat{f}_k^{[i]}(0.90)\\) is the estimate of \\(f(0.90)\\) using the \\(i\\)-th from the polynomial degree \\(k\\) model. We also write an accompanying R function. get_mse = function(truth, estimate) { mean((estimate - truth) ^ 2) } Similarly, for the bias of our predictions we use, \\[ \\widehat{\\text{bias}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) \\right) - f(0.90) \\] And again, we write an accompanying R function. get_bias = function(estimate, truth) { mean(estimate) - truth } Lastly, for the variance of our predictions we have \\[ \\widehat{\\text{var}} \\left(\\hat{f}(0.90) \\right) = \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}} \\left(\\hat{f}_k^{[i]}(0.90) - \\frac{1}{n_{\\texttt{sims}}}\\sum_{i = 1}^{n_{\\texttt{sims}}}\\hat{f}_k^{[i]}(0.90) \\right)^2 \\] While there is already R function for variance, the following is more appropriate in this situation. get_var = function(estimate) { mean((estimate - mean(estimate)) ^ 2) } To quickly obtain these results for each of the four models, we utilize the apply() function. bias = apply(predictions, 2, get_bias, truth = f(x = 0.90)) variance = apply(predictions, 2, get_var) mse = apply(predictions, 2, get_mse, truth = f(x = 0.90)) We summarize these results in the following table. Degree Mean Squared Error Bias Squared Variance 0 0.22643 0.22476 0.00167 1 0.00829 0.00508 0.00322 2 0.00387 0.00005 0.00381 9 0.01019 0.00002 0.01017 A number of things to notice here: We use squared bias in this table. Since bias can be positive or negative, squared bias is more useful for observing the trend as flexibility increases. The squared bias trend which we see here is decreasing as flexibility increases, which we expect to see in general. The exact opposite is true of variance. As model flexibility increases, variance increases. The mean squared error, which is a function of the bias and variance, decreases, then increases. This is a result of the bias-variance tradeoff. We can decrease bias, by increasing variance. Or, we can decrease variance by increasing bias. By striking the correct balance, we can find a good mean squared error! We can check for these trends with the diff() function in R. all(diff(bias ^ 2) &lt; 0) ## [1] TRUE all(diff(variance) &gt; 0) ## [1] TRUE diff(mse) &lt; 0 ## 1 2 9 ## TRUE TRUE FALSE The models with polynomial degrees 2 and 9 are both essentially unbiased. We see some bias here as a result of using simulation. If we increased the number of simulations, we would see both biases go down. Since they are both unbiased, the model with degree 2 outperforms the model with degree 9 due to its smaller variance. Models with degree 0 and 1 are biased because they assume the wrong form of the regression function. While the degree 9 model does this as well, it does include all the necessary polynomial degrees. \\[ \\hat{f}_9(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\ldots + \\hat{\\beta}_9 x^9 \\] Then, since least squares estimation is unbiased, importantly, \\[ \\mathbb{E}\\left[\\hat{\\beta}_d\\right] = \\beta_d = 0 \\] for \\(d = 3, 4, \\ldots 9\\), we have \\[ \\mathbb{E}\\left[\\hat{f}_9(x)\\right] = \\beta_0 + \\beta_1 x + \\beta_2 x^2 \\] Now we can finally verify the bias-variance decomposition. bias ^ 2 + variance == mse ## 0 1 2 9 ## FALSE FALSE FALSE FALSE But wait, this says it isn’t true, except for the degree 9 model? It turns out, this is simply a computational issue. If we allow for some very small error tolerance, we see that the bias-variance decomposition is indeed true for predictions from these for models. all.equal(bias ^ 2 + variance, mse) ## [1] TRUE See ?all.equal() for details. So far, we’ve focused our efforts on looking at the mean squared error of estimating \\(f(0.90)\\) using \\(\\hat{f}(0.90)\\). We could also look at the expected prediction error of using \\(\\hat{f}(X)\\) when \\(X = 0.90\\) to estimate \\(Y\\). \\[ \\text{EPE}\\left(Y, \\hat{f}_k(0.90)\\right) = \\mathbb{E}_{Y \\mid X, \\mathcal{D}} \\left[ \\left(Y - \\hat{f}_k(X) \\right)^2 \\mid X = 0.90 \\right] \\] We can estimate this quantity for each of the four models using the simulation study we already performed. get_epe = function(realized, estimate) { mean((realized - estimate) ^ 2) } y = rnorm(n = nrow(predictions), mean = f(x = 0.9), sd = 0.3) epe = apply(predictions, 2, get_epe, realized = y) epe ## 0 1 2 9 ## 0.3180470 0.1104055 0.1095955 0.1205570 What about the unconditional expected prediction error. That is, for any \\(X\\), not just \\(0.90\\). Specifically, the expected prediction error of estimating \\(Y\\) using \\(\\hat{f}(X)\\). The following (new) simulation study provides an estimate of \\[ \\text{EPE}\\left(Y, \\hat{f}_k(X)\\right) = \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ \\left( Y - \\hat{f}_k(X) \\right)^2 \\right] \\] for the quadratic model, that is \\(k = 2\\) as we have defined \\(k\\). set.seed(42) n_sims = 2500 X = runif(n = n_sims, min = 0, max = 1) Y = rnorm(n = n_sims, mean = f(X), sd = 0.3) f_hat_X = rep(0, length(X)) for (i in seq_along(X)) { sim_data = gen_sim_data(f) fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data) f_hat_X[i] = predict(fit_2, newdata = data.frame(x = X[i])) } # truth 0.3 ^ 2 ## [1] 0.09 # via simulation mean((Y - f_hat_X) ^ 2) ## [1] 0.09566445 Note that in practice, we should use many more simulations in this study. 4.6 Estimating Expected Prediction Error While previously, we only decomposed the expected prediction error conditionally, a similar argument holds unconditionally. Assuming \\[ \\mathbb{V}[Y \\mid X = x] = \\sigma ^ 2. \\] we have \\[ \\text{EPE}\\left(Y, \\hat{f}(X)\\right) = \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ (Y - \\hat{f}(X))^2 \\right] = \\underbrace{\\mathbb{E}_{X} \\left[\\text{bias}^2\\left(\\hat{f}(X)\\right)\\right] + \\mathbb{E}_{X} \\left[\\text{var}\\left(\\hat{f}(X)\\right)\\right]}_\\textrm{reducible error} + \\sigma^2 \\] Lastly, we note that if \\[ \\mathcal{D} = \\mathcal{D}_{\\texttt{trn}} \\cup \\mathcal{D}_{\\texttt{tst}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i = 1, 2, \\ldots n \\] where \\[ \\mathcal{D}_{\\texttt{trn}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i \\in \\texttt{trn} \\] and \\[ \\mathcal{D}_{\\texttt{tst}} = (x_i, y_i) \\in \\mathbb{R}^p \\times \\mathbb{R}, \\ i \\in \\texttt{tst} \\] Then, if we have a model fit to the training data \\(\\mathcal{D}_{\\texttt{trn}}\\), we can use the test mean squared error \\[ \\sum_{i \\in \\texttt{tst}}\\left(y_i - \\hat{f}(x_i)\\right) ^ 2 \\] as an estimate of \\[ \\mathbb{E}_{X, Y, \\mathcal{D}} \\left[ (Y - \\hat{f}(X))^2 \\right] \\] the expected prediction error.66 How good is this estimate? Well, if \\(\\mathcal{D}\\) is a random sample from \\((X, Y)\\), and the \\(\\texttt{tst}\\) data are randomly sampled observations randomly sampled from \\(i = 1, 2, \\ldots, n\\), then it is a reasonable estimate. However, it is rather variable due to the randomness of selecting the observations for the test set, if the test set is small. 4.7 Model Flexibility Let’s return to the dataset we saw in the linear regression chapter with a single feature \\(x\\). # define regression function cubic_mean = function(x) { 1 - 2 * x - 3 * x ^ 2 + 5 * x ^ 3 } Recall that this data was generated with a cubic mean function. # define full data generating process gen_slr_data = function(sample_size = 100, mu) { x = runif(n = sample_size, min = -1, max = 1) y = mu(x) + rnorm(n = sample_size) tibble(x, y) } After defining the data generating process, we generate and split the data. # simulate entire dataset set.seed(3) sim_slr_data = gen_slr_data(sample_size = 100, mu = cubic_mean) # test-train split slr_trn_idx = sample(nrow(sim_slr_data), size = 0.8 * nrow(sim_slr_data)) slr_trn = sim_slr_data[slr_trn_idx, ] slr_tst = sim_slr_data[-slr_trn_idx, ] # estimation-validation split slr_est_idx = sample(nrow(slr_trn), size = 0.8 * nrow(slr_trn)) slr_est = slr_trn[slr_est_idx, ] slr_val = slr_trn[-slr_est_idx, ] # check data head(slr_trn, n = 10) ## # A tibble: 10 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.573 -1.18 ## 2 0.807 0.576 ## 3 0.272 -0.973 ## 4 -0.813 -1.78 ## 5 -0.161 0.833 ## 6 0.736 1.07 ## 7 -0.242 2.97 ## 8 0.520 -1.64 ## 9 -0.664 0.269 ## 10 -0.777 -2.02 For validating models, we will use RMSE. # helper function for calculating RMSE calc_rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } Let’s check how linear, k-nearest neighbors, and decision tree models fit to this data make errors, while paying attention to their flexibility. This picture is an idealized version of what we expect to see, but we’ll illustrate the sorts of validate “curves” that we might see in practice. Note that in the following three sub-sections, a significant portion of the code is suppressed for visual clarity. See the source document for full details. 4.7.1 Linear Models First up, linear models. We will fit polynomial models with degree from one to nine, and then validate. # fit polynomial models poly_mod_est_list = list( poly_mod_1_est = lm(y ~ poly(x, degree = 1), data = slr_est), poly_mod_2_est = lm(y ~ poly(x, degree = 2), data = slr_est), poly_mod_3_est = lm(y ~ poly(x, degree = 3), data = slr_est), poly_mod_4_est = lm(y ~ poly(x, degree = 4), data = slr_est), poly_mod_5_est = lm(y ~ poly(x, degree = 5), data = slr_est), poly_mod_6_est = lm(y ~ poly(x, degree = 6), data = slr_est), poly_mod_7_est = lm(y ~ poly(x, degree = 7), data = slr_est), poly_mod_8_est = lm(y ~ poly(x, degree = 8), data = slr_est), poly_mod_9_est = lm(y ~ poly(x, degree = 9), data = slr_est) ) The plot below visualizes the results. What do we see here? As the polynomial degree increases: The training error decreases. The validation error decreases, then increases. This more of less matches the idealized version above, but the validation “curve” is much more jagged. This is something that we can expect in practice. We have previously noted that training error isn’t particularly useful for validating models. That is still true. However, it can be useful for checking that everything is working as planned. In this case, since we known that training error decreases as model flexibility increases, we can verify our intuition that a higher degree polynomial is indeed more flexible.67 4.7.2 k-Nearest Neighbors Next up, k-nearest neighbors. We will consider values for \\(k\\) that are odd and between \\(1\\) and \\(45\\) inclusive. # helper function for fitting knn models fit_knn_mod = function(neighbors) { knnreg(y ~ x, data = slr_est, k = neighbors) } # define values of tuning parameter k to evaluate k_to_try = seq(from = 1, to = 45, by = 2) # fit knn models knn_mod_est_list = lapply(k_to_try, fit_knn_mod) The plot below visualizes the results. Here we see the “opposite” of the usual plot. Why? Because with k-nearest neighbors, a small value of \\(k\\) generates a flexible model compared to larger values of \\(k\\). So visually, this plot is flipped. That is we see that as \\(k\\) increases: The training error increases. The validation error decreases, then increases. Important to note here: the pattern above only holds “in general,” that is, there can be minor deviations in the validation pattern along the way. This is due to the random nature of selection the data for the validate set. 4.7.3 Decision Trees Lastly, we evaluate some decision tree models. We choose some arbitrary values of cp to evaluate, while holding minsplit constant at 5. There are arbitrary choices that produce a plot that is useful for discussion. # helper function for fitting decision tree models tree_knn_mod = function(flex) { rpart(y ~ x, data = slr_est, cp = flex, minsplit = 5) } # define values of tuning parameter cp to evaluate cp_to_try = c(0.5, 0.3, 0.1, 0.05, 0.01, 0.001, 0.0001) # fit decision tree models tree_mod_est_list = lapply(cp_to_try, tree_knn_mod) The plot below visualizes the results. Based on this plot, how is cp related to model flexibility?68 Note that in this chapter, we will refer to \\(f(x)\\) as the regression function instead of \\(\\mu(x)\\) for unimportant and arbitrary reasons.↩︎ Someday, someone will tell you this is a lie. They aren’t wrong. In modern deep learning, there is a concept called Deep Double Descent. See also Mikhail Belkin et al., “Reconciling Modern Machine Learning Practice and the Bias-Variance Trade-Off,” arXiv Preprint arXiv:1812.11118, 2018, https://arxiv.org/abs/1812.11118.↩︎ It’s actually the same as the 0 predictor linear model. Can you see why?↩︎ In practice we prefer RMSE to MSE for comparing models and reporting because of the units.↩︎ In practice, if you already know how your model’s flexibility works, by checking that the training error goes down as you increase flexibility, you can check that you have done your coding and model training correctly.↩︎ As cp increases, model flexibility decreases.↩︎ "],["regression-overview.html", "Chapter 5 Regression Overview 5.1 The Goal 5.2 General Strategy 5.3 Aglorithms 5.4 Model Flexibility 5.5 Overfitting 5.6 Bias-Variance Tradeoff 5.7 No Free Lunch 5.8 Curse of Dimensionality", " Chapter 5 Regression Overview This chapter will provide an overview of the regression concepts that we have learned thus far. It will also serve to outline the general concepts of supervised learning which will also apply to our next task, classification. Specifically, we’ll discuss: A review of the key concepts introduced thus far. Overfitting, underfitting and how to identify when they occur. The no free lunch concept as it applies to machine learning. The curse of dimensionality and how it influences modeling decision we make. This chapter is currently under construction. While it is being developed, the following links to the STAT 432 course notes. Notes: Supervised - Regression 5.1 The Goal What is the goal of regression models in the context of machine learning? We can discuss it in two ways: Make predictions on unseen data.69 Estimate the regression function, which under squared error loss is the conditional mean of \\(Y\\), the response, given \\(X\\), the features. These goal are essentially the same. We want to fit a model that “generalizes” well, that is, works well on new data that was not used to fit the model. To do this, we want to use a model of appropriate flexibility so as not to overfit to the training data. In other words, we want to train a model that learns the signal, the regression function, and not the noise, the random variation in the training data. In previous chapters we have formalized this goal a bit more mathematically, but for a general recap, we stick to more casual language. 5.2 General Strategy How do we find and train models that generalize well to unseen data? We generally follow these steps. Split the data in to training data and testing data. Within the training data, we will do whatever we want. The testing data will never be used to make any decision that lead to the selection of a model. We often use 80% of the available data for training. Split the training data into estimation data and validation data. We often use 80% of the available data for estimation. Decide on a set of candidate models. For parametric models: We assume a form of the model up to but not including the model parameters. For nonparametric models: We pick feature variables to be used and possible values of any tuning parameters. Fit (train) each candidate model on the estimation data. Evaluate all candidate models fit to the estimation data based on their performance on the validation data. Performance here is based on the ability of the model to predict on the validation data which was not used to train the models. Select one of the candidate models based on the validation performance. Fit the chosen model to the entire training dataset. Estimate model performance using the test data. Note that we are using the validation data to select a model, while the test data is used to estimate model performance. 5.3 Aglorithms While there are many, many models that can be used for regression, we have focused on three “families” of models. We saw how each can be used to estimate the regression function. Importantly, each model family can be made more or less flexible to accommodate different datasets in order to find a model that predicts well. We are possibly being a little loose with the way we use the term “model” here. It might be more helpful to think in terms of algorithms. A supervised machine learning algorithm for regression takes as input: A dataset. Additional information such as tuning parameter values or the form of a parametric modeling technique. A supervised machine learning algorithm for regression outputs: A fitted model. In particular, a fitted model that can be used to estimate the regression function which is used to make predictions. So, to oversimplify: Algorithms take as input data and output a model. That model takes as input new data and outputs predictions! 5.3.0.1 Linear Models Linear models are a family of parametric models which assume that the regression function is a linear combination of the features. For example, with a single feature \\(x\\), we could assume \\[ \\mu(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\ldots + \\beta_9 x^9 \\] Here, the \\(\\beta\\) coefficients are model parameters that are learned from the data via least squares or maximum likelihood.70 5.3.0.2 k-Nearest Neighbors k-Nearest Neighbors models are a family of nonparametric models with a single tuning parameter, \\(k\\), the number of neighbors to use when estimating the regression function. We can also control which features are used when calculating distances, and how distance is measured. 5.3.0.3 Decision Trees Decision Tree models are a family of nonparametric models with a number of tuning parameters, but most notably, cp, the “complexity parameter” which indirectly controls the number of splits used to create neighborhoods of observations. We can also control which features are used when considering splits. 5.4 Model Flexibility The plot below shows how train and validation “error” change as a function of model flexibility. The “error” in this plot could be any reasonable error metric used, for example, RMSE. While here we are specifically discussing estimation and validation error, we more generally are discussing an error metric calculated on the same data used to train the model (estimation) and an error metric calculated on data not used to train the model, for example the validation data. The “line” and “curve” seen above are highly idealized, that is, you won’t see nice linear and quadratic trends in practice. However, you will see that training error decreases as model flexibility increases.71 On the other hand, often we will see that validation error first decreases, then increases as model flexibility increases.72 While the validation “curve” is idealized as a “smooth curve” that decreases then increases, in practice, it might be a bit more “jagged” just due to the random nature of the validation split. How can we modify the flexibility of the models we have considered? 5.4.0.1 Linear Models To increase the flexibility of linear models, add additional transformed features or simply add features. For example a model that assumes a quadratic mean function \\[ \\mu_1(x) = \\beta_0 + \\beta_1 x + \\beta_2 x ^ 2 \\] is more flexible than a model that assumes a linear mean function \\[ \\mu_2(x) = \\beta_0 + \\beta_1 x \\] The model that assumes a quadratic mean function can learn a linear mean function, but this added flexibility comes with a price. (Possible overfitting.) Similarly, a model that assumes the mean function is a function of two features \\[ \\mu_1(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] is more flexible than a model that assumes the mean function is only a function of one of these features. \\[ \\mu_2(x) = \\beta_0 + \\beta_1 x \\] 5.4.0.2 k-Nearest Neighbors Given a set of feature variables, as \\(k\\) increases, model flexibility decreases.73 5.4.0.3 Decision Given a set of feature variables, as cp increases, model flexibility decreases.74 5.5 Overfitting Overfitting occurs when we have fit to not just the signal but also the noise. That is, a model performs too well on the training data. Let’s take a look at this visually. In each of the plots above, the dashed black curve represents the true mean function of interest, in this case, \\[ \\mu(x) = \\sin(x) \\] with points simulated about this mean according to a standard normal. That is, the noise is standard normal. We see that the model with \\(k = 1\\) has fit far too well to the training data. The estimated mean function, seen in green, goes through each training point. That is, there is no training error. This model is too flexible and is overfitting. We have learned both the signal and the noise, thus this model will predict poorly on new data. The model with \\(k = 25\\) has fits the training data poorly. The estimated mean function, seen in red, does not match the true mean function well. The points are far from the estimated mean function. his model is too inflexible and is underfitting. It has learned neither the signal not the noise. The model with \\(k = 5\\) seems like a reasonable in-between. Doesn’t seem to be chasing noise. Seems to reasonably approximate the true mean function. How do we assess over and underfitting in practice, when we don’t know the true mean function? We have to look at train and validation errors. Models that are probably underfitting: “Large” Train RMSE and a Validation RMSE larger than the smallest. The less flexible, the more probable the underfitting. Models that are probably overfitting: “Small” Train RMSE and a Validation RMSE larger than the smallest. The more flexible, the more probable the overfitting. The further a model is to the left of this plot, the greater the chance it is underfit. The further a model is to the right of this plot, the greater the chance it is overfit. 5.6 Bias-Variance Tradeoff Why does changing the model flexibility influence the predictive performance of these models? The bias-variance tradeoff. As models increase in flexibility, bias is decreased variance is increased And together, the MSE is equal to the bias squared plus the variance. However, the rate at which the variance increases can be and generally is different than the rate at which the bias deceases. This is why we must validate our models. Essentially, by modifying the model complexity and validating the results, we are trying to find the right balance between bias and variance. 5.7 No Free Lunch The no free lunch concept has some very specific formulations, but we will introduce the general idea as it is often applied in supervised learning: No algorithm will outperform all other algorithms over all possible datasets. So in theory: We cannot simply find one algorithm that we know will always be the best. In practice: We must validate our models! While for a particular dataset, without going through the validation process we cannot know ahead of time what algorithm will perform best, later in this text when we get to more practical applications, we will nudge you towards some methods more than others. 5.8 Curse of Dimensionality One topic that we have avoided is the issue of high dimensional problems, that is, data that contains a large number of features. Moving into very high dimensions brings an issue which is often called the curse of dimensionality.75 Stated very simply: In high dimensions any particular datapoint has no “close” neighbors.76 Why might that be a problem? Well, because most nonparametric methods rely on using observations that are “close” to each other. “Nearest neighbors” is literally in the name of k-nearest neighbors! Parametric methods may be less effected, especially when we assume a form of the model that is close to correct, but later we will see methods that are better equipped to handle this issue. Remember, we are looking for predictive relationships. We are not necessarily attempting to explain this relationship and we are certainly not going to attempt to make statements about causality.↩︎ Additionally, we could assume a conditional normal distribution with a constant variance, which would require estimating the \\(\\sigma\\) parameters. This is not necessary to estimate the mean.↩︎ Again, this is essentially one of the few use cases for actually calculating training error, to verify this relationship as a sanity check.↩︎ Sometimes you might see only an increase or decrease which would suggest you need to also try additional models with more or less flexibility.↩︎ Note that adding and removing features does have an effect on model flexibility, generally adding flexibility with additional features, but there are situations where adding features will increase training error.↩︎ Note that adding and removing features does have an effect on model flexibility, generally adding flexibility with additional features.↩︎ Wikipedia: Curse of Dimensionality↩︎ In low dimensions, say \\(p = 1\\) all points live on a line, so in some sense they’re already close together. In “high” dimensions, say for examples \\(p = 20\\), there just a lot more “space,” so observations are much less likely to live “close” to each other. Note that in practice what is considered a “high” dimensional problem is based on the relationship between \\(n\\) and \\(p\\). With a big enough samples size, we can deal with large \\(p\\).↩︎ "],["classification.html", "Chapter 6 Classification 6.1 R Setup and Source 6.2 Data Setup 6.3 Mathematical Setup 6.4 Example 6.5 Bayes Classifier 6.6 Building a Classifier 6.7 Modeling 6.8 Classification Metrics", " Chapter 6 Classification This chapter continues our discussion of supervised learning by introducing the classification tasks. Like regression, we will focus on the conditional distribution of the response. Specifically, we will discuss: The setup for the classification task. The Bayes classifier and Bayes error. Estimating conditional probabilities. Two simple metrics for the classification task. This chapter is currently under construction. While it is being developed, the following links to the STAT 432 course notes. Notes: Classification 6.1 R Setup and Source library(tibble) # data frame printing library(dplyr) # data manipulation library(knitr) # creating tables library(kableExtra) # styling tables Additionally, objects from ggplot2, GGally, and ISLR are accessed. Recall that the Welcome chapter contains directions for installing all necessary packages for following along with the text. The R Markdown source is provided as some code, mostly for creating plots, has been suppressed from the rendered document that you are currently reading. R Markdown Source: classification.Rmd 6.2 Data Setup TODO: Add data setup example. 6.3 Mathematical Setup 6.4 Example \\(X = 1\\) \\(X = 2\\) \\(X = 3\\) \\(X = 4\\) \\(Y = A\\) 0.12 0.01 0.04 0.14 \\(Y = B\\) 0.05 0.03 0.10 0.15 \\(Y = C\\) 0.09 0.06 0.08 0.13 \\(X = 1\\) \\(X = 2\\) \\(X = 3\\) \\(X = 4\\) 0.26 0.1 0.22 0.42 \\(Y = A\\) \\(Y = B\\) \\(Y = C\\) 0.31 0.33 0.36 6.5 Bayes Classifier \\[ p_k(x) = P\\left[ Y = k \\mid X = x \\right] \\] \\[ C^B(x) = \\underset{k \\in \\{1, 2, \\ldots K\\}}{\\text{argmax}} P\\left[ Y = k \\mid X = x \\right] \\] Warning: The Bayes classifier should not be confused with a naive Bayes classifier. The Bayes classifier assumes that we know \\(P\\left[ Y = k \\mid X = x \\right]\\) which is almost never known in practice. A naive Bayes classifier is a method we will see later that learns a classifier from data.77 6.5.1 Bayes Error Rate \\[ 1 - \\mathbb{E}_X\\left[ \\underset{k}{\\text{max}} \\ P[Y = k \\mid X = x] \\right] \\] 6.6 Building a Classifier \\[ \\hat{p}_k(x) = \\hat{P}\\left[ Y = k \\mid X = x \\right] \\] \\[ \\hat{C}(x) = \\underset{k \\in \\{1, 2, \\ldots K\\}}{\\text{argmax}} \\hat{p}_k(x) \\] TODO: first estimation conditional distribution, then classify to label with highest probability TODO: do it in r with knn, tree, or glm / nnet TODO: note about estimating probabilities vs training a classifier 6.7 Modeling 6.7.1 Linear Models TODO: use nnet::multinom in place of glm()? always? 6.7.2 k-Nearest Neighbors TODO: use caret::knn3() 6.7.3 Decision Trees TODO: use rpart::rpart() 6.8 Classification Metrics Like regression, classification metrics will depend on the learned function (in this case, the learned classifier \\(\\hat{C}\\)) as well as an additional dataset that is being used to make classifications with the learned function.78 Like regression, this will make the notation for a “simple” metric like accuracy look significantly more complicated than it truly is, but it will be helpful to make the dependency on the datasets explicit. 6.8.1 Misclassification \\[ \\text{miclass}\\left(\\hat{C}_{\\texttt{set_f}}, \\mathcal{D}_{\\texttt{set_D}} \\right) = \\frac{1}{n_{\\texttt{set_D}}}\\displaystyle\\sum_{i \\in {\\texttt{set_D}}}^{} I\\left(y_i \\neq \\hat{C}_{\\texttt{set_f}}({x}_i)\\right) \\] calc_misclass = function(actual, predicted) { mean(actual != predicted) } 6.8.2 Accuracy \\[ \\text{accuracy}\\left(\\hat{C}_{\\texttt{set_f}}, \\mathcal{D}_{\\texttt{set_D}} \\right) = \\frac{1}{n_{\\texttt{set_D}}}\\displaystyle\\sum_{i \\in {\\texttt{set_D}}}^{} I\\left(y_i = \\hat{C}_{\\texttt{set_f}}({x}_i)\\right) \\] calc_accuracy = function(actual, predicted) { mean(actual == predicted) } Plugging in the appropriate dataset will allow for calculation of train, test, and validation metrics. This author feels that the use of “Bayes” in “Bayes classifier” is actually confusing because you don’t actually need to apply Bayes’ theorem to state or understand the result. Meanwhile, Bayes’ theorem is needed to understand the naive Bayes classifier. Oh well. Naming things is hard.↩︎ The metrics also implicitly depend on the dataset used to learn the classifier.↩︎ "],["nonparametric-classification.html", "Chapter 7 Nonparametric Classification 7.1 Example: KNN on Simulated Data 7.2 Example: Decision Tree on Penguin Data", " Chapter 7 Nonparametric Classification Full book chapter still delayed! Keeping up with writing every week is getting tough. Below are the notes and code from the video. Notes: Nonparametric Classification # load packages library(tibble) library(ggplot2) library(rpart) library(rpart.plot) library(caret) ## Loading required package: lattice library(palmerpenguins) library(mlbench) # set seed set.seed(42) # generate data sim_data = as_tibble(mlbench.2dnormals(n = 1000, cl = 3, sd = 1.3)) # tst-trn split data trn_idx = sample(nrow(sim_data), size = 0.8 * nrow(sim_data)) trn = sim_data[trn_idx, ] tst = sim_data[-trn_idx, ] # est-val split data est_idx = sample(nrow(trn), size = 0.8 * nrow(trn)) est = trn[est_idx, ] val = trn[-est_idx, ] # check data trn ## # A tibble: 800 x 3 ## x.1 x.2 classes ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.929 2.32 1 ## 2 -3.25 2.27 2 ## 3 -0.767 1.06 2 ## 4 3.42 3.10 1 ## 5 2.37 0.786 3 ## 6 3.90 1.54 1 ## 7 -0.373 -1.89 2 ## 8 -1.10 -1.58 2 ## 9 0.609 1.29 1 ## 10 2.31 1.49 1 ## # … with 790 more rows # visualize data p1 = ggplot(data = trn, aes(x = x.1)) + geom_density(aes(fill = classes), alpha = 0.5) + scale_fill_manual(values=c(&quot;grey&quot;, 2, 3)) p2 = ggplot(data = trn, aes(x = x.2)) + geom_density(aes(fill = classes), alpha = 0.5) + scale_fill_manual(values=c(&quot;grey&quot;, 2, 3)) p3 = ggplot(data = trn, aes(x = x.1)) + geom_histogram(aes(fill = classes), alpha = 0.7, position = &quot;identity&quot;) + scale_fill_manual(values=c(&quot;grey&quot;, 2, 3)) p4 = ggplot(data = trn, aes(x = x.2)) + geom_histogram(aes(fill = classes), alpha = 0.7, position = &quot;identity&quot;) + scale_fill_manual(values=c(&quot;grey&quot;, 2, 3)) gridExtra::grid.arrange(p1, p2, p3, p4) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. plot(x.2 ~ x.1, data = trn, col = classes, pch = 20, cex = 1.5) grid() # fit knn model mod_knn = knn3(classes ~ ., data = trn, k = 10) # make &quot;predictions&quot; with knn model new_obs = data.frame(x.1 = 2, x.2 = -2) predict(mod_knn, new_obs, type = &quot;prob&quot;) ## 1 2 3 ## [1,] 0 0 1 predict(mod_knn, new_obs, type = &quot;class&quot;) ## [1] 3 ## Levels: 1 2 3 # fit tree model mod_tree = rpart(classes ~ ., data = trn, minsplit = 5) # make &quot;predictions&quot; with knn model new_obs = data.frame(x.1 = 3, x.2 = 3) predict(mod_tree, new_obs, type = &quot;prob&quot;) ## 1 2 3 ## 1 0.7508772 0.122807 0.1263158 predict(mod_tree, new_obs, type = &quot;class&quot;) ## 1 ## 1 ## Levels: 1 2 3 # visualize tree results par(mfrow = c(1, 2)) plot(x.2 ~ x.1, data = trn, col = classes, pch = 20, cex = 1.5) grid() rpart.plot(mod_tree, type = 2, box.palette = list(&quot;Grays&quot;, &quot;Reds&quot;, &quot;Greens&quot;)) # helper function to calculate misclassification calc_misclass = function(actual, predicted) { mean(actual != predicted) } # calculate test metric mod_knn = knn3(classes ~ ., data = trn, k = 10) calc_misclass( actual = tst$classes, predicted = predict(mod_knn, tst, type = &quot;class&quot;) ) ## [1] 0.215 mean(tst$classes != predict(mod_knn, tst, type = &quot;class&quot;)) ## [1] 0.225 7.1 Example: KNN on Simulated Data # tune knn model ############################################################### # set seed set.seed(42) # k values to consider k_val = seq(1, 101, by = 2) # function to fit knn to est for various k fit_knn_to_est = function(k) { knn3(classes ~ ., data = est, k = k) } # fit models knn_mods = lapply(k_val, fit_knn_to_est) # make predictions knn_preds = lapply(knn_mods, predict, val, type = &quot;class&quot;) # calculate validation misclass knn_misclass = sapply(knn_preds, calc_misclass, actual = val$classes) # plot results plot(k_val, knn_misclass, pch = 20, type = &quot;b&quot;) grid() k_val[which.min(knn_misclass)] ## [1] 55 TODO: add training misclass TODO: expand possible k value TODO: re-fit to training data, report test misclass 7.2 Example: Decision Tree on Penguin Data # trees and penguins ########################################################### # check data penguins ## # A tibble: 344 x 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## 7 Adelie Torgersen 38.9 17.8 181 3625 ## 8 Adelie Torgersen 39.2 19.6 195 4675 ## 9 Adelie Torgersen 34.1 18.1 193 3475 ## 10 Adelie Torgersen 42 20.2 190 4250 ## # … with 334 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; # visualize data par(mfrow = c(1, 2)) plot(body_mass_g ~ flipper_length_mm, data = penguins, col = species, pch = 20, cex = 1.5) grid() plot(bill_length_mm ~ flipper_length_mm, data = penguins, col = species, pch = 20, cex = 1.5) grid() # fit tree and visualize peng_mod = rpart(species ~ . - year, data = penguins) rpart.plot(peng_mod, type = 2, box.palette = list(&quot;Grays&quot;, &quot;Reds&quot;, &quot;Greens&quot;)) # fit bigger tree and visualize peng_mod_big = rpart(species ~ . - year, data = penguins, minsplit = 2, cp = 0) rpart.plot(peng_mod_big, type = 2, box.palette = list(&quot;Grays&quot;, &quot;Reds&quot;, &quot;Greens&quot;)) TODO: tune "],["logistic-regression.html", "Chapter 8 Logistic Regression", " Chapter 8 Logistic Regression Full book chapter still delayed! Keeping up with writing every week is getting tough. Below are the notes from the video. Notes: Logistic Regression # load packages library(tibble) library(mlbench) # single feature variable ###################################################### sim_logistic_data = function(sample_size = 25, beta_0 = -2, beta_1 = 3, factor = TRUE) { x = rnorm(n = sample_size) eta = beta_0 + beta_1 * x p = 1 / (1 + exp(-eta)) y = rbinom(n = sample_size, size = 1, prob = p) if (factor) { y = factor(y) } tibble::tibble(x, y) } # simulate data for logistic regression set.seed(3) sim_data_factor = sim_logistic_data() sim_data_factor levels(sim_data_factor$y) # simulate data for linear regression set.seed(3) sim_data_numeric = sim_logistic_data(factor = FALSE) sim_data_numeric # initial plot plot(y ~ x, data = sim_data_numeric, pch = 19, ylab = &quot;Estimated Probability&quot;, main = &quot;Ordinary vs Logistic Regression&quot;, ylim = c(-0.2, 1.2), cex = 1.5) grid() abline(h = 0, lty = 3) abline(h = 1, lty = 3) # E[Y | X = x] = 1 * P(Y = 1 | X = x) + 0 * P(Y = 0 | X = x) = P(Y = 1 | X = x) # ordinary linear regression fit_lm = lm(y ~ x, data = sim_data_numeric) fit_lm = glm(y ~ x, data = sim_data_numeric) # logistic regression fit_glm = glm(y ~ x, data = sim_data_numeric, family = binomial) fit_glm = glm(y ~ x, data = sim_data_numeric, family = binomial(link = &quot;logit&quot;)) # plot results plot(y ~ x, data = sim_data_numeric, pch = 19, ylab = &quot;Estimated Probability&quot;, main = &quot;Ordinary vs Logistic Regression&quot;, ylim = c(-0.2, 1.2), cex = 1.5) grid() abline(h = 0, lty = 3) abline(h = 1, lty = 3) abline(fit_lm, col = &quot;darkorange&quot;) curve(predict(fit_glm, data.frame(x), type = &quot;response&quot;), add = TRUE, col = &quot;dodgerblue&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;Ordinary&quot;, &quot;Logistic&quot;, &quot;Data&quot;), lty = c(1, 2, 0), pch = c(NA, NA, 20), lwd = 2, col = c(&quot;darkorange&quot;, &quot;dodgerblue&quot;, &quot;black&quot;)) abline(h = 0.5, lty = 2) # abline(v = -coef(fit_glm)[1] / coef(fit_glm)[2], col = &quot;dodgerblue&quot;, lty = 2) # abline(v = (0.5 - coef(fit_lm)[1]) / coef(fit_lm)[2], col = &quot;darkorange&quot;) # two feature variables, blobs ################################################# # simulate data set.seed(42) blob_trn = as_tibble(mlbench.2dnormals(n = 100)) blob_tst = as_tibble(mlbench.2dnormals(n = 1000)) # check data blob_trn levels(blob_trn$classes) # check balance table(blob_trn$classes) # initial plot plot(x.2 ~ x.1, data = blob_trn, col = blob_trn$classes, pch = 19) grid() # points where we will predict xy_vals = expand.grid( x.1 = seq(from = -3.5, to = 3.5, by = 0.05), x.2 = seq(from = -3.5, to = 3.5, by = 0.05) ) head(xy_vals) # fit model, bad mod = glm(classes ~ ., data = blob_trn, family = binomial) pred_xy = ifelse(predict(mod, xy_vals, type = &quot;response&quot;) &gt; 0.5, &quot;lightpink&quot;, &quot;lightgrey&quot;) pred_xy = ifelse(predict(mod, xy_vals) &gt; 0, &quot;lightpink&quot;, &quot;lightgrey&quot;) # check predictions on plot plot(x.2 ~ x.1, data = xy_vals, col = pred_xy, xlim = c(-3, 3), ylim = c(-3, 3), pch = 15) points(x.2 ~ x.1, data = blob_trn, col = blob_trn$classes, pch = 19) # 0 = beta_0 + beta_1 x.1 + beta_2 x.2 # x.2 = -(beta_0 + beta_1 x.1) / beta_2 # add analytic decision boundary plot(x.2 ~ x.1, data = xy_vals, col = pred_xy, xlim = c(-3, 3), ylim = c(-3, 3), pch = 15) points(x.2 ~ x.1, data = blob_trn, col = blob_trn$classes, pch = 19) abline( a = -coef(mod)[1] / coef(mod)[3], b = -coef(mod)[2] / coef(mod)[3], lwd = 5, col = &quot;white&quot;) # check performance, miclassification pred = factor(ifelse(predict(mod, blob_tst) &gt; 0.0, &quot;2&quot;, &quot;1&quot;)) mean(blob_tst$classes != pred) ## two variables, circle ####################################################### # simulate data set.seed(42) circle_trn = as_tibble(mlbench.circle(n = 250)) circle_tst = as_tibble(mlbench.circle(n = 1000)) # check data circle_trn # check balance table(circle_trn$classes) # initial plot plot(x.2 ~ x.1, data = circle_trn, col = circle_trn$classes, pch = 19) grid() # points where we will predict xy_vals = expand.grid( x.1 = seq(from = -1.1, to = 1.1, by = 0.01), x.2 = seq(from = -1.1, to = 1.1, by = 0.01) ) head(xy_vals) # fit model, bad mod_bad = glm(classes ~ ., data = circle_trn, family = binomial) pred_bad_xy = ifelse(predict(mod_bad, xy_vals, type = &quot;response&quot;) &gt; 0.5, &quot;lightpink&quot;, &quot;lightgrey&quot;) # check predictions on plot plot(x.2 ~ x.1, data = xy_vals, col = pred_bad_xy, xlim = c(-1, 1), ylim = c(-1, 1), pch = 15) points(x.2 ~ x.1, data = circle_trn, col = circle_trn$classes, pch = 19) # check performance, accuracy pred_bad = factor(ifelse(predict(mod_bad, circle_tst) &gt; 0.0, &quot;2&quot;, &quot;1&quot;)) mean(circle_tst$classes == pred_bad) # fit model, good mod_good = glm(classes ~ poly(x.1, 2) + poly(x.2, 2) + x.1:x.2, data = circle_trn, family = binomial) pred_good_xy = ifelse(predict(mod_good, xy_vals, type = &quot;response&quot;) &gt; 0.5, &quot;lightpink&quot;, &quot;lightgrey&quot;) # check predictions on plot plot(x.2 ~ x.1, data = xy_vals, col = pred_good_xy, xlim = c(-1, 1), ylim = c(-1, 1), pch = 15) points(x.2 ~ x.1, data = circle_trn, col = circle_trn$classes, pch = 19) # check performance, accuracy pred_good = factor(ifelse(predict(mod_good, circle_tst) &gt; 0.5, &quot;2&quot;, &quot;1&quot;)) mean(circle_tst$classes == pred_good) "],["binary-classification.html", "Chapter 9 Binary Classification 9.1 R Setup and Source 9.2 Breast Cancer Data 9.3 Confusion Matrix 9.4 Binary Classification Metrics 9.5 Probability Cutoff 9.6 R Packages and Function", " Chapter 9 Binary Classification This chapter will introduce no new modeling techniques, but instead will focus on evaluating models for binary classification. Specifically, we will discuss: Using a confusion matrix to summarize the results of a binary classifier. Various metrics for binary classification, including but not limited to: sensitivity, specificity, and prevalence. Using different probability cutoffs to create different classifiers with the same model. This chapter is currently under construction. While it is being developed, the following links to the STAT 432 course notes. Notes: Binary Classification 9.1 R Setup and Source library(ucidata) # access to data library(tibble) # data frame printing library(dplyr) # data manipulation library(caret) # fitting knn library(rpart) # fitting trees Recall that the Welcome chapter contains directions for installing all necessary packages for following along with the text. The R Markdown source is provided as some code, mostly for creating plots, has been suppressed from the rendered document that you are currently reading. R Markdown Source: binary-classification.Rmd 9.2 Breast Cancer Data devtools::install_github(&quot;coatless/ucidata&quot;) # load data bc = na.omit(tibble::as_tibble(bcw_original)) # data prep bc = bc %&gt;% dplyr::mutate(class = factor(class, labels = c(&quot;benign&quot;, &quot;malignant&quot;))) %&gt;% dplyr::select(-sample_code_number) For this example, and many medical testing examples, we will refer to the diseased status, in this case \"Malignant\" as the “Positive” class. Note that this is a somewhat arbitrary choice. # set seed set.seed(42) # test-train split bc_trn_idx = sample(nrow(bc), size = 0.8 * nrow(bc)) bc_trn = bc[bc_trn_idx, ] bc_tst = bc[-bc_trn_idx, ] # estimation-validation split bc_est_idx = sample(nrow(bc_trn), size = 0.8 * nrow(bc_trn)) bc_est = bc_trn[bc_est_idx, ] bc_val = bc_trn[-bc_est_idx, ] # check data head(bc_trn) ## # A tibble: 6 x 10 ## clump_thickness uniformity_of_cell_si… uniformity_of_cell_sh… marginal_adhesi… ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 5 1 2 1 ## 2 8 6 7 3 ## 3 1 2 2 1 ## 4 1 1 2 1 ## 5 10 4 5 5 ## 6 8 8 7 4 ## # … with 6 more variables: single_epithelial_cell_size &lt;int&gt;, ## # bare_nuclei &lt;int&gt;, bland_chromatin &lt;int&gt;, normal_nucleoli &lt;int&gt;, ## # mitoses &lt;int&gt;, class &lt;fct&gt; # inspect response variable levels(bc_trn$class) ## [1] &quot;benign&quot; &quot;malignant&quot; Note that in this case, the positive class also corresponds to the class that logistic regression would view as \\(Y = 1\\) which makes things somewhat simplier to discuss, but these do not actually need to be aligned. 9.3 Confusion Matrix # fit models mod_tree = rpart(class ~ clump_thickness + mitoses, data = bc_est) # obtain prediction using tree for validation data pred_tree = predict(mod_tree, bc_val, type = &quot;class&quot;) Template Confusion Matrix tp = sum(bc_val$class == &quot;malignant&quot; &amp; pred_tree == &quot;malignant&quot;) fp = sum(bc_val$class == &quot;benign&quot; &amp; pred_tree == &quot;malignant&quot;) fn = sum(bc_val$class == &quot;malignant&quot; &amp; pred_tree == &quot;benign&quot;) tn = sum(bc_val$class == &quot;benign&quot; &amp; pred_tree == &quot;benign&quot;) c(tp = tp, fp = fp, fn = fn, tn = tn) ## tp fp fn tn ## 24 7 9 70 # note that these are not in the same positions are the image above # that is OK and almost expected! table( predicted = pred_tree, actual = bc_val$class ) ## actual ## predicted benign malignant ## benign 70 9 ## malignant 7 24 n_obs = nrow(bc_val) pos = tp + fn neg = tn + fp c(n_obs = n_obs, pos = pos, neg = neg) ## n_obs pos neg ## 110 33 77 9.4 Binary Classification Metrics First, before we introduce new metrics, we could re-define previous metrics that we have seen as functions of true and false positives and negatives. Note, we will use \\(P\\) for number of actual positive cases in the dataset under consideration, and \\(N\\) for the number of actual negative cases. \\[ \\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + FP + TN + FN}} = \\frac{\\text{TP + TN}}{\\text{P + N}} \\] (acc = (tp + tn) / (tp + fp + tn + fn)) ## [1] 0.8545455 (acc = mean(bc_val$class == pred_tree)) ## [1] 0.8545455 Here we pause to introduce the prevalence and the no information rate. \\[ \\text{Prevalence} = \\frac{\\text{P}}{\\text{P + N}} \\] (prev = pos / (pos + neg)) ## [1] 0.3 (prev = pos / n_obs) ## [1] 0.3 \\[ \\text{No Information Rate} = \\max\\left(\\frac{\\text{P}}{\\text{P + N}}, \\frac{\\text{N}}{\\text{P + N}}\\right) \\] (nir = max(c(pos / (pos + neg), neg / (pos + neg)))) ## [1] 0.7 (nir = max(c(prev, 1 - prev))) ## [1] 0.7 The prevalence tells us the proportion of the positive class in the data. This is an important baseline for judging binary classifiers, especially as it relates to the no information rate. The no information rate is essentially the proportion of observations that fall into the “majority” class. If a classifier does not achieve an accuracy above this rate, the classifier is performing worse than simply always guessing the majority class! \\[ \\text{Misclassification} = \\frac{\\text{FP + FN}}{\\text{TP + FP + TN + FN}} = \\frac{\\text{FP + FN}}{\\text{P + N}} \\] (mis = (fp + fn) / (tp + fp + tn + fn)) ## [1] 0.1454545 (mis = mean(bc_val$class != pred_tree)) ## [1] 0.1454545 Beyond simply looking at accuracy (or misclassification), when we are specifically concerned with binary classification, there are many more informative metrics that we could consider. First, sensitivity or true positive rate (TPR) looks at the number of observations correctly classified to the positive class, divided by the number of positive cases. In other words, how many of the positive cases did we detect? \\[ \\text{Sensitivity} = \\text{True Positive Rate} = \\frac{\\text{TP}}{\\text{P}} = \\frac{\\text{TP}}{\\text{TP + FN}} \\] (sens = tp / (tp + fn)) ## [1] 0.7272727 A related but somewhat opposite quantity is the specificity or true negative rate. (TNR) \\[ \\text{Specificity} = \\text{True Negative Rate} = \\frac{\\text{TN}}{\\text{N}} = \\frac{\\text{TN}}{\\text{TN + FP}} \\] (spec = tn / (tn + fp)) ## [1] 0.9090909 While we obviously want an accurate classifier, sometimes we care more about sensitivity or specificity. For example, in this cancer example, with \"Malignant\" as the “positive” class, do we care more about sensitivity or specificity? From here, we could define many, many more metrics for binary classification. We first note that there are several easy-to-read Wikipedia articles on this topic. Wikipedia: Confusion Matrix Wikipedia: Sensitivity and Specificity Wikipedia: Precision and Recall Wikipedia: Evaluation of Binary Classifiers As a STAT 432 student you aren’t responsible for memorizing all of these, but based on how their definitions relate to true and false positives and negatives you should be able to calculate any of these metrics. A few more examples: \\[ \\text{Positive Predictive Value} = \\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}} \\] (ppv = tp / (tp + fp)) ## [1] 0.7741935 \\[ \\text{Negative Predictive Value} = \\frac{\\text{TN}}{\\text{TN + FN}} \\] (npv = tn / (tn + fn)) ## [1] 0.8860759 \\[ \\text{False Discovery Rate} = \\frac{\\text{FP}}{\\text{TP + FP}} \\] (fdr = fp / (tp + fp)) ## [1] 0.2258065 What if we cared more about some of these metrics, and want to make our classifiers better on these metrics, possibly at the expense of accuracy? 9.5 Probability Cutoff Recall that we use the notation \\[ \\hat{p}(x) = \\hat{P}(Y = 1 \\mid X = x). \\] And in this example because it is the second level of the response variable, \"malignant\" is considered \\(Y = 1\\). (It also is coincidentally the positive class.) \\[ \\hat{C}(x) = \\begin{cases} 1 &amp; \\hat{p}(x) &gt; 0.5 \\\\ 0 &amp; \\hat{p}(x) \\leq 0.5 \\end{cases} \\] First let’s verify that directly making classifications in R is the same as first obtaining probabilities, and then comparing to a cutoff. # fit tree model mod_tree = rpart(class ~ clump_thickness + mitoses, data = bc_est) # obtain prediction using tree for validation data pred_tree = predict(mod_tree, bc_val, type = &quot;class&quot;) head(predict(mod_tree, bc_val, type = &quot;prob&quot;)) ## benign malignant ## 1 0.08547009 0.9145299 ## 2 0.88235294 0.1176471 ## 3 0.88235294 0.1176471 ## 4 0.88235294 0.1176471 ## 5 0.88235294 0.1176471 ## 6 0.88235294 0.1176471 prob_tree = predict(mod_tree, bc_val, type = &quot;prob&quot;)[, &quot;malignant&quot;] all.equal( factor(ifelse(prob_tree &gt; 0.5, &quot;malignant&quot;, &quot;benign&quot;)), pred_tree) ## [1] TRUE Now let’s switch to using logistic regression. (This is just to make a graph later slightly nice for illustrating a concept.) mod_glm = glm(class ~ clump_thickness + mitoses, data = bc_est, family = &quot;binomial&quot;) prob_glm = predict(mod_glm, bc_val, type = &quot;response&quot;) pred_glm = factor(ifelse(prob_glm &gt; 0.5, &quot;malignant&quot;, &quot;benign&quot;)) We fit the model, obtain probabilities for the \"malignant\" class, then make predictions with the usual cutoff. We then calculate validation accuracy, sensitivity, and specificity. tp = sum(bc_val$class == &quot;malignant&quot; &amp; pred_glm == &quot;malignant&quot;) fp = sum(bc_val$class == &quot;benign&quot; &amp; pred_glm == &quot;malignant&quot;) fn = sum(bc_val$class == &quot;malignant&quot; &amp; pred_glm == &quot;benign&quot;) tn = sum(bc_val$class == &quot;benign&quot; &amp; pred_glm == &quot;benign&quot;) c(acc = (tp + tn) / (tp + fp + tn + fn), sens = tp / (tp + fn), spec = tn / (tn + fp)) ## acc sens spec ## 0.8727273 0.7575758 0.9220779 What if we change the cutoff? \\[ \\hat{C}(x) = \\begin{cases} 1 &amp; \\hat{p}(x) &gt; \\alpha \\\\ 0 &amp; \\hat{p}(x) \\leq \\alpha \\end{cases} \\] For example, if we raise the cutoff, the classifier is going to clasify to \"malignant\" less often. So we can predict that the sensitivity will go down. pred_glm = factor(ifelse(prob_glm &gt; 0.8, &quot;malignant&quot;, &quot;benign&quot;)) tp = sum(bc_val$class == &quot;malignant&quot; &amp; pred_glm == &quot;malignant&quot;) fp = sum(bc_val$class == &quot;benign&quot; &amp; pred_glm == &quot;malignant&quot;) fn = sum(bc_val$class == &quot;malignant&quot; &amp; pred_glm == &quot;benign&quot;) tn = sum(bc_val$class == &quot;benign&quot; &amp; pred_glm == &quot;benign&quot;) c(acc = (tp + tn) / (tp + fp + tn + fn), sens = tp / (tp + fn), spec = tn / (tn + fp)) ## acc sens spec ## 0.8727273 0.6363636 0.9740260 What if we take the cutoff in the other direction? pred_glm = factor(ifelse(prob_glm &gt; 0.2, &quot;malignant&quot;, &quot;benign&quot;)) tp = sum(bc_val$class == &quot;malignant&quot; &amp; pred_glm == &quot;malignant&quot;) fp = sum(bc_val$class == &quot;benign&quot; &amp; pred_glm == &quot;malignant&quot;) fn = sum(bc_val$class == &quot;malignant&quot; &amp; pred_glm == &quot;benign&quot;) tn = sum(bc_val$class == &quot;benign&quot; &amp; pred_glm == &quot;benign&quot;) c(acc = (tp + tn) / (tp + fp + tn + fn), sens = tp / (tp + fn), spec = tn / (tn + fp)) ## acc sens spec ## 0.8363636 0.9090909 0.8051948 Hmmm. We see to really be repeating ourselves a lot… Maybe we should write a function. # note that this is not a generic function # it only works for specific data, and the pos and neg classes we have defined calc_metrics_cutoff = function(probs, cutoff) { pred = factor(ifelse(probs &gt; cutoff, &quot;malignant&quot;, &quot;benign&quot;)) tp = sum(bc_val$class == &quot;malignant&quot; &amp; pred == &quot;malignant&quot;) fp = sum(bc_val$class == &quot;benign&quot; &amp; pred == &quot;malignant&quot;) fn = sum(bc_val$class == &quot;malignant&quot; &amp; pred == &quot;benign&quot;) tn = sum(bc_val$class == &quot;benign&quot; &amp; pred == &quot;benign&quot;) c(acc = (tp + tn) / (tp + fp + tn + fn), sens = tp / (tp + fn), spec = tn / (tn + fp)) } # testing function calc_metrics_cutoff(probs = prob_glm, cutoff = 0.2) ## acc sens spec ## 0.8363636 0.9090909 0.8051948 Now we can try a lot of cutoffs. # trying a bunch of cutoffs cutoffs = seq(from = 0, to = 1, by = 0.01) results = sapply(cutoffs, calc_metrics_cutoff, probs = prob_glm) results = as.data.frame(t(rbind(cutoffs, results))) # checking the results head(results) ## cutoffs acc sens spec ## 1 0.00 0.3000000 1.000000 0.0000000 ## 2 0.01 0.3000000 1.000000 0.0000000 ## 3 0.02 0.5454545 0.969697 0.3636364 ## 4 0.03 0.5454545 0.969697 0.3636364 ## 5 0.04 0.6181818 0.969697 0.4675325 ## 6 0.05 0.6181818 0.969697 0.4675325 # plot the results plot(acc ~ cutoffs, data = results, type = &quot;l&quot;, ylim = c(0, 1), lwd = 2, main = &quot;Accuracy, Sensitivity, Specificity versus Probability Cutoff&quot;) lines(sens ~ cutoffs, data = results, type = &quot;l&quot;, col = &quot;dodgerblue&quot;, lwd = 2, lty = 2) lines(spec ~ cutoffs, data = results, type = &quot;l&quot;, col = &quot;darkorange&quot;, lwd = 2, lty = 3) grid() Here we see the general pattern. We can sacrifice some accuracy for more sensitivity or specificity. There is also a sensitivity-specificity. tradeoff. 9.6 R Packages and Function Use these at your own risk. cvms::evalaute() caret::confusionMatrix() "],["generative.html", "Chapter 10 Generative Models 10.1 R Setup and Source 10.2 Linear Discriminant Analysis 10.3 Quadratic Discriminant Analysis 10.4 Naive Bayes 10.5 Categorical Features", " Chapter 10 Generative Models In this chapter, we continue our discussion of classification methods. We introduce three new methods, each a generative method. Specifically, we will discuss: How generative methods are different than discriminative methods like logistic regression. How generative methods model the joint probability, \\(p(\\boldsymbol{x}, y)\\), often by assuming some distribution for the conditional distribution of \\(\\boldsymbol{X}\\) given \\(Y\\), \\(f(\\boldsymbol{x} \\mid y)\\). How to use Bayes theorem to classify according to \\(p(y \\mid \\boldsymbol{x})\\) as compared to discriminative methods such as logistic regression directly model this conditional directly. Two potential additional readigns: Ng and Jordan, 2002. ISL Chapter 4, Sections 4 This chapter is currently under construction. While it is being developed, the following links to the STAT 432 course notes. Notes: Generative Models 10.1 R Setup and Source library(palmerpenguins) # access to data library(tibble) # data frame printing library(MASS) # fitting lda and qda library(klaR) # fitting naive bayes library(knitr) # creating tables library(kableExtra) # styling tables Recall that the Welcome chapter contains directions for installing all necessary packages for following along with the text. The R Markdown source is provided as some code, mostly for creating plots, has been suppressed from the rendered document that you are currently reading. R Markdown Source: generative.Rmd Each of the methods in this chapter will use Bayes theorem to build a classifier. \\[ p_k(\\boldsymbol{x}) = P(Y = k \\mid \\boldsymbol{X} = \\boldsymbol{x}) = \\frac{\\pi_k \\cdot f_k(\\boldsymbol{x})}{\\sum_{g = 1}^{G} \\pi_g \\cdot f_g(\\boldsymbol{x})} \\] We call \\(p_k(\\boldsymbol{x})\\) the posterior probability, which we will estimate then use to create classifications. The \\(\\pi_g\\) are called the prior probabilities for each possible classes \\(g\\). That is, \\(\\pi_g = P(Y = g)\\), unconditioned on \\(\\boldsymbol X\\). (Here, there are \\(G\\) possible classes, denoted \\(1, 2, \\ldots G\\). We use \\(k\\) to refer to a particular class.) The \\(f_g(x)\\) are called the likelihoods, which are indexed by \\(g\\) to denote that they are conditional on the classes. The denominator is often referred to as a normalizing constant. The methods will differ by placing different modeling assumptions on the likelihoods, \\(f_g(\\boldsymbol x)\\). For each method, the priors could be learned from data or pre-specified. For each method, classifications are made to the class with the highest estimated posterior probability, which is equivalent to the class with the largest \\[ \\log(\\hat{\\pi}_k \\cdot \\hat{f}_k(\\boldsymbol{x})). \\] By substituting the corresponding likelihoods, simplifying, and eliminating unnecessary terms, we could derive the discriminant function for each. To illustrate these new methods, we return to the Palmer penguins data, which you may remember has three classes. After a train-test and estimation-validation split, we create a number of plots to refresh our memory. Note that these splits are a bit odd in terms of proprotions. This is for illustrative purposes only. peng = na.omit(penguins) # set seed set.seed(2) # train-test split trn_idx = sample(nrow(peng), size = trunc(0.50 * nrow(peng))) peng_trn = peng[trn_idx, ] peng_tst = peng[-trn_idx, ] # train-test split est_idx = sample(nrow(peng_trn), size = trunc(0.50 * nrow(peng_trn))) peng_est = peng_trn[est_idx, ] peng_val = peng_trn[-est_idx, ] caret::featurePlot( x = peng_trn[, c(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, &quot;flipper_length_mm&quot;, &quot;body_mass_g&quot;)], y = peng_trn$species, plot = &quot;density&quot;, scales = list( x = list(relation = &quot;free&quot;), y = list(relation = &quot;free&quot;) ), adjust = 1.5, pch = &quot;|&quot;, layout = c(2, 2), auto.key = list(columns = 3) ) caret::featurePlot( x = peng_trn[, c(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, &quot;flipper_length_mm&quot;, &quot;body_mass_g&quot;)], y = peng_trn$species, plot = &quot;ellipse&quot;, auto.key = list(columns = 3) ) caret::featurePlot( x = peng_trn[, c(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, &quot;flipper_length_mm&quot;, &quot;body_mass_g&quot;)], y = peng_trn$species, plot = &quot;box&quot;, scales = list(y = list(relation = &quot;free&quot;), x = list(rot = 90)), layout = c(4, 1) ) Especially based on the pairs plot, we see that it should not be too difficult to find a good classifier. Because it is so easy will we create models using only bill_length_mm and flipper_length_mm so that they will make some errors that we can discuss. Notice that we use caret::featurePlot to access the featurePlot() function without loading the entire caret package. 10.2 Linear Discriminant Analysis Linear Discriminant Analysis, LDA, assumes that the features are multivariate normal conditioned on the classes. \\[ \\boldsymbol{X} \\mid Y = k \\sim N(\\boldsymbol{\\mu}_k, \\boldsymbol\\Sigma) \\] \\[ f_k(\\boldsymbol{x}) = \\frac{1}{(2\\pi)^{p/2}|\\boldsymbol\\Sigma|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol\\mu_k)^{\\prime}\\boldsymbol\\Sigma^{-1}(\\boldsymbol x - \\boldsymbol\\mu_k)\\right] \\] Notice that \\(\\boldsymbol\\Sigma\\) does not depend on \\(k\\), that is, we are assuming the same \\(\\Sigma\\) for each class. We then use information from all the classes to estimate \\(\\boldsymbol\\Sigma\\). peng_lda = lda(species ~ bill_length_mm + flipper_length_mm, data = peng_est) peng_lda ## Call: ## lda(species ~ bill_length_mm + flipper_length_mm, data = peng_est) ## ## Prior probabilities of groups: ## Adelie Chinstrap Gentoo ## 0.4457831 0.1566265 0.3975904 ## ## Group means: ## bill_length_mm flipper_length_mm ## Adelie 39.08108 190.0000 ## Chinstrap 48.32308 194.9231 ## Gentoo 46.75152 215.9697 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## bill_length_mm 0.03300363 -0.4289076 ## flipper_length_mm 0.18519532 0.1087346 ## ## Proportion of trace: ## LD1 LD2 ## 0.7984 0.2016 Here we see the estimated \\(\\hat{\\pi}_k\\) and \\(\\hat{\\boldsymbol\\mu}_k\\) for each class. is.list(predict(peng_lda, peng_est)) ## [1] TRUE names(predict(peng_lda, peng_est)) ## [1] &quot;class&quot; &quot;posterior&quot; &quot;x&quot; head(predict(peng_lda, peng_est)$class, n = 10) ## [1] Gentoo Adelie Adelie Adelie Adelie Gentoo Chinstrap ## [8] Adelie Adelie Gentoo ## Levels: Adelie Chinstrap Gentoo head(predict(peng_lda, peng_est)$posterior, n = 10) ## Adelie Chinstrap Gentoo ## 1 3.106976e-03 6.281033e-06 9.968867e-01 ## 2 9.988138e-01 1.185724e-03 4.300706e-07 ## 3 9.998919e-01 1.066133e-04 1.517790e-06 ## 4 9.828029e-01 1.719710e-02 5.064431e-10 ## 5 8.222950e-01 2.184971e-03 1.755200e-01 ## 6 7.514839e-05 7.801291e-06 9.999171e-01 ## 7 3.723269e-02 9.627490e-01 1.830132e-05 ## 8 9.966890e-01 8.415898e-06 3.302621e-03 ## 9 9.999794e-01 2.016388e-05 4.025448e-07 ## 10 3.830348e-09 1.184353e-05 9.999882e-01 As we should come to expect, the predict() function operates in a new way when called on an lda object. By default, it returns an entire list. Within that list class stores the classifications and posterior contains the estimated probability for each class. peng_lda_est_pred = predict(peng_lda, peng_est)$class peng_lda_val_pred = predict(peng_lda, peng_val)$class We store the predictions made on the estimation and validatino sets. calc_misclass = function(actual, predicted) { mean(actual != predicted) } calc_misclass(predicted = peng_lda_est_pred, actual = peng_est$species) ## [1] 0.01204819 calc_misclass(predicted = peng_lda_val_pred, actual = peng_val$species) ## [1] 0.08433735 As expected, LDA performs well on both the estimation and validation data. table(predicted = peng_lda_val_pred, actual = peng_val$species) ## actual ## predicted Adelie Chinstrap Gentoo ## Adelie 31 2 0 ## Chinstrap 1 16 0 ## Gentoo 2 2 29 Looking at the validation set, we see that we are perfectly within the Gentoos. peng_lda_flat = lda(species ~ bill_length_mm + flipper_length_mm, data = peng_est, prior = c(1, 1, 1) / 3) peng_lda_flat ## Call: ## lda(species ~ bill_length_mm + flipper_length_mm, data = peng_est, ## prior = c(1, 1, 1)/3) ## ## Prior probabilities of groups: ## Adelie Chinstrap Gentoo ## 0.3333333 0.3333333 0.3333333 ## ## Group means: ## bill_length_mm flipper_length_mm ## Adelie 39.08108 190.0000 ## Chinstrap 48.32308 194.9231 ## Gentoo 46.75152 215.9697 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## bill_length_mm -0.05945813 -0.42604662 ## flipper_length_mm 0.20416030 0.06662646 ## ## Proportion of trace: ## LD1 LD2 ## 0.6858 0.3142 Instead of learning (estimating) the proportion of the three species from the data, we could instead specify them ourselves. Here we choose a uniform distributions over the possible species. We would call this a “flat” prior. peng_lda_flat_est_pred = predict(peng_lda_flat, peng_est)$class peng_lda_flat_val_pred = predict(peng_lda_flat, peng_val)$class calc_misclass(predicted = peng_lda_flat_est_pred, actual = peng_est$species) ## [1] 0.01204819 calc_misclass(predicted = peng_lda_flat_val_pred, actual = peng_val$species) ## [1] 0.07228916 This actually gives a better test accuracy! In practice, this could be useful if you have prior knowledge about the future proportions of the response variable. table(peng_val$species) / length(peng_val$species) ## ## Adelie Chinstrap Gentoo ## 0.4096386 0.2409639 0.3493976 peng_lda$prior ## Adelie Chinstrap Gentoo ## 0.4457831 0.1566265 0.3975904 Looking at the above, we see this makes sense. In the validation data, the proportions of the classes are closer to flat. However, you should not use this information to choose your prior in practice. That would be cheating. If you have other information that suggests you should try this, then go right ahead. 10.3 Quadratic Discriminant Analysis Quadratic Discriminant Analysis, QDA, also assumes that the features are multivariate normal conditioned on the classes. \\[ \\boldsymbol X \\mid Y = k \\sim N(\\boldsymbol\\mu_k, \\boldsymbol\\Sigma_k) \\] \\[ f_k(\\boldsymbol x) = \\frac{1}{(2\\pi)^{p/2}|\\boldsymbol\\Sigma_k|^{1/2}}\\exp\\left[-\\frac{1}{2}(\\boldsymbol x - \\boldsymbol\\mu_k)^{\\prime}\\boldsymbol\\Sigma_{k}^{-1}(\\boldsymbol x - \\boldsymbol\\mu_k)\\right] \\] Notice that now \\(\\boldsymbol\\Sigma_k\\) does depend on \\(k\\), that is, we are allowing a different \\(\\boldsymbol\\Sigma_k\\) for each class. We only use information from class \\(k\\) to estimate \\(\\Sigma_k\\). peng_qda = qda(species ~ bill_length_mm + flipper_length_mm, data = peng_est) peng_qda ## Call: ## qda(species ~ bill_length_mm + flipper_length_mm, data = peng_est) ## ## Prior probabilities of groups: ## Adelie Chinstrap Gentoo ## 0.4457831 0.1566265 0.3975904 ## ## Group means: ## bill_length_mm flipper_length_mm ## Adelie 39.08108 190.0000 ## Chinstrap 48.32308 194.9231 ## Gentoo 46.75152 215.9697 Here the output is similar to LDA, again giving the estimated \\(\\hat{\\pi}_k\\) and \\(\\hat{\\boldsymbol\\mu}_k\\) for each class. Like lda(), the qda() function is found in the MASS package. Consider trying to fit QDA again, but this time with a very small estimation set. This will cause an error because there are not enough observations within each class to estimate the large number of parameters in the \\(\\boldsymbol\\Sigma_k\\) matrices. This is less of a problem with LDA, since all observations, no matter the class, are being use to estimate the shared \\(\\boldsymbol\\Sigma\\) matrix. peng_qda_est_pred = predict(peng_qda, peng_est)$class peng_qda_val_pred = predict(peng_qda, peng_val)$class The predict() function operates the same as the predict() function for LDA. calc_misclass(predicted = peng_qda_est_pred, actual = peng_est$species) ## [1] 0.01204819 calc_misclass(predicted = peng_qda_val_pred, actual = peng_val$species) ## [1] 0.09638554 table(predicted = peng_qda_val_pred, actual = peng_val$species) ## actual ## predicted Adelie Chinstrap Gentoo ## Adelie 30 1 0 ## Chinstrap 1 16 0 ## Gentoo 3 3 29 Here we see that QDA has similar performance to LDA, but a little worse. This isn’t too surprising as based on the plots, the covariance within each of the classes seems similar. QDA may be too flexible here. Since QDA is a more flexible model than LDA (it has many more parameters), QDA is more likely to overfit than LDA. Also note that, QDA creates quadratic decision boundaries, while LDA creates linear decision boundaries. We could also add quadratic terms to LDA to allow it to create quadratic decision boundaries. 10.4 Naive Bayes Naive Bayes comes in many forms. With only numeric features, it often assumes a multivariate normal conditioned on the classes, but a very specific multivariate normal. \\[ {\\boldsymbol X} \\mid Y = k \\sim N(\\boldsymbol\\mu_k, \\boldsymbol\\Sigma_k) \\] Naive Bayes assumes that the features \\(X_1, X_2, \\ldots, X_p\\) are independent given \\(Y = k\\). This is the “naive” part of naive Bayes. The Bayes part is nothing new. Since \\(X_1, X_2, \\ldots, X_p\\) are assumed independent, each \\(\\boldsymbol\\Sigma_k\\) is diagonal, that is, we assume no correlation between features. Independence implies zero correlation. This will allow us to write the (joint) likelihood as a product of univariate distributions. In this case, the product of univariate normal distributions instead of a (joint) multivariate distribution. \\[ f_k(\\boldsymbol x) = \\prod_{j = 1}^{p} f_{kj}(\\boldsymbol x_j) \\] Here, \\(f_{kj}(\\boldsymbol x_j)\\) is the density for the \\(j\\)-th feature conditioned on the \\(k\\)-th class. Notice that there is a \\(\\sigma_{kj}\\) for each feature for each class. \\[ f_{kj}(\\boldsymbol x_j) = \\frac{1}{\\sigma_{kj}\\sqrt{2\\pi}}\\exp\\left[-\\frac{1}{2}\\left(\\frac{x_j - \\mu_{kj}}{\\sigma_{kj}}\\right)^2\\right] \\] When \\(p = 1\\), this version of naive Bayes is equivalent to QDA. peng_nb = NaiveBayes(species ~ bill_length_mm + flipper_length_mm, data = peng_est) peng_nb$apriori ## grouping ## Adelie Chinstrap Gentoo ## 0.4457831 0.1566265 0.3975904 peng_nb$tables ## $bill_length_mm ## [,1] [,2] ## Adelie 39.08108 2.560907 ## Chinstrap 48.32308 2.120595 ## Gentoo 46.75152 2.768474 ## ## $flipper_length_mm ## [,1] [,2] ## Adelie 190.0000 4.960959 ## Chinstrap 194.9231 3.729646 ## Gentoo 215.9697 5.849599 Many packages implement naive Bayes. Here we choose to use NaiveBayes() from the package klaR. The output from peng_nb$tables gives the mean and standard deviation of the normal distribution for each feature in each class. Notice how these mean estimates match those for LDA and QDA above. head(predict(peng_nb, peng_est)$class) ## [1] Gentoo Adelie Adelie Adelie Adelie Gentoo ## Levels: Adelie Chinstrap Gentoo head(predict(peng_nb, peng_est)$posterior) ## Adelie Chinstrap Gentoo ## [1,] 1.617414e-03 1.961748e-05 9.983630e-01 ## [2,] 9.999804e-01 1.929359e-05 2.701873e-07 ## [3,] 9.999974e-01 2.282747e-06 2.994029e-07 ## [4,] 9.999981e-01 1.857175e-06 2.992396e-09 ## [5,] 8.417837e-01 1.069401e-02 1.475223e-01 ## [6,] 3.308450e-06 2.470560e-06 9.999942e-01 peng_nb_est_pred = predict(peng_nb, peng_est)$class peng_nb_val_pred = predict(peng_nb, peng_val)$class ## Warning in FUN(X[[i]], ...): Numerical 0 probability for all classes with ## observation 46 calc_misclass(predicted = peng_nb_est_pred, actual = peng_est$species) ## [1] 0.01204819 calc_misclass(predicted = peng_nb_val_pred, actual = peng_val$species) ## [1] 0.1325301 table(predicted = peng_nb_val_pred, actual = peng_val$species) ## actual ## predicted Adelie Chinstrap Gentoo ## Adelie 31 2 0 ## Chinstrap 1 12 0 ## Gentoo 2 6 29 Here we see worse performance again. Method Train Error Validation Error LDA 0.0120482 0.0843373 LDA, Flat Prior 0.0120482 0.0722892 QDA 0.0120482 0.0963855 Naive Bayes 0.0120482 0.1325301 Summarizing the results, we see that Naive Bayes is the worst of LDA, QDA, and NB for this data. This isn’t surprising as there is clear dependence within the features. So why should we care about naive Bayes? The strength of Naive Bayes comes from its ability to handle a large number of features, \\(p\\), even with a limited sample size \\(n\\). Even with the naive independence assumption, Naive Bayes works rather well in practice. Also because of this assumption, we can often train naive Bayes where LDA and QDA may be impossible to train because of the large number of parameters relative to the number of observations. Here naive Bayes doesn’t get a chance to show its strength since LDA and QDA already perform well, and the number of features is low. The choice between LDA and QDA is mostly down to a consideration about the amount of complexity needed. (Also note that complexity within these models can also be altered by changing the features used. More features generally means a more flexible model.) 10.5 Categorical Features So far, we have assumed that all features are numeric. What happens with categorical features? Let’s add the sex variable which is categorical. NaiveBayes(species ~ bill_length_mm + flipper_length_mm + sex, data = peng_est)$tables ## $bill_length_mm ## [,1] [,2] ## Adelie 39.08108 2.560907 ## Chinstrap 48.32308 2.120595 ## Gentoo 46.75152 2.768474 ## ## $flipper_length_mm ## [,1] [,2] ## Adelie 190.0000 4.960959 ## Chinstrap 194.9231 3.729646 ## Gentoo 215.9697 5.849599 ## ## $sex ## var ## grouping female male ## Adelie 0.5405405 0.4594595 ## Chinstrap 0.4615385 0.5384615 ## Gentoo 0.5757576 0.4242424 Naive Bayes makes a somewhat obvious and intelligent choice to model the categorical variable as a multinomial. It then estimates the probability parameters of a multinomial distribution. lda(species ~ bill_length_mm + flipper_length_mm + sex, data = peng_est) ## Call: ## lda(species ~ bill_length_mm + flipper_length_mm + sex, data = peng_est) ## ## Prior probabilities of groups: ## Adelie Chinstrap Gentoo ## 0.4457831 0.1566265 0.3975904 ## ## Group means: ## bill_length_mm flipper_length_mm sexmale ## Adelie 39.08108 190.0000 0.4594595 ## Chinstrap 48.32308 194.9231 0.5384615 ## Gentoo 46.75152 215.9697 0.4242424 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## bill_length_mm 0.2652470 -0.4650328 ## flipper_length_mm 0.1800828 0.1383358 ## sexmale -1.8987355 0.7946053 ## ## Proportion of trace: ## LD1 LD2 ## 0.8527 0.1473 LDA (and QDA) however creates dummy variables, here with male as the reference level, then continues to model them as normally distributed. Not great, but better then not using a categorical variable. "],["cross-validation.html", "Chapter 11 Cross-Validation", " Chapter 11 Cross-Validation This chapter will introduce no new modeling techniques, but instead will focus on evaluating models through the use of cross-validation. Specifically, we will discuss: How to “manually” perform cross-validation for any model-metric combination. The need for cross-validation. The need for a separate test set outside of the training data where cross-validation takes place. This chapter is currently under construction. While it is being developed, the following links to the STAT 432 course notes. Notes: Simulation Notes: Bootstrap Notes: Cross-Validation "],["regularization.html", "Chapter 12 Regularization 12.1 Ridge Regression 12.2 Lasso 12.3 broom 12.4 Simulated Data, \\(p &gt; n\\)", " Chapter 12 Regularization Chapter Status: Currently this chapter is very sparse. It essentially only expands upon an example discussed in ISL, thus only illustrates usage of the methods. Mathematical and conceptual details of the methods will be added later. Notes: Cross-Validation We will use the Hitters dataset from the ISLR package to explore two shrinkage methods: ridge regression and lasso. These are otherwise known as penalized regression methods. data(Hitters, package = &quot;ISLR&quot;) This dataset has some missing data in the response Salaray. We use the na.omit() function to clean the dataset for ease-of-use. sum(is.na(Hitters)) ## [1] 59 sum(is.na(Hitters$Salary)) ## [1] 59 Hitters = na.omit(Hitters) sum(is.na(Hitters)) ## [1] 0 The feature variables are offensive and defensive statistics for a number of baseball players. names(Hitters) ## [1] &quot;AtBat&quot; &quot;Hits&quot; &quot;HmRun&quot; &quot;Runs&quot; &quot;RBI&quot; &quot;Walks&quot; ## [7] &quot;Years&quot; &quot;CAtBat&quot; &quot;CHits&quot; &quot;CHmRun&quot; &quot;CRuns&quot; &quot;CRBI&quot; ## [13] &quot;CWalks&quot; &quot;League&quot; &quot;Division&quot; &quot;PutOuts&quot; &quot;Assists&quot; &quot;Errors&quot; ## [19] &quot;Salary&quot; &quot;NewLeague&quot; We use the glmnet() and cv.glmnet() functions from the glmnet package to fit penalized regressions. library(glmnet) Unfortunately, the glmnet function does not allow the use of model formulas, so we setup the data for ease of use with glmnet. Eventually we will use train() from caret which does allow for fitting penalized regression with the formula syntax, but to explore some of the details, we first work with the functions from glmnet directly. X = model.matrix(Salary ~ ., Hitters)[, -1] y = Hitters$Salary Note, we’re being lazy and just using the full dataset as the training dataset. First, we fit an ordinary linear regression, and note the size of the features’ coefficients, and features’ coefficients squared. (The two penalties we will use.) fit = lm(Salary ~ ., Hitters) coef(fit) ## (Intercept) AtBat Hits HmRun Runs RBI ## 163.1035878 -1.9798729 7.5007675 4.3308829 -2.3762100 -1.0449620 ## Walks Years CAtBat CHits CHmRun CRuns ## 6.2312863 -3.4890543 -0.1713405 0.1339910 -0.1728611 1.4543049 ## CRBI CWalks LeagueN DivisionW PutOuts Assists ## 0.8077088 -0.8115709 62.5994230 -116.8492456 0.2818925 0.3710692 ## Errors NewLeagueN ## -3.3607605 -24.7623251 sum(abs(coef(fit)[-1])) ## [1] 238.7295 sum(coef(fit)[-1] ^ 2) ## [1] 18337.3 12.1 Ridge Regression We first illustrate ridge regression, which can be fit using glmnet() with alpha = 0 and seeks to minimize \\[ \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right) ^ 2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 . \\] Notice that the intercept is not penalized. Also, note that that ridge regression is not scale invariant like the usual unpenalized regression. Thankfully, glmnet() takes care of this internally. It automatically standardizes predictors for fitting, then reports fitted coefficient using the original scale. The two plots illustrate how much the coefficients are penalized for different values of \\(\\lambda\\). Notice none of the coefficients are forced to be zero. par(mfrow = c(1, 2)) fit_ridge = glmnet(X, y, alpha = 0) plot(fit_ridge) plot(fit_ridge, xvar = &quot;lambda&quot;, label = TRUE) We use cross-validation to select a good \\(\\lambda\\) value. The cv.glmnet()function uses 10 folds by default. The plot illustrates the MSE for the \\(\\lambda\\)s considered. Two lines are drawn. The first is the \\(\\lambda\\) that gives the smallest MSE. The second is the \\(\\lambda\\) that gives an MSE within one standard error of the smallest. fit_ridge_cv = cv.glmnet(X, y, alpha = 0) plot(fit_ridge_cv) The cv.glmnet() function returns several details of the fit for both \\(\\lambda\\) values in the plot. Notice the penalty terms are smaller than the full linear regression. (As we would expect.) # estimated coefficients, using 1-SE rule lambda, default behavior coef(fit_ridge_cv) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 199.418113624 ## AtBat 0.093426871 ## Hits 0.389767263 ## HmRun 1.212875007 ## Runs 0.623229048 ## RBI 0.618547529 ## Walks 0.810467707 ## Years 2.544170910 ## CAtBat 0.007897059 ## CHits 0.030554662 ## CHmRun 0.226545984 ## CRuns 0.061265846 ## CRBI 0.063384832 ## CWalks 0.060720300 ## LeagueN 3.743295031 ## DivisionW -23.545192292 ## PutOuts 0.056202373 ## Assists 0.007879196 ## Errors -0.164203267 ## NewLeagueN 3.313773161 # estimated coefficients, using minimum lambda coef(fit_ridge_cv, s = &quot;lambda.min&quot;) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 8.112693e+01 ## AtBat -6.815959e-01 ## Hits 2.772312e+00 ## HmRun -1.365680e+00 ## Runs 1.014826e+00 ## RBI 7.130225e-01 ## Walks 3.378558e+00 ## Years -9.066800e+00 ## CAtBat -1.199478e-03 ## CHits 1.361029e-01 ## CHmRun 6.979958e-01 ## CRuns 2.958896e-01 ## CRBI 2.570711e-01 ## CWalks -2.789666e-01 ## LeagueN 5.321272e+01 ## DivisionW -1.228345e+02 ## PutOuts 2.638876e-01 ## Assists 1.698796e-01 ## Errors -3.685645e+00 ## NewLeagueN -1.810510e+01 # penalty term using minimum lambda sum(coef(fit_ridge_cv, s = &quot;lambda.min&quot;)[-1] ^ 2) ## [1] 18367.29 # estimated coefficients, using 1-SE rule lambda coef(fit_ridge_cv, s = &quot;lambda.1se&quot;) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 199.418113624 ## AtBat 0.093426871 ## Hits 0.389767263 ## HmRun 1.212875007 ## Runs 0.623229048 ## RBI 0.618547529 ## Walks 0.810467707 ## Years 2.544170910 ## CAtBat 0.007897059 ## CHits 0.030554662 ## CHmRun 0.226545984 ## CRuns 0.061265846 ## CRBI 0.063384832 ## CWalks 0.060720300 ## LeagueN 3.743295031 ## DivisionW -23.545192292 ## PutOuts 0.056202373 ## Assists 0.007879196 ## Errors -0.164203267 ## NewLeagueN 3.313773161 # penalty term using 1-SE rule lambda sum(coef(fit_ridge_cv, s = &quot;lambda.1se&quot;)[-1] ^ 2) ## [1] 588.9958 # predict using minimum lambda predict(fit_ridge_cv, X, s = &quot;lambda.min&quot;) # predict using 1-SE rule lambda, default behavior predict(fit_ridge_cv, X) # calculate &quot;train error&quot; mean((y - predict(fit_ridge_cv, X)) ^ 2) ## [1] 130404.9 # CV-MSEs fit_ridge_cv$cvm ## [1] 205333.0 203746.6 203042.6 202800.1 202535.1 202245.8 201930.0 201585.5 ## [9] 201210.0 200800.7 200355.2 199870.4 199343.3 198770.8 198149.6 197476.1 ## [17] 196746.9 195958.5 195106.9 194188.7 193200.1 192137.6 190997.7 189777.3 ## [25] 188473.6 187084.0 185606.8 184040.5 182384.5 180639.3 178805.9 176886.6 ## [33] 174885.0 172805.6 170654.4 168438.7 166167.0 163848.8 161494.9 159117.2 ## [41] 156728.1 154340.9 151968.9 149625.5 147323.7 145076.2 142894.4 140789.0 ## [49] 138769.1 136842.4 135014.7 133290.7 131673.5 130164.1 128762.4 127466.9 ## [57] 126275.0 125183.1 124186.7 123280.9 122460.4 121719.7 121053.1 120454.6 ## [65] 119919.2 119444.3 119024.2 118649.9 118318.3 118028.6 117776.4 117554.6 ## [73] 117360.3 117194.2 117049.5 116921.7 116812.9 116716.9 116632.4 116553.6 ## [81] 116485.1 116419.6 116353.8 116293.9 116229.4 116166.5 116098.3 116029.8 ## [89] 115952.0 115877.2 115786.6 115706.7 115603.7 115516.4 115403.0 115306.9 ## [97] 115188.2 115085.7 114964.5 114865.5 # CV-MSE using minimum lambda fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.min] ## [1] 114865.5 # CV-MSE using 1-SE rule lambda fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.1se] ## [1] 135014.7 12.2 Lasso We now illustrate lasso, which can be fit using glmnet() with alpha = 1 and seeks to minimize \\[ \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right) ^ 2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| . \\] Like ridge, lasso is not scale invariant. The two plots illustrate how much the coefficients are penalized for different values of \\(\\lambda\\). Notice some of the coefficients are forced to be zero. par(mfrow = c(1, 2)) fit_lasso = glmnet(X, y, alpha = 1) plot(fit_lasso) plot(fit_lasso, xvar = &quot;lambda&quot;, label = TRUE) Again, to actually pick a \\(\\lambda\\), we will use cross-validation. The plot is similar to the ridge plot. Notice along the top is the number of features in the model. (Which changed in this plot.) fit_lasso_cv = cv.glmnet(X, y, alpha = 1) plot(fit_lasso_cv) cv.glmnet() returns several details of the fit for both \\(\\lambda\\) values in the plot. Notice the penalty terms are again smaller than the full linear regression. (As we would expect.) Some coefficients are 0. # estimated coefficients, using 1-SE rule lambda, default behavior coef(fit_lasso_cv) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 251.4491565 ## AtBat . ## Hits 1.0036044 ## HmRun . ## Runs . ## RBI . ## Walks 0.9857987 ## Years . ## CAtBat . ## CHits . ## CHmRun . ## CRuns 0.1094218 ## CRBI 0.2911570 ## CWalks . ## LeagueN . ## DivisionW . ## PutOuts . ## Assists . ## Errors . ## NewLeagueN . # estimated coefficients, using minimum lambda coef(fit_lasso_cv, s = &quot;lambda.min&quot;) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 20.9671791 ## AtBat . ## Hits 1.8647356 ## HmRun . ## Runs . ## RBI . ## Walks 2.2144706 ## Years . ## CAtBat . ## CHits . ## CHmRun . ## CRuns 0.2067687 ## CRBI 0.4123098 ## CWalks . ## LeagueN 0.8359508 ## DivisionW -102.7759368 ## PutOuts 0.2197627 ## Assists . ## Errors . ## NewLeagueN . # penalty term using minimum lambda sum(coef(fit_lasso_cv, s = &quot;lambda.min&quot;)[-1] ^ 2) ## [1] 10572.23 # estimated coefficients, using 1-SE rule lambda coef(fit_lasso_cv, s = &quot;lambda.1se&quot;) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 251.4491565 ## AtBat . ## Hits 1.0036044 ## HmRun . ## Runs . ## RBI . ## Walks 0.9857987 ## Years . ## CAtBat . ## CHits . ## CHmRun . ## CRuns 0.1094218 ## CRBI 0.2911570 ## CWalks . ## LeagueN . ## DivisionW . ## PutOuts . ## Assists . ## Errors . ## NewLeagueN . # penalty term using 1-SE rule lambda sum(coef(fit_lasso_cv, s = &quot;lambda.1se&quot;)[-1] ^ 2) ## [1] 2.075766 # predict using minimum lambda predict(fit_lasso_cv, X, s = &quot;lambda.min&quot;) # predict using 1-SE rule lambda, default behavior predict(fit_lasso_cv, X) # calcualte &quot;train error&quot; mean((y - predict(fit_lasso_cv, X)) ^ 2) ## [1] 134647.7 # CV-MSEs fit_lasso_cv$cvm ## [1] 204170.8 195843.9 186963.3 179622.0 172688.0 165552.2 159227.8 153508.8 ## [9] 148571.9 144422.7 140935.6 137857.2 134976.5 132308.5 129776.3 127472.4 ## [17] 125422.6 123615.4 122061.6 120771.5 119696.7 118804.2 118063.8 117451.0 ## [25] 116946.3 116563.7 116300.4 116170.3 116132.8 116127.2 116149.0 116183.0 ## [33] 116349.2 116600.5 116841.5 117174.1 117598.5 118064.5 118494.5 118796.4 ## [41] 118929.0 118976.5 118618.7 118138.5 117636.1 117229.2 116903.9 116659.7 ## [49] 116511.3 116429.7 116433.4 116496.7 116635.7 116759.7 116884.4 116892.4 ## [57] 116844.4 116781.5 116684.3 116606.2 116552.0 116510.0 116482.6 116475.8 ## [65] 116499.1 116537.0 116558.3 116586.5 116598.2 116638.3 116645.8 116686.1 ## [73] 116722.1 116751.7 116760.4 116771.7 116792.0 116806.0 116816.9 116825.5 # CV-MSE using minimum lambda fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min] ## [1] 116127.2 # CV-MSE using 1-SE rule lambda 12.3 broom Sometimes, the output from glmnet() can be overwhelming. The broom package can help with that. library(broom) # the output from the commented line would be immense # fit_lasso_cv tidy(fit_lasso_cv) ## # A tibble: 80 x 6 ## lambda estimate std.error conf.low conf.high nzero ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 255. 204171. 39048. 165123. 243219. 0 ## 2 233. 195844. 38973. 156870. 234817. 1 ## 3 212. 186963. 37751. 149212. 224714. 2 ## 4 193. 179622. 36640. 142982. 216262. 2 ## 5 176. 172688. 35736. 136952. 208424. 3 ## 6 160. 165552. 35177. 130376. 200729. 4 ## 7 146. 159228. 34767. 124461. 193995. 4 ## 8 133. 153509. 34383. 119126. 187892. 4 ## 9 121. 148572. 34027. 114545. 182599. 4 ## 10 111. 144423. 33723. 110699. 178146. 4 ## # … with 70 more rows # the two lambda values of interest glance(fit_lasso_cv) ## # A tibble: 1 x 3 ## lambda.min lambda.1se nobs ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 17.2 111. 263 12.4 Simulated Data, \\(p &gt; n\\) Aside from simply shrinking coefficients (ridge and lasso) and setting some coefficients to 0 (lasso), penalized regression also has the advantage of being able to handle the \\(p &gt; n\\) case. set.seed(1234) n = 1000 p = 5500 X = replicate(p, rnorm(n = n)) beta = c(1, 1, 1, rep(0, 5497)) z = X %*% beta prob = exp(z) / (1 + exp(z)) y = as.factor(rbinom(length(z), size = 1, prob = prob)) We first simulate a classification example where \\(p &gt; n\\). # glm(y ~ X, family = &quot;binomial&quot;) # will not converge We then use a lasso penalty to fit penalized logistic regression. This minimizes \\[ \\sum_{i=1}^{n} L\\left(y_i, \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}\\right) + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\] where \\(L\\) is the appropriate negative log-likelihood. library(glmnet) fit_cv = cv.glmnet(X, y, family = &quot;binomial&quot;, alpha = 1) plot(fit_cv) We’re being lazy again and using the entire dataset as the training data. head(coef(fit_cv), n = 10) ## 10 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 0.02397452 ## V1 0.59674958 ## V2 0.56251761 ## V3 0.60065105 ## V4 . ## V5 . ## V6 . ## V7 . ## V8 . ## V9 . fit_cv$nzero ## s0 s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 s15 s16 s17 s18 s19 ## 0 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## s20 s21 s22 s23 s24 s25 s26 s27 s28 s29 s30 s31 s32 s33 s34 s35 s36 s37 s38 s39 ## 3 3 3 3 3 3 3 3 3 3 4 6 7 10 18 24 35 54 65 75 ## s40 s41 s42 s43 s44 s45 s46 s47 s48 s49 s50 s51 s52 s53 s54 s55 s56 s57 s58 s59 ## 86 100 110 129 147 168 187 202 221 241 254 269 283 298 310 324 333 350 364 375 ## s60 s61 s62 s63 s64 s65 s66 s67 s68 s69 s70 s71 s72 s73 s74 s75 s76 s77 s78 s79 ## 387 400 411 429 435 445 453 455 462 466 475 481 487 491 496 498 502 504 512 518 ## s80 s81 s82 s83 s84 s85 s86 s87 s88 s89 s90 s91 s92 s93 s94 s95 s96 s97 s98 s99 ## 523 526 528 536 543 550 559 561 563 566 570 571 576 582 586 590 596 596 600 599 Notice, only the first three predictors generated are truly significant, and that is exactly what the suggested model finds. fit_1se = glmnet(X, y, family = &quot;binomial&quot;, lambda = fit_cv$lambda.1se) which(as.vector(as.matrix(fit_1se$beta)) != 0) ## [1] 1 2 3 We can also see in the following plots, the three features entering the model well ahead of the irrelevant features. par(mfrow = c(1, 2)) plot(glmnet(X, y, family = &quot;binomial&quot;)) plot(glmnet(X, y, family = &quot;binomial&quot;), xvar = &quot;lambda&quot;) We can extract the two relevant \\(\\lambda\\) values. fit_cv$lambda.min ## [1] 0.03718493 fit_cv$lambda.1se ## [1] 0.0514969 TODO: use default of type.measure=\"deviance\" but note that type.measure=\"class\" exists. "],["ensemble-methods.html", "Chapter 13 Ensemble Methods 13.1 Bagging 13.2 Random Forest 13.3 Boosting", " Chapter 13 Ensemble Methods While this chapter is currently completely incomplete, the following resources will be useful for navigating the Graduate Student quiz in the Fall 2020 semester of STAT 432. Note that these resources might not necessarily follow all conventions of STAT 432, so notation and nomenclature may have minor differences. This chapter introduces ensemble methods that use the combination of several models fit to the same data to create one model that may perform better than any single model. The following are old notes from STAT 432. Like the resources below, these notes suffer from some deviation of the conventions established throughout the course this semester. R for Statistical Learning: Ensemble Methods 13.1 Bagging Bagging is a combination of the words bootstrap and aggregation and refers to a process of fitting many models to bootstrap resamples of data and then aggregating the predictions from these models. Most of the reason we introduced the bootstrap earlier was for its use in the creation of ensemble methods. Bagging is often associated with decision trees, but you could use any methods that you’d like! 13.1.1 Reading Introduction to Statistical Learning: Section 8.2.1 Hands-On Machine Learning with R: Bagging 13.2 Random Forest A random forest is a method that combines decision trees, bagging, and a little bit of extra randomness. The randomness is added to overcome the correlation in the results of the models in the ensemble. 13.2.1 Reading Introduction to Statistical Learning: Section 8.2.2 Hands-On Machine Learning with R: Random Forest 13.2.2 Video StatQuest: Random Forests Part 1 - Building, Using and Evaluating StatQuest: Random Forests Part 2: Missing data and clustering StatQuest: Random Forests in R 13.3 Boosting Like bagging, boosting uses many models, but instead of many models in parallel, it uses many models in sequence. Many methods can be boosted, but most often we boost decision trees. 13.3.1 Reading Introduction to Statistical Learning: Section 8.2.3 Hands-On Machine Learning with R: Gradient Boosting 13.3.2 Video Gradient Boost Part 1: Regression Main Ideas Gradient Boost Part 2: Regression Details Gradient Boost Part 3: Classification Gradient Boost Part 4: Classification Details XGBoost Part 1: Regression XGBoost Part 2: Classification XGBoost Part 3: Mathematical Details XGBoost Part 4: Crazy Cool Optimizations XGBoost Part 1: Regression "],["supervised-overview.html", "Chapter 14 Supervised Learning Overview II 14.1 Classification 14.2 Regression 14.3 External Links", " Chapter 14 Supervised Learning Overview II This chapter is currently just a bit of a placeholder that introduces caret as a systematized way of doing supervised learning. Now that we have seen a number of classification and regression methods, and introduced cross-validation, we see the general outline of a predictive analysis: Test-train split the available data Consider a learning method(s) Decide on a set of candidate models (specify possible tuning parameters for each method) Use resampling to find the “best model” by choosing the values of the tuning parameters Use chosen model to make predictions Calculate relevant metrics on the test data At face value it would seem like it should be easy to repeat this process for a number of different methods, however we have run into a number of difficulties attempting to do so with R. The predict() function seems to have a different behavior for each new method we see. Many methods have different cross-validation functions, or worse yet, no built-in process for cross-validation. (We’ve been writing code ourselves to perform cross-validation.) Not all methods expect the same data format. Some methods do not use formula syntax. Different methods have different handling of categorical predictors. Some methods cannot handle factor variables. Thankfully, the R community has essentially provided a silver bullet for these issues, the caret package. Returning to the above list, we will see that a number of these tasks are directly addressed in the caret package. Test-train split the available data createDataPartition() will take the place of our manual data splitting. It will also do some extra work to ensure that the train and test samples are somewhat similar, especially with respect to the distribution of the response variable. Specify possible tuning parameters for method expand.grid() is not a function in caret, but we will get in the habit of using it to specify a grid of tuning parameters. Use resampling to find the “best model” by choosing the values of the tuning parameters trainControl() will specify the resampling scheme train() is the workhorse of caret. It takes the following information then trains (tunes) the requested model: form, a formula, such as y ~ . This specifies the response and which predictors (or transformations of) should be used. data, the data used for training trControl which specifies the resampling scheme, that is, how cross-validation should be performed to find the best values of the tuning parameters preProcess which allows for specification of data pre-processing such as centering and scaling method, a statistical learning method from a long list of available models tuneGrid which specifies the tuning parameters to train over Use chosen model to make predictions predict() used on objects of type train will be truly magical! 14.1 Classification To illustrate caret, first for classification, we will use the Default data from the ISLR package. data(Default, package = &quot;ISLR&quot;) library(caret) We first test-train split the data using createDataPartition. Here we are using 75% of the data for training. set.seed(430) default_idx = createDataPartition(Default$default, p = 0.75, list = FALSE) default_trn = Default[default_idx, ] default_tst = Default[-default_idx, ] At first glance, it might appear as if the use of createDataPartition() is no different than our previous use of sample(). However, createDataPartition() tries to ensure a split that has a similar distribution of the supplied variable in both datasets. See the documentation for details. After splitting the data, we can begin training a number of models. We begin with a simple additive logistic regression. default_glm_mod = train( form = default ~ ., data = default_trn, trControl = trainControl(method = &quot;cv&quot;, number = 5), method = &quot;glm&quot;, family = &quot;binomial&quot; ) Here, we have supplied four arguments to the train() function form the caret package. form = default ~ . specifies the default variable as the response. It also indicates that all available predictors should be used. data = default_trn specifies that training will be down with the default_trn data trControl = trainControl(method = \"cv\", number = 5) specifies that we will be using 5-fold cross-validation. method = glm specifies that we will fit a generalized linear model. The method essentially specifies both the model (and more specifically the function to fit said model in R) and package that will be used. The train() function is essentially a wrapper around whatever method we chose. In this case, the function is the base R function glm(), so no additional package is required. When a method requires a function from a certain package, that package will need to be installed. See the list of available models for package information. The list that we have passed to the trControl argument is created using the trainControl() function from caret. The trainControl() function is a powerful tool for specifying a number of the training choices required by train(), in particular the resampling scheme. trainControl(method = &quot;cv&quot;, number = 5)[1:3] ## $method ## [1] &quot;cv&quot; ## ## $number ## [1] 5 ## ## $repeats ## [1] NA Here we see just the first three elements of this list, which are related to how the resampling will be done. These are the three elements that we will be most interested in. Here, only the first two are relevant. method specifies how resampling will be done. Examples include cv, boot, LOOCV, repeatedcv, and oob. number specifies the number of times resampling should be done for methods that require resample, such as, cv and boot. repeats specifies the number of times to repeat resampling for methods such as repeatedcv For details on the full capabilities of this function, see the relevant documentation. The out-of-bag, oob which is a sort of automatic resampling for certain statistical learning methods, will be introduced later. We’ve also passed an additional argument of \"binomial\" to family. This isn’t actually an argument for train(), but an additional argument for the method glm. In actuality, we don’t need to specify the family. Since default is a factor variable, caret automatically detects that we are trying to perform classification, and would automatically use family = \"binomial\". This isn’t the case if we were simply using glm(). default_glm_mod ## Generalized Linear Model ## ## 7501 samples ## 3 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 6001, 6000, 6001, 6001, 6001 ## Resampling results: ## ## Accuracy Kappa ## 0.9728038 0.4248613 Called the stored train() object summarizes the training that we have done. We see that we used 7501 observations that had a binary class response and three predictors. We have not done any data pre-processing, and have utilized 5-fold cross-validation. The cross-validated accuracy is reported. Note that, caret is an optimist, and prefers to report accuracy (proportion of correct classifications) instead of the error that we often considered before (proportion of incorrect classifications). names(default_glm_mod) ## [1] &quot;method&quot; &quot;modelInfo&quot; &quot;modelType&quot; &quot;results&quot; &quot;pred&quot; ## [6] &quot;bestTune&quot; &quot;call&quot; &quot;dots&quot; &quot;metric&quot; &quot;control&quot; ## [11] &quot;finalModel&quot; &quot;preProcess&quot; &quot;trainingData&quot; &quot;resample&quot; &quot;resampledCM&quot; ## [16] &quot;perfNames&quot; &quot;maximize&quot; &quot;yLimits&quot; &quot;times&quot; &quot;levels&quot; ## [21] &quot;terms&quot; &quot;coefnames&quot; &quot;contrasts&quot; &quot;xlevels&quot; We see that there is a wealth of information stored in the list returned by train(). Two elements that we will often be interested in are results and finalModel. default_glm_mod$results ## parameter Accuracy Kappa AccuracySD KappaSD ## 1 none 0.9728038 0.4248613 0.002956624 0.07544475 The resutls show some more detailed results, in particular AccuracySD which gives us an estimate of the uncertainty in our accuracy estimate. default_glm_mod$finalModel ## ## Call: NULL ## ## Coefficients: ## (Intercept) studentYes balance income ## -1.070e+01 -6.992e-01 5.676e-03 4.383e-07 ## ## Degrees of Freedom: 7500 Total (i.e. Null); 7497 Residual ## Null Deviance: 2192 ## Residual Deviance: 1186 AIC: 1194 The finalModel is a model object, in this case, the object returned from glm(). This final model, is fit to all of the supplied training data. This model object is often used when we call certain relevant functions on the object returned by train(), such as summary() summary(default_glm_mod) ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1317 -0.1420 -0.0568 -0.0210 3.7348 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.070e+01 5.607e-01 -19.079 &lt; 2e-16 *** ## studentYes -6.992e-01 2.708e-01 -2.582 0.00984 ** ## balance 5.676e-03 2.644e-04 21.471 &lt; 2e-16 *** ## income 4.383e-07 9.389e-06 0.047 0.96276 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2192.2 on 7500 degrees of freedom ## Residual deviance: 1185.8 on 7497 degrees of freedom ## AIC: 1193.8 ## ## Number of Fisher Scoring iterations: 8 We see that this summary is what we had seen previously from objects of type glm. calc_acc = function(actual, predicted) { mean(actual == predicted) } To obtain test accuracy, we will need to make predictions on the test data. With the object returned by train(), this is extremely easy. head(predict(default_glm_mod, newdata = default_tst)) ## [1] No No No No No No ## Levels: No Yes We see that by default, the predict() function is returning classifications. This will be true no matter what method we use! # test acc calc_acc(actual = default_tst$default, predicted = predict(default_glm_mod, newdata = default_tst)) ## [1] 0.9735894 If instead of the default behavior of returning classifications, we instead wanted predicted probabilities, we simply specify type = \"prob\". # get probs head(predict(default_glm_mod, newdata = default_trn, type = &quot;prob&quot;)) ## No Yes ## 2 0.9988332 1.166819e-03 ## 4 0.9995369 4.630821e-04 ## 7 0.9975279 2.472097e-03 ## 8 0.9988855 1.114516e-03 ## 10 0.9999771 2.290522e-05 ## 11 0.9999887 1.134693e-05 Notice that this returns the probabilities for all possible classes, in this case No and Yes. Again, this will be true for all methods! This is especially useful for multi-class data!. 14.1.1 Tuning Since logistic regression has no tuning parameters, we haven’t really highlighted the full potential of caret. We’ve essentially used it to obtain cross-validated results, and for the more well-behaved predict() function. These are excellent improvements over our previous methods, but the real power of caret is its ability to provide a framework for tuning model. To illustrate tuning, we now use knn as our method, which performs \\(k\\)-nearest neighbors. default_knn_mod = train( default ~ ., data = default_trn, method = &quot;knn&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5) ) First, note that we are using formula syntax here, where previously we needed to create separate response and predictors matrices. Also, we’re using a factor variable as a predictor, and caret seems to be taking care of this automatically. default_knn_mod ## k-Nearest Neighbors ## ## 7501 samples ## 3 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 6001, 6000, 6001, 6001, 6001 ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 5 0.9677377 0.2125623 ## 7 0.9664047 0.1099835 ## 9 0.9680044 0.1223319 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 9. Here we are again using 5-fold cross-validation and no pre-processing. Notice that we now have multiple results, for k = 5, k = 7, and k = 9. Let’s modifying this training by introducing pre-processing, and specifying our own tuning parameters, instead of the default values above. default_knn_mod = train( default ~ ., data = default_trn, method = &quot;knn&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5), preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid(k = seq(1, 101, by = 2)) ) Here, we’ve specified that we would like to center and scale the data. Essentially transforming each predictor to have mean 0 and variance 1. The documentation on the preProcess() function provides examples of additional possible pre-processing. IN our call to train() we’re essentially specifying how we would like this function applied to our data. We’ve also provided a “tuning grid,” in this case, the values of k to try. The tuneGrid argument expects a data frame, which expand.grid() returns. We don’t actually need expand.grid() for this example, but it will be a useful habit to develop when we move to methods with multiple tuning parameters. head(default_knn_mod$results, 5) ## k Accuracy Kappa AccuracySD KappaSD ## 1 1 0.9544051 0.2843363 0.006566472 0.08989820 ## 2 3 0.9662702 0.3633701 0.004317478 0.09324199 ## 3 5 0.9710698 0.4122812 0.004076929 0.11219277 ## 4 7 0.9712032 0.4031654 0.003576867 0.09624856 ## 5 9 0.9721364 0.4250236 0.003987725 0.09388466 Since how we have a large number of results, display the entire results would create a lot of clutter. Instead, we can plot the tuning results by calling plot() on the object returned by train(). plot(default_knn_mod) By default, caret utilizes the lattice graphics package to create these plots. Recently, additional support for ggplot2 style graphics has been added for some plots. ggplot(default_knn_mod) + theme_bw() Now that we are dealing with a tuning parameter, train() determines the best value of those considered, by default selecting the best (highest cross-validated) accuracy, and returning that value as bestTune. default_knn_mod$bestTune ## k ## 6 11 get_best_result = function(caret_fit) { best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune)) best_result = caret_fit$results[best, ] rownames(best_result) = NULL best_result } Sometimes it will be useful to obtain the results for only that value. The above function does this automatically. get_best_result(default_knn_mod) ## k Accuracy Kappa AccuracySD KappaSD ## 1 11 0.97227 0.4225433 0.002654708 0.07773239 While we did fit a large number of models, the “best” model is stored in finalModel. After this model was determined to be the best via cross-validation, it is then fit to the entire training dataset. default_knn_mod$finalModel ## 11-nearest neighbor model ## Training set outcome distribution: ## ## No Yes ## 7251 250 With this model, we can again make predictions and obtain predicted probabilities. head(predict(default_knn_mod, newdata = default_tst, type = &quot;prob&quot;)) ## No Yes ## 1 1.0000000 0.00000000 ## 2 1.0000000 0.00000000 ## 3 1.0000000 0.00000000 ## 4 1.0000000 0.00000000 ## 5 0.9090909 0.09090909 ## 6 0.9090909 0.09090909 As an example of a multi-class response consider the following three models fit to the the iris data. Note that the first model is essentially “multinomial logistic regression,” but you might notice it also has a tuning parameter now. (Spoiler: It’s actually a neural network, so you’ll need the nnet package.) iris_log_mod = train( Species ~ ., data = iris, method = &quot;multinom&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5), trace = FALSE ) iris_knn_mod = train( Species ~ ., data = iris, method = &quot;knn&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5), preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid(k = seq(1, 21, by = 2)) ) iris_qda_mod = train( Species ~ ., data = iris, method = &quot;qda&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5) ) We can obtain predicted probabilities with these three models. Notice that they give the predicted probability for each class, using the same syntax for each model. head(predict(iris_log_mod, type = &quot;prob&quot;)) ## setosa versicolor virginica ## 1 1.0000000 1.526406e-09 2.716417e-36 ## 2 0.9999996 3.536476e-07 2.883729e-32 ## 3 1.0000000 4.443506e-08 6.103424e-34 ## 4 0.9999968 3.163905e-06 7.117010e-31 ## 5 1.0000000 1.102983e-09 1.289946e-36 ## 6 1.0000000 3.521573e-10 1.344907e-35 head(predict(iris_knn_mod, type = &quot;prob&quot;)) ## setosa versicolor virginica ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 0 ## 5 1 0 0 ## 6 1 0 0 head(predict(iris_qda_mod, type = &quot;prob&quot;)) ## setosa versicolor virginica ## 1 1 4.918517e-26 2.981541e-41 ## 2 1 7.655808e-19 1.311032e-34 ## 3 1 1.552279e-21 3.380440e-36 ## 4 1 8.300396e-19 8.541858e-32 ## 5 1 3.365614e-27 2.010147e-41 ## 6 1 1.472533e-26 1.271928e-40 14.2 Regression To illustrate the use of caret for regression, we’ll consider some simulated data. gen_some_data = function(n_obs = 50) { x1 = seq(0, 10, length.out = n_obs) x2 = runif(n = n_obs, min = 0, max = 2) x3 = sample(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), size = n_obs, replace = TRUE) x4 = round(runif(n = n_obs, min = 0, max = 5), 1) x5 = round(runif(n = n_obs, min = 0, max = 5), 0) y = round(x1 ^ 2 + x2 ^ 2 + 2 * (x3 == &quot;B&quot;) + rnorm(n = n_obs), 3) data.frame(y, x1, x2, x3, x4, x5) } We first simulate a train and test dataset. set.seed(42) sim_trn = gen_some_data(n_obs = 500) sim_tst = gen_some_data(n_obs = 5000) Fitting knn works nearly identically to its use for classification. Really, the only difference here is that we have a numeric response, which caret understands to be a regression problem. sim_knn_mod = train( y ~ ., data = sim_trn, method = &quot;knn&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5), # preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = expand.grid(k = seq(1, 31, by = 2)) ) sim_knn_mod$modelType ## [1] &quot;Regression&quot; Notice that we’ve commented out the line to perform pre-processing. Can you figure out why? get_best_result(sim_knn_mod) ## k RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 7 3.6834 0.9863968 2.646447 0.587191 0.002630169 0.3388258 A few things to notice in the results. In addition to the usual RMSE, which is be used to determine the best model, we also have MAE, the mean absolute error. We are also given standard deviations of both of these metrics. plot(sim_knn_mod) The following plot adds error bars to RMSE estimate for each k. Sometimes, instead of simply picking the model with the best RMSE (or accuracy), we pick the simplest model within one standard error of the model with the best RMSE. Here then, we would consider k = 11 instead of k = 7 since there isn’t a statistically significant difference. This is potentially a very good idea in practice. By picking a a simpler model, we are essentially at less risk of overfitting, especially since in practice, future data may be slightly different than the data that we are training on. If you’re trying to win a Kaggle competition, this might not be as useful, since often the test and train data come from the exact same source. TODO: additional details about 1-SE rule. calc_rmse = function(actual, predicted) { sqrt(mean((actual - predicted) ^ 2)) } Since we simulated this data, we have a rather large test dataset. This allows us to compare our cross-validation error estimate, to an estimate using (an impractically large) test set. get_best_result(sim_knn_mod)$RMSE ## [1] 3.6834 calc_rmse(actual = sim_tst$y, predicted = predict(sim_knn_mod, sim_tst)) ## [1] 3.412332 Here we see that the cross-validated RMSE is a bit of an overestimate, but still rather close to the test error. The real question is, are either of these any good? Is this model predicting well? No! Notice that we simulated this data with an error standard deviation of 1! 14.2.1 Methods Now that caret has given us a pipeline for a predictive analysis, we can very quickly and easily test new methods. For example, in an upcoming chapter we will discuss boosted tree models, but now that we understand how to use caret, in order to use a boosted tree model, we simply need to know the “method” to do so, which in this case is gbm. Beyond knowing that the method exists, we just need to know its tuning parameters, in this case, there are four. We actually could get away with knowing nothing about them, and simply specify a tuneLength, then caret would automatically try some reasonable values. Instead, we could read the caret documentation to find the tuning parameters, as well as the required packages. For now, we’ll simply use the following tuning grid. In later chapters, we’ll discuss how these effect the model. gbm_grid = expand.grid(interaction.depth = c(1, 2, 3), n.trees = (1:30) * 100, shrinkage = c(0.1, 0.3), n.minobsinnode = 20) head(gbm_grid) ## interaction.depth n.trees shrinkage n.minobsinnode ## 1 1 100 0.1 20 ## 2 2 100 0.1 20 ## 3 3 100 0.1 20 ## 4 1 200 0.1 20 ## 5 2 200 0.1 20 ## 6 3 200 0.1 20 set.seed(42) sim_gbm_mod = train( y ~ ., data = sim_trn, trControl = trainControl(method = &quot;cv&quot;, number = 5), method = &quot;gbm&quot;, tuneGrid = gbm_grid, verbose = FALSE ) We added verbose = FALSE to the train() call to suppress some of the intermediate output of the gbm fitting procedure. How this training is happening is a bit of a mystery to us right now. What is this method? How does it deal with the factor variable as a predictor? We’ll answer these questions later, for now, we do know how to evaluate how well the method is working. knitr::kable(head(sim_gbm_mod$results), digits = 3) shrinkage interaction.depth n.minobsinnode n.trees RMSE Rsquared MAE RMSESD RsquaredSD MAESD 1 0.1 1 20 100 2.151 0.995 1.569 0.321 0.001 0.185 91 0.3 1 20 100 2.800 0.991 2.068 0.180 0.001 0.183 31 0.1 2 20 100 1.996 0.996 1.439 0.286 0.001 0.165 121 0.3 2 20 100 2.377 0.994 1.784 0.285 0.001 0.191 61 0.1 3 20 100 1.967 0.996 1.410 0.325 0.001 0.148 151 0.3 3 20 100 2.166 0.995 1.620 0.150 0.001 0.135 plot(sim_gbm_mod) sim_gbm_mod$bestTune ## n.trees interaction.depth shrinkage n.minobsinnode ## 35 500 2 0.1 20 Here we obtain the set of tuning parameters that performed best. Based on the above plot, do you think we considered enough possible tuning parameters? get_best_result(sim_gbm_mod) ## shrinkage interaction.depth n.minobsinnode n.trees RMSE Rsquared ## 1 0.1 2 20 500 1.854777 0.9962175 ## MAE RMSESD RsquaredSD MAESD ## 1 1.346841 0.2556658 0.0007945475 0.1555363 calc_rmse(actual = sim_tst$y, predicted = predict(sim_gbm_mod, sim_tst)) ## [1] 1.568517 Again, the cross-validated result is overestimating the error a bit. Also, this model is a big improvement over the knn model, but we can still do better. sim_lm_mod = train( y ~ poly(x1, 2) + poly(x2, 2) + x3, data = sim_trn, method = &quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 5) ) sim_lm_mod$finalModel ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Coefficients: ## (Intercept) `poly(x1, 2)1` `poly(x1, 2)2` `poly(x2, 2)1` `poly(x2, 2)2` ## 34.75615 645.50804 167.12875 26.00951 6.86587 ## x3B x3C ## 1.80700 0.07108 Here we fit a good old linear model, except, we specify a very specific formula. sim_lm_mod$results$RMSE ## [1] 1.046702 calc_rmse(actual = sim_tst$y, predicted = predict(sim_lm_mod, sim_tst)) ## [1] 1.035896 This model dominates the previous two. The gbm model does still have a big advantage. The lm model needed the correct form of the model, whereas gbm nearly learned it automatically! This question of which variables should be included is where we will turn our focus next. We’ll consider both what variables are useful for prediction, and learn tools to asses how useful they are. 14.3 External Links The caret Package - Reference documentation for the caret package in bookdown format. caret Model List - List of available models in caret. caret Model List, By Tag - Gives information on tuning parameters and necessary packages. Applied Predictive Modeling - Book from the author of the caret package, Max Kuhn, as well as Kjell Johnson. Further discussion on use of statistical learning methods in practice. "],["additional-reading.html", "A Additional Reading A.1 Books A.2 Papers A.3 Blog Posts A.4 Miscellaneous", " A Additional Reading A.1 Books An Introduction to Statistical Learning Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani The Elements of Statistical Learning Trevor Hastie, Robert Tibshirani, and Jerome Friedman Mathematics for Machine Learning Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong Understanding Machine Learning: From Theory to Algorithms Shai Shalev-Shwartz and Shai Ben-David Foundations of Machine Learning Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar The caret Package Max Kuhn Feature Engineering and Selection: A Practical Approach for Predictive Models Max Kuhn and Kjell Johnson Applied Predictive Modeling Max Kuhn and Kjell Johnson Machine Learning: A Probabilistic Perspective Kevin Murphy Probability for Statistics and Machine Learning Anirban DasGupta From Linear Models to Machine Learning Norman Matloff Interpretable Machine Learning Christoph Molnar Grokking Deep Learning Andrew W. Trask A.2 Papers Do we Need Hundreds of Classifiers to Solve Real World Classification Problems? Manuel Fernandez-Delgado, Eva Cernadas, Senen Barro, Dinani Amorim Statistical Modeling: The Two Cultures Leo Breiman 50 Years of Data Science David Donoho A.3 Blog Posts Peekaboo: Don’t cite the No Free Lunch Theorem aleatoric: Overfitting: A Guided Tour Math for Machines: Optimal Boundaries Variance Explained: What’s the difference between data science, machine learning, and artificial intelligence? A.4 Miscellaneous Machine Learning verus Statistics, A Glossary Rob Tibshirani "],["computing.html", "B Computing B.1 Reading B.2 Additional Resources B.3 STAT 432 Idioms", " B Computing STAT 432 is not a course about R. It is however, a course that makes heavy use of R. Because of this, you will need to be familiar with R. This text will point out some things about R along the way, but some previous study of R is necessary. B.1 Reading The following reading suggestions are long. While it may seem daunting to read all of this material, it will likely prove to be valuable. A smart strategy would be: “Read” as much of the information here, where “read” simply means read every single word and line of code, but don’t slow down if you don’t fully understand. Return to some selections from this reading every week spending more time understanding specific sections. Whether you’re a novice or an expert, there is a high probability that any effort towards reading these sources will provide a return. If you use the strategy above, over time you will start to see the bigger picture. Required: Hands-On Programming with R - Garrett Grolemund If you have never used R or RStudio before, Part 1 (Chapters 1 - 3) will be useful. Even if you have used R before, you will likely gain something from reading these sections. Recommended: R for Data Science - Garrett Grolemund, Hadley Wickham This book helps getting you up to speed working with data in R. While it is a lot of reading, Chapters 1 - 21 may prove useful. It probably isn’t worth sitting down and reading this from start to finish right now, but reading it here and there during some free time would be a good idea. Recommended: Advanced R - Hadley Wickham Part I (Chapters 1 - 8) of this book will help create a mental model for working with R. These chapters are not an easy read, so they should be returned to often. Chapter 2 could be safely skipped for our purposes, but is important if you will use R in the long term. Reference: Applied Statistics with R - David Dalpiaz If you are a UIUC student who took the course STAT 420, the first six chapters of that book could serve as a nice refresher, however, the three readings above are preferred. Chapter 6 contain three very useful video playlists and an R Markdown template. Note that the videos were created a few years ago, thus there may be minor differences between then and now, but the general ideas are the same. Video Playlist: R and RStudio Video Playlist: Data in R Video Playlist: R Markdown Template: R Markdown R Markdown will be used throughout the course, but you will not be required to use R Markdown until the final weeks when we begin working on data analyses. At that time, we will suggest some additional reading about R Markdown. When working on quizzes until then, you can use either an R script or an R Markdown document, whichever you are more comfortable with. B.2 Additional Resources In addition to the above readings, the following resources are more specific or more advanced, but could still prove to be useful. B.2.1 R Efficient R programming R Programming for Data Science R Graphics Cookbook Modern Dive What They Forgot to Teach You About R The R Inferno Data Wrangling, Exploration, and Analysis with R The tidyverse Website dplyr Website readr Website tibble Website forcats Website B.2.2 RStudio RStudio IDE Cheatsheet RStudio Resources B.2.3 R Markdown R Markdown Cheatsheet R Markdown: The Definitive Guide - Yihui Xie, J. J. Allaire, Garrett Grolemund R Markdown Cookbook R4DS: R Markdown Chapter B.2.3.1 Markdown Daring Fireball - Markdown: Basics Markdown Guide GitHub - Mastering Markdown CommonMark B.3 STAT 432 Idioms R tutorials and advice are plentiful online. While we do not want to discourage you from using the resources above, or using your own creativity, the following sections will specify some strong suggestions for how to use R, RStudio, and R Markdown in STAT 432. In other words, information below here supersedes any information from the above sources. B.3.1 Don’t Restore Old Workspaces Due to some odd default settings in RStudio, some students never clear their R environment. This is a problem for a number of reasons. (It could prevent your results from being reproducible.) To get ready for STAT 432, do the following: Clear your current environment. Change some RStudio defaults. Deselect \"Restore .RData into workspace at startup. Set “Save workspace .RData on exit:” to “Never” This will save you a lot of headache in STAT 432 and the future. B.3.2 R Versions You should always use the most up-to-date version of R and RStudio. You must use at least R version 4.0.2. Importantly, R versions after 3.6.0 have slightly different random number generation. Although, even with the most recent version, sometimes R keeps the old random number generation. To check that you are on the most recent random number generation, run: RNGkind() ## [1] &quot;Mersenne-Twister&quot; &quot;Inversion&quot; &quot;Rejection&quot; If your output matches the output above, you’re all set. If not, run: RNGversion(getRversion()) Then re-run RNGkind() and everything should match up and you should be good to go! B.3.3 Packages Be sure to keep your packages up-to-date. To do so: Restart R. In RStudio, use Session &gt; Restart R. (Restarting RStudio will accomplish this if you aren’t restoring your workspace.) Click the “Packages” tab. Click “Update”. Click “Select All”. Click “Install Updates”. At this point, you may be asked: “Do you want to restart R prior to installing?” Select “Yes.” If asked again, Select “No.” During the update you may be asked “Do you want to install from sources the packages which need compilation? (Yes/no/cancel)” Type no and press enter. Do the same if you encounter this when installing packages.79 Errors often rise when installing packages. Two common tips to overcome these: Simply retry the installation. If you see the name of another package in an error message, try installing that package, then retry installing the original package. You may need to repeat this process. B.3.4 Code Style Code needs to be read by two distinct groups: Computers Humans Computers will complain, very loudly, when you write “bad” code, that is code that does not run. We need to write code that is syntactically correct for the computer to be able to “read” our code. Computers only care about syntax. If we relate this to natural language, we would say that computers really only care about grammar and punctuation. They don’t worry about style like phrasing, tone, etc. While computers will complain about bad code, does anyone really care about wasting their time? (OK, sure, computer scientists might.) If we give them bad code, they try to run it, fail, then complain. However, they’re soulless machines, they can handle it. Humans on the other hand have a finite amount of time on this earth. Even if we solve aging, we still have to deal with the heat death of the universe. Thus, while wasting a computer’s time is no big deal, wasting a human’s time is, frankly, immoral. What does this have to do with R programming? Humans are going to read your R code. One of those humans is likely to be you, but future you. To make this reading possible and efficient, you need to develop style. Here is some code that a computer can read, but a human will struggle to read: set.seed(1337);mu=10;sample_size=50;samples=100000;x_bars=rep(0, samples); for(i in 1:samples){x_bars[i]=mean(rpois(sample_size,lambda = mu))} x_bar_hist=hist(x_bars,breaks=50,main=&quot;Histogram of Sample Means&quot;, xlab=&quot;Sample Means&quot;,col=&quot;darkorange&quot;,border = &quot;dodgerblue&quot;) Now, written again, but readable by a human: # set seed for reproducibility set.seed(1337) # setup simulation parameters mu = 10 sample_size = 50 samples = 100000 # create vector to store simulation results x_bars = rep(0, samples) # run simulation for (i in 1:samples) { x_bars[i] = mean(rpois(sample_size, lambda = mu)) } # plot results x_bar_hist = hist( x_bars, breaks = 50, main = &quot;Histogram of Sample Means&quot;, xlab = &quot;Sample Means&quot;, col = &quot;darkorange&quot;, border = &quot;dodgerblue&quot; ) To the computer, these are the same. To a human, one makes you want to pull your hair out, the other you can glance at and have a pretty good idea about what is going on. Style is subjective, but we’ll define some guiding principles, and a few rules. B.3.5 Reference Style So as to not have to define a style from the ground up, we will use the tidyverse style as our initial reference. tidyverse Style Guide We will agree with the vast majority of the guidelines here. The exceptions are listed in the next section. B.3.6 STAT 432 R Style Overrides All commas must be followed by a space. (Additionally, commas should never be preceded by a space.) Infix operators (==, +, -, &lt;-, etc.) should always be surrounded by spaces. Exceptions: :, ::, $, [, [[, ], ]] ^: Use x ^ 2 instead of x^2. You may use = instead of &lt;-. This is very much a minority position in the R community. But we see more examples of it being promoted every day. My reasoning for this is complicated (and I should write more about it soon) but not super important. Instead, what is important: Do not mix assignment operators. Either use = or use &lt;- but do not mix and match in the same script. Never use T or F, only TRUE or FALSE. While this should never happen, take a look at this terrifying example. FALSE == TRUE # checking for equality ## [1] FALSE F == TRUE # checking for equality ## [1] FALSE F = TRUE # A VERY BAD ASSIGNMENT! DO NOT DO THIS! F == TRUE # checking for equality, with a wild result! ## [1] TRUE # TRUE = FALSE # This won&#39;t run, which is good! Do not use ;. This is mostly a readability issue. Do not use attach(). Without going into the details, you will save yourself a lot of headache someday if you follow this advice. Do not use &lt;&lt;-. You probably didn’t know this exists. Pretend that is still the case. Do not set a working directory by using setwd() or any other method. This will make your scripts and R Markdown documents much more reproducible. Do not use absolute paths. Place a space after any # used to create a comment. No more than one newline (blank line) in a row Do not put spaces in filenames. Use dashes - or underscores _. Also consider only using lowercase. Load all packages before setting a seed. Opening (left) curly braces should not be on their own line. Except for the first argument to a function, argument names should be written in function calls. (Exception for the predict() function. Do not name the second argument to the predict() function. If you do, you will regret it eventually!) Place a newline at the end of the file. B.3.7 STAT 432 R Markdown Style Some of the previous section applies here as well, but additionally, some more specific R Markdown style guidelines: No more than one newline (blank line) in a row in an R Markdown document. No more than one newline (blank line) in a row in an R chunk. A newline before and after each chunk in an R Markdown document. No newline to start a chunk. No newline at end of chunk. (The first and last line of each chunk should contain code, or a comment for the first line.) Use headers appropriately! (Short names, good structure.) Load all needed packages at the beginning of an analysis in a single chunk. One plot per chunk! Plotting chunks should return one plot and nothing else. (No numeric printing.) B.3.8 Style Heuristics Now that we’ve overwhelmed you with information about style, we will leave you with the real advice. The most important thing to consider when evaluating the style of your code is consistency. In order, you should be consistent: with yourself! with your group! with your organization! Blindly following the rules is foolish. Breaking rules can be fun! If you do it in a way that makes life easier for everyone, by all mean, do it. B.3.9 Objects and Functions To understand computations in R, two slogans are helpful: Everything that exists is an object. Everything that happens is a function call. — John Chambers80 As you continue to sharpen your R skills, keep this quotation in mind. Eventually, you will realize that everything you “do” in R, you do by running a function. To debug your code, you will need to explore the objects returned by functions. To fix your code, you will need to alter the inputs to functions, which are objects. In STAT 432, the objects that we will encounter will almost always be: (atomic) vectors lists (data frames, tibbles) functions model objects (mostly lists with a class of the model type) If you become proficient at creating, manipulating, and accessing these objects, you will likely have success in STAT 432. B.3.10 Print versus Return One of the more confusing aspects of R is the difference between what is returned when running a function, and what is printed when running a function (interactively) as a user. Consider fitting the following linear model. cars_mod = lm(dist ~ speed, data = cars) You might think that you can simply type cars_mod to see what was returned by lm(). cars_mod ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 However, this is not what was returned. This is what was printed. To better understand what was returned, we use the `str() function. str(cars_mod) ## List of 12 ## $ coefficients : Named num [1:2] -17.58 3.93 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;speed&quot; ## $ residuals : Named num [1:50] 3.85 11.85 -5.95 12.05 2.12 ... ## ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ effects : Named num [1:50] -303.914 145.552 -8.115 9.885 0.194 ... ## ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;(Intercept)&quot; &quot;speed&quot; &quot;&quot; &quot;&quot; ... ## $ rank : int 2 ## $ fitted.values: Named num [1:50] -1.85 -1.85 9.95 9.95 13.88 ... ## ..- attr(*, &quot;names&quot;)= chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ assign : int [1:2] 0 1 ## $ qr :List of 5 ## ..$ qr : num [1:50, 1:2] -7.071 0.141 0.141 0.141 0.141 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:50] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;speed&quot; ## .. ..- attr(*, &quot;assign&quot;)= int [1:2] 0 1 ## ..$ qraux: num [1:2] 1.14 1.27 ## ..$ pivot: int [1:2] 1 2 ## ..$ tol : num 1e-07 ## ..$ rank : int 2 ## ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; ## $ df.residual : int 48 ## $ xlevels : Named list() ## $ call : language lm(formula = dist ~ speed, data = cars) ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language dist ~ speed ## .. ..- attr(*, &quot;variables&quot;)= language list(dist, speed) ## .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:2] &quot;dist&quot; &quot;speed&quot; ## .. .. .. ..$ : chr &quot;speed&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;speed&quot; ## .. ..- attr(*, &quot;order&quot;)= int 1 ## .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. ..- attr(*, &quot;response&quot;)= int 1 ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. ..- attr(*, &quot;predvars&quot;)= language list(dist, speed) ## .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;dist&quot; &quot;speed&quot; ## $ model :&#39;data.frame&#39;: 50 obs. of 2 variables: ## ..$ dist : num [1:50] 2 10 4 22 16 10 18 26 34 17 ... ## ..$ speed: num [1:50] 4 4 7 7 8 9 10 10 10 11 ... ## ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language dist ~ speed ## .. .. ..- attr(*, &quot;variables&quot;)= language list(dist, speed) ## .. .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. .. ..$ : chr [1:2] &quot;dist&quot; &quot;speed&quot; ## .. .. .. .. ..$ : chr &quot;speed&quot; ## .. .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;speed&quot; ## .. .. ..- attr(*, &quot;order&quot;)= int 1 ## .. .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. .. ..- attr(*, &quot;response&quot;)= int 1 ## .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. .. ..- attr(*, &quot;predvars&quot;)= language list(dist, speed) ## .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;dist&quot; &quot;speed&quot; ## - attr(*, &quot;class&quot;)= chr &quot;lm&quot; This is a huge mess, but most importantly, at the top we are told that cars_mod is a list. (It’s technically a object of class \"lm\", but it functions like a list.) class(cars_mod) ## [1] &quot;lm&quot; is.list(cars_mod) ## [1] TRUE Thus to access certain information returned by lm() we need to access cars_mod as a list. cars_mod$coefficients ## (Intercept) speed ## -17.579095 3.932409 Note that what is returned here is a vector, so we could pull out a particular value using vector syntax. cars_mod$coefficients[&quot;speed&quot;] ## speed ## 3.932409 Since lm() truly returns an object of type \"lm\" we can pass cars_mods to some generic functions, and then specific versions for objects of type \"lm\" will be used. coef(cars_mod) ## (Intercept) speed ## -17.579095 3.932409 predict(cars_mod, data.frame(speed = 5:10)) ## 1 2 3 4 5 6 ## 2.082949 6.015358 9.947766 13.880175 17.812584 21.744993 summary(cars_mod) ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.069 -9.525 -2.272 9.215 43.201 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5791 6.7584 -2.601 0.0123 * ## speed 3.9324 0.4155 9.464 1.49e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.38 on 48 degrees of freedom ## Multiple R-squared: 0.6511, Adjusted R-squared: 0.6438 ## F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12 Hey, wait, what is returned by summary()? class(summary(cars_mod)) ## [1] &quot;summary.lm&quot; is.list(summary(cars_mod)) ## [1] TRUE The summary() function returns an object of type \"summary.lm\" which functions as a list! summary(cars_mod)$fstatistic ## value numdf dendf ## 89.56711 1.00000 48.00000 You can also use the View() function, which is specific to RStudio, to view the structure of any object. View(summary(cars_mod)) # RStudio only B.3.11 Help To get documentation about a function in R, simply put a question mark in front of the function name and RStudio will display the documentation, for example: ?log ?sin ?paste ?lm Frequently one of the most difficult things to do when learning R is asking for help. First, you need to decide to ask for help, then you need to know how to ask for help. Your very first line of defense should be to Google your error message or a short description of your issue. (The ability to solve problems using this method is quickly becoming an extremely valuable skill.) If that fails, and it eventually will, you should ask for help. There are a number of things you should include when emailing an instructor, or posting to a help website such as Stack Exchange. Describe what you expect the code to do. State the end goal you are trying to achieve. (Sometimes what you expect the code to do, is not what you want to actually do.) Provide the full text of any errors you have received. Provide enough code to recreate the error. That is, create a minimal reproducible example. If you follow these steps, you will get your issue resolved much quicker, and possibly learn more in the process. Do not be discouraged by running into errors and difficulties when learning R. (Or any technical skill.) It is simply part of the learning process. While taking STAT 432: Come to office hours! B.3.12 Keyboard Shortcuts This section should be expanded over time, but for now, two strong suggestions: Get in the habit of using the keyboard as much as possible, and the mouse as little as possible. A keyboard is a precise entry tool, a mouse is not. Using a mouse requires you to move your hands, a keyboard does not. Hit the [TAB] key often. Like, all the time. It will autocomplete function and object names. It will autofill argument names. (This removes the need to memorize arguments!) You’ll technically not be as up to date with this selection, but this shouldn’t cause any issues, and will make installation quicker and easier.↩︎ John M Chambers, “S, R, and Data Science,” Proceedings of the ACM on Programming Languages 4, no. HOPL (2020): 1–17↩︎ "],["probability.html", "C Probability C.1 Reading C.2 Probability Models C.3 Probability Axioms C.4 Probability Rules C.5 Random Variables C.6 Expectations C.7 Likelihood C.8 Additional References", " C Probability STAT 432 is not a course about probability. STAT 432 is a course that uses probability. We give a very brief review of some necessary probability concepts. As the treatment is less than complete, a list of references is given at the end of the chapter. For example, we ignore the usual recap of basic set theory and omit proofs and examples. Reading the information below will likely be unsatisfying. Instead, we suggest that you skip it, engage with the relevant quizzes, then return as needed for reference. C.1 Reading Required: Probability Distributions in R Reference: STAT 400 @ UIUC: Notes and Homework Reference: MIT 6.041: Video Lectures Reference: MIT 6.041: Lecture Notes Reference: MIT 6.041: Readings Reference: STAT 414 @ PSU: Notes C.2 Probability Models When discussing probability models, we speak of random experiments that produce one of a number of possible outcomes. A probability model that describes the uncertainty of an experiment consists of two elements: The sample space, often denoted as \\(\\Omega\\), which is a set that contains all possible outcomes. A probability function that assigns to an event \\(A\\) a nonnegative number, \\(P[A]\\), that represents how likely it is that event \\(A\\) occurs as a result of the experiment. We call \\(P[A]\\) the probability of event \\(A\\). An event \\(A\\) could be any subset of the sample space, not necessarily a single possible outcome. The probability law must follow a number of rules, which are the result of a set of axioms that we introduce now. C.3 Probability Axioms Given a sample space \\(\\Omega\\) for a particular experiment, the probability function associated with the experiment must satisfy the following axioms. Nonnegativity: \\(P[A] \\geq 0\\) for any event \\(A \\subset \\Omega\\). Normalization: \\(P[\\Omega] = 1\\). That is, the probability of the entire space is 1. Additivity: For mutually exclusive events \\(E_1, E_2, \\ldots\\) \\[ P\\left[\\bigcup_{i = 1}^{\\infty} E_i\\right] = \\sum_{i = 1}^{\\infty} P[E_i] \\] Using these axioms, many additional probability rules can easily be derived. C.4 Probability Rules Given an event \\(A\\), and its complement, \\(A^c\\), that is, the outcomes in \\(\\Omega\\) which are not in \\(A\\), we have the complement rule: \\[ P[A^c] = 1 - P[A] \\] In general, for two events \\(A\\) and \\(B\\), we have the addition rule: \\[ P[A \\cup B] = P[A] + P[B] - P[A \\cap B] \\] If \\(A\\) and \\(B\\) are also disjoint, then we have: \\[ P[A \\cup B] = P[A] + P[B] \\] If we have \\(n\\) mutually exclusive events, \\(E_1, E_2, \\ldots E_n\\), then we have: \\[ P\\left[\\textstyle\\bigcup_{i = 1}^{n} E_i\\right] = \\sum_{i = 1}^{n} P[E_i] \\] Often, we would like to understand the probability of an event \\(A\\), given some information about the outcome of event \\(B\\). In that case, we have the conditional probability rule provided \\(P[B] &gt; 0\\). \\[ P[A \\mid B] = \\frac{P[A \\cap B]}{P[B]} \\] Rearranging the conditional probability rule, we obtain the multiplication rule: \\[ P[A \\cap B] = P[B] \\cdot P[A \\mid B] \\cdot \\] For a number of events \\(E_1, E_2, \\ldots E_n\\), the multiplication rule can be expanded into the chain rule: \\[ P\\left[\\textstyle\\bigcap_{i = 1}^{n} E_i\\right] = P[E_1] \\cdot P[E_2 \\mid E_1] \\cdot P[E_3 \\mid E_1 \\cap E_2] \\cdots P\\left[E_n \\mid \\textstyle\\bigcap_{i = 1}^{n - 1} E_i\\right] \\] Define a partition of a sample space \\(\\Omega\\) to be a set of disjoint events \\(A_1, A_2, \\ldots, A_n\\) whose union is the sample space \\(\\Omega\\). That is \\[ A_i \\cap A_j = \\emptyset \\] for all \\(i \\neq j\\), and \\[ \\bigcup_{i = 1}^{n} A_i = \\Omega. \\] Now, let \\(A_1, A_2, \\ldots, A_n\\) form a partition of the sample space where \\(P[A_i] &gt; 0\\) for all \\(i\\). Then for any event \\(B\\) with \\(P[B] &gt; 0\\) we have Bayes’ Theorem: \\[ P[A_i | B] = \\frac{P[A_i]P[B | A_i]}{P[B]} = \\frac{P[A_i]P[B | A_i]}{\\sum_{i = 1}^{n}P[A_i]P[B | A_i]} \\] The denominator of the latter equality is often called the law of total probability: \\[ P[B] = \\sum_{i = 1}^{n}P[A_i]P[B | A_i] \\] Note: When working with Bayes’ Theorem it is often useful to draw a tree diagram. Two events \\(A\\) and \\(B\\) are said to be independent if they satisfy \\[ P[A \\cap B] = P[A] \\cdot P[B] \\] This becomes the new multiplication rule for independent events. A collection of events \\(E_1, E_2, \\ldots E_n\\) is said to be independent if \\[ P\\left[\\bigcap_{i \\in S} E_i \\right] = \\prod_{i \\in S}P[E_i] \\] for every subset \\(S\\) of \\(\\{1, 2, \\ldots n\\}\\). If this is the case, then the chain rule is greatly simplified to: \\[ P\\left[\\textstyle\\bigcap_{i = 1}^{n} E_i\\right] = \\prod_{i=1}^{n}P[E_i] \\] C.5 Random Variables A random variable is simply a function which maps outcomes in the sample space to real numbers. C.5.1 Distributions We often talk about the distribution of a random variable, which can be thought of as: \\[ \\text{distribution} = \\text{list of possible} \\textbf{ values} + \\text{associated} \\textbf{ probabilities} \\] This is not a strict mathematical definition, but is useful for conveying the idea. If the possible values of a random variables are discrete, it is called a discrete random variable. If the possible values of a random variables are continuous, it is called a continuous random variable. C.5.2 Discrete Random Variables The distribution of a discrete random variable \\(X\\) is most often specified by a list of possible values and a probability mass function, \\(p(x)\\). The mass function directly gives probabilities, that is, \\[ p(x) = p_X(x) = P[X = x]. \\] Note we almost always drop the subscript from the more correct \\(p_X(x)\\) and simply refer to \\(p(x)\\). The relevant random variable is discerned from context The most common example of a discrete random variable is a binomial random variable. The mass function of a binomial random variable \\(X\\), is given by \\[ p(x | n, p) = {n \\choose x} p^x(1 - p)^{n - x}, \\ \\ \\ x = 0, 1, \\ldots, n, \\ n \\in \\mathbb{N}, \\ 0 &lt; p &lt; 1. \\] This line conveys a large amount of information. The function \\(p(x | n, p)\\) is the mass function. It is a function of \\(x\\), the possible values of the random variable \\(X\\). It is conditional on the parameters \\(n\\) and \\(p\\). Different values of these parameters specify different binomial distributions. \\(x = 0, 1, \\ldots, n\\) indicates the sample space, that is, the possible values of the random variable. \\(n \\in \\mathbb{N}\\) and \\(0 &lt; p &lt; 1\\) specify the parameter spaces. These are the possible values of the parameters that give a valid binomial distribution. Often all of this information is simply encoded by writing \\[ X \\sim \\text{bin}(n, p). \\] C.5.3 Continuous Random Variables The distribution of a continuous random variable \\(X\\) is most often specified by a set of possible values and a probability density function, \\(f(x)\\). (A cumulative density or moment generating function would also suffice.) The probability of the event \\(a &lt; X &lt; b\\) is calculated as \\[ P[a &lt; X &lt; b] = \\int_{a}^{b} f(x)dx. \\] Note that densities are not probabilities. The most common example of a continuous random variable is a normal random variable. The density of a normal random variable \\(X\\), is given by \\[ f(x | \\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\cdot \\exp\\left[\\frac{-1}{2} \\left(\\frac{x - \\mu}{\\sigma}\\right)^2 \\right], \\ \\ \\ -\\infty &lt; x &lt; \\infty, \\ -\\infty &lt; \\mu &lt; \\infty, \\ \\sigma &gt; 0. \\] The function \\(f(x | \\mu, \\sigma^2)\\) is the density function. It is a function of \\(x\\), the possible values of the random variable \\(X\\). It is conditional on the parameters \\(\\mu\\) and \\(\\sigma^2\\). Different values of these parameters specify different normal distributions. \\(-\\infty &lt; x &lt; \\infty\\) indicates the sample space. In this case, the random variable may take any value on the real line. \\(-\\infty &lt; \\mu &lt; \\infty\\) and \\(\\sigma &gt; 0\\) specify the parameter space. These are the possible values of the parameters that give a valid normal distribution. Often all of this information is simply encoded by writing \\[ X \\sim N(\\mu, \\sigma^2) \\] C.5.4 Distributions in R R is an excellent, if not best, tool for performing probability distribution calculations. For a large number of distributions, it has four built in functions: d*(x, ...) returns the PDF at \\(x\\) (for continuous distributions) or the PMG at \\(x\\) (for discrete distributions) p*(q, ...) returns the CDF at quantile \\(q\\), that is \\(P[X \\leq q]\\) q*(p, ...) returns \\(c\\) such that \\(P[x \\leq c] = p\\) r*(n, ...) returns \\(n\\) randomly generated observations The * can be any of the disributions built in to R. The ... represents additional arguments, including the parameters of the various distributions. C.5.5 Several Random Variables Consider two random variables \\(X\\) and \\(Y\\). We say they are independent if \\[ f(x, y) = f(x) \\cdot f(y) \\] for all \\(x\\) and \\(y\\). Here \\(f(x, y)\\) is the joint density (mass) function of \\(X\\) and \\(Y\\). We call \\(f(x)\\) the marginal density (mass) function of \\(X\\). Then \\(f(y)\\) the marginal density (mass) function of \\(Y\\). The joint density (mass) function \\(f(x, y)\\) together with the possible \\((x, y)\\) values specify the joint distribution of \\(X\\) and \\(Y\\). Similar notions exist for more than two variables. C.6 Expectations For discrete random variables, we define the expectation of the function of a random variable \\(X\\) as follows. \\[ \\mathbb{E}[g(X)] \\triangleq \\sum_{x} g(x)p(x) \\] For continuous random variables we have a similar definition. \\[ \\mathbb{E}[g(X)] \\triangleq \\int g(x)f(x) dx \\] For specific functions \\(g\\), expectations are given names. The mean of a random variable \\(X\\) is given by \\[ \\mu_{X} = \\text{mean}[X] \\triangleq \\mathbb{E}[X]. \\] So for a discrete random variable, we would have \\[ \\text{mean}[X] = \\sum_{x} x \\cdot p(x) \\] For a continuous random variable we would simply replace the sum by an integral. The variance of a random variable \\(X\\) is given by \\[ \\sigma^2_{X} = \\text{var}[X] \\triangleq \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2. \\] The standard deviation of a random variable \\(X\\) is given by \\[ \\sigma_{X} = \\text{sd}[X] \\triangleq \\sqrt{\\sigma^2_{X}} = \\sqrt{\\text{var}[X]}. \\] The covariance of random variables \\(X\\) and \\(Y\\) is given by \\[ \\text{cov}[X, Y] \\triangleq \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] = \\mathbb{E}[XY] - \\mathbb{E}[X] \\cdot \\mathbb{E}[Y]. \\] C.7 Likelihood Consider \\(n\\) iid random variables \\(X_1, X_2, \\ldots X_n\\). We can then write their likelihood as \\[ \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) \\triangleq f(x_1, x_2, \\ldots, x_n; \\theta) = \\prod_{i = 1}^n f(x_i; \\theta) \\] where \\(f(x_1, x_2, \\ldots, x_n; \\theta)\\) is the joint density (or mass) of \\(X_1, X_2, \\ldots X_n\\) and \\(f(x_i; \\theta)\\) is the density (or mass) function of random variable \\(X_i\\) evaluated at \\(x_i\\) with parameter \\(\\theta\\). (Note: The last equality above only holds for iid random variables.) Whereas a probability or density is a function of a possible observed value given a particular parameter value, a likelihood is the opposite. It is a function of a possible parameter values given observed data. Likelihoods are calculated when the data (the \\(x_i\\)) are known and the parameters (\\(\\theta\\)) are unknown. That is, a likelihood and a joint density will “look” the same, that is contain the same symbols. The meaning of these symbols change depending on what is known. If the data is known, and the parameters is unknown, you have a likelihood. If the parameters are known, and the data are unknown, you have a joint density. The definition above is an acknowledgement of this. The likelihood is defined to be the joint density when the data are known but the parameter(s) is unknown. Maximizing a likelihood is a common technique for fitting a model to data, however, most often we maximum the log-likelihood, as the likelihood and log-likelihood obtain their maximum at the same point. \\[ \\log \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) = \\sum_{i = 1}^{n} \\log f(x_i; \\theta) \\] As an example, suppose that the data vector x_data contains observations from a random sample \\(X_1, X_2, \\ldots, X_n\\) that is assumed to be sampled from a Poisson distribution with (unknown) parameter \\(\\lambda\\). set.seed(42) x_data = rpois(n = 25, lambda = 6) # generating data (assume this is not known) head(x_data) # check data ## [1] 9 10 5 8 7 6 We can use R to calculate the likelihood for various possible values of \\(\\lambda\\) given this data. # calculate the likelihood when lambda = 5 prod(dpois(x = x_data, lambda = 5)) ## [1] 2.609375e-30 The above code takes advantage of the vectorized nature of the dpois() function. Often, especially for computational reasons, we prefer to directly obtain the log-likelihood. # calculate the log-likelihood when lambda = 5 sum(log(dpois(x = x_data, lambda = 5))) ## [1] -68.11844 To understand why this is necessary, repeat the above, but with a much larger sample size. Also note that the d*() functions in R have an option to return logged values. # calculate the log-likelihood when lambda = 5 sum(dpois(x = x_data, lambda = 5, log = TRUE)) ## [1] -68.11844 C.8 Additional References Any of the following are either dedicated to, or contain a good coverage of the details of the topics above. Probability Texts Introduction to Probability by Dimitri P. Bertsekas and John N. Tsitsiklis A First Course in Probability by Sheldon Ross Machine Learning Texts with Probability Focus Probability for Statistics and Machine Learning by Anirban DasGupta Machine Learning: A Probabilistic Perspective by Kevin P. Murphy Statistics Texts with Introduction to Probability Probability and Statistical Inference by Robert V. Hogg, Elliot Tanis, and Dale Zimmerman Introduction to Mathematical Statistics by Robert V. Hogg, Joseph McKean, and Allen T. Craig C.8.1 Videos The YouTube channel mathematicalmonk has a great Probability Primer playlist containing lectures on many fundamental probability concepts. Some of the more important concepts are covered in the following videos: Conditional Probability Independence More Independence Bayes Rule "],["statistics.html", "D Statistics D.1 Reading D.2 Statistics D.3 Estimators", " D Statistics STAT 432 is a course about statistics, in particular, some specific statistics. To discuss the statistics of interest in STAT 432, we will need some general concepts about statistics. D.1 Reading Reference: STAT 400 @ UIUC: Notes and Homework Reference: STAT 3202 @ OSU: Fitting a Probability Model Reference: STAT 415 @ PSU: Notes D.2 Statistics In short: a statistic is a function of (sample) data. (This mirrors parameters being functions of (population) distributions. In SAT terminology, statistics : data :: parameter : distribution.) Consider a random variable \\(X\\), with PDF \\(f(x)\\) which defines the distribution of \\(X\\). Now consider the parameter \\(\\mu\\), which we usually refer to as the mean of a distribution. We use \\(\\mu_X\\) to note that \\(\\mu\\) is dependent on the distribution of \\(X\\). \\[ \\mu_X = \\text{E}[X] = \\int_{-\\infty}^{\\infty}xf(x)dx \\] Note that this expression is a function of \\(f(x)\\). When we change the distribution of \\(X\\), that is, it has a different \\(f(x)\\), that effects \\(\\mu\\). Now, given a random sample \\(X_1, X_2, \\ldots, X_n\\), define a statistic, \\[ \\hat{\\mu}(x_1, x_2, \\ldots, x_n) = \\frac{1}{n}\\sum_{i = 1}^{n}x_i \\] Often, we will simplify notation and instead simply write \\[ \\hat{\\mu} = \\frac{1}{n}\\sum_{i = 1}^{n}x_i \\] and the fact that \\(\\hat{\\mu}\\) is a function of the sample is implied. (You might also notice that this is the sample mean, which is often denoted by \\(\\bar{x}\\).) Another confusing aspect of statistics is that they are random variables! Sometimes we would write the above as \\[ \\hat{\\mu}(X_1, X_2, \\ldots, X_n) = \\frac{1}{n}\\sum_{i = 1}^{n}X_i \\] When written this way, we are emphasizing that the random sample has not yet be observed, thus is still random. When this is the case, we can investigate the properties of the statistic as a random variable. When the sample has been observed, we use \\(x_1, x_2, \\ldots, x_n\\) to note that we are inputting these observed values into a function, which outputs some value. (Sometimes we, and others, will be notationally sloppy and simply use lower case \\(x\\) and you will be expected to understand via context if we are dealing with random variables or observed values of random variables. This is admittedly confusing.) As a final note, suppose we observe some data \\[ x_1 = 2, x_2 = 1, x_3 =5 \\] and we calculate \\(\\hat{\\mu}\\) given these values. We would obtain \\[ \\hat{\\mu} = \\frac{8}{3} \\approx 2.66 \\] Note that 2.66 is not a statistic. It is the value of a statistic given a particular set of data. The statistic is still \\(\\hat{\\mu}\\) which has output the value 2.66. Statistics output values given some data. D.3 Estimators Estimators are just statistics with a purpose, that is, estimators are statistics that attempt to estimate some quantity of interest, usually some parameter. (In other words, learn from data.) Like statistics, estimators are functions of data that output values, which we call estimates. D.3.1 Properties Bias and Variance Visually Illustrated Because they are just statistics, estimators are simply functions of data. What makes an estimator good? Essentially, an estimator is good if it produces estimates that are close to the thing being estimated. The following properties help to better define this “closeness” as a function of the errors made by estimators. To estimate some parameter \\(\\theta\\) we will consider some estimator \\(\\hat{\\theta}\\). D.3.1.1 Bias The bias of an estimator defines the systematic error of the estimator, that is, how the estimator “misses” on average. \\[ \\text{bias}\\left[\\hat{\\theta}\\right] \\triangleq \\mathbb{E}\\left[\\hat{\\theta}\\right] - \\theta \\] D.3.1.2 Variance The variance of an estimator defines how close resulting estimates are to each other. (Assuming the estimated was repeated.) \\[ \\text{var}\\left[\\hat{\\theta}\\right] \\triangleq \\mathbb{E}\\left[ \\left( \\hat{\\theta} - \\mathbb{E}\\left[\\hat{\\theta}\\right] \\right)^2 \\right] \\] D.3.1.3 Mean Squared Error The mean squared error (MSE) is exactly what the name suggests, it is the average squared error of the estimator. Interestingly, the MSE decomposes into terms related to the bias and the variance. We will return to this idea later for a detailed discussion in the context of machine learning. \\[ \\text{MSE}\\left[\\hat{\\theta}\\right] \\triangleq \\mathbb{E}\\left[\\left(\\hat{\\theta} - \\theta\\right)^2\\right] = \\left(\\text{bias}\\left[\\hat{\\theta}\\right]\\right)^2 + \\text{var}\\left[\\hat{\\theta}\\right] \\] D.3.1.4 Consistency An estimator \\(\\hat{\\theta}_n\\) is said to be a consistent estimator of \\(\\theta\\) if, for any positive \\(\\epsilon\\), \\[ \\lim_{n \\rightarrow \\infty} P\\left( \\left| \\hat{\\theta}_n - \\theta \\right| \\leq \\epsilon\\right) =1 \\] or, equivalently, \\[ \\lim_{n \\rightarrow \\infty} P\\left( \\left| \\hat{\\theta}_n - \\theta \\right| &gt; \\epsilon\\right) =0 \\] We say that \\(\\hat{\\theta}_n\\) converges in probability to \\(\\theta\\) and we write \\(\\hat{\\theta}_n \\overset P \\rightarrow \\theta\\). D.3.2 Example: MSE of an Estimator Consider \\(X_1, X_2, X_3 \\sim N(\\mu, \\sigma^2)\\). Define two estimators for the true mean, \\(\\mu\\). \\[ \\bar{X} = \\frac{1}{n}\\sum_{i = 1}^{3} X_i \\] \\[ \\hat{\\mu} = \\frac{1}{4}X_1 + \\frac{1}{5}X_2 + \\frac{1}{6}X_3 \\] We will now calculate and compare the mean squared error of both \\(\\bar{X}\\) and \\(\\hat{\\mu}\\) as estimators of \\(\\mu\\). First, recall from properties of the sample mean that \\[ \\text{E}\\left[\\bar{X}\\right] = \\mu \\] and \\[ \\text{var}\\left[\\bar{X}\\right] = \\frac{\\sigma^2}{3} \\] Thus we have \\[ \\text{bias}\\left[\\bar{X}\\right] = \\mathbb{E}\\left[\\bar{X}\\right] - \\mu = \\mu - \\mu = 0 \\] Then, \\[ \\text{MSE}\\left[\\bar{X}\\right] \\triangleq \\left(\\text{bias}\\left[\\bar{X}\\right]\\right)^2 + \\text{var}\\left[\\bar{X}\\right] = 0 + \\frac{\\sigma^2}{3} = \\frac{\\sigma^2}{3} \\] Next, \\[ \\text{E}\\left[\\hat{\\mu}\\right] = \\frac{\\mu}{4} + \\frac{\\mu}{5} + \\frac{\\mu}{6} = \\frac{37}{60}\\mu \\] and \\[ \\text{var}\\left[\\hat{\\mu}\\right] = \\frac{\\sigma^2}{16} + \\frac{\\sigma^2}{25} + \\frac{\\sigma^2}{36} = \\frac{469}{3600}\\sigma^2 \\] Now we have \\[ \\text{bias}\\left[\\hat{\\mu}\\right] = \\mathbb{E}\\left[\\hat{\\mu}\\right] - \\mu = \\frac{37}{60}\\mu - \\mu = \\frac{-23}{60}\\mu \\] Then finally we obtain the mean squared error for \\(\\hat{\\mu}\\), \\[ \\text{MSE}\\left[\\hat{\\mu}\\right] \\triangleq \\left(\\text{bias}\\left[\\hat{\\mu}\\right]\\right)^2 + \\text{var}\\left[\\hat{\\mu}\\right] = \\left( \\frac{-23}{60}\\mu \\right)^2 + \\frac{469}{3600}\\sigma^2 \\] Note that \\(\\text{MSE}\\left[\\hat{\\mu}\\right]\\) is small when \\(\\mu\\) is close to 0. D.3.3 Estimation Methods So far we have discussed properties of estimators, but how do we create estimators? You could just define a bunch of estimators and then evaluate them to see what works best (an idea we will return to later in the context of ML) but (the field of) statistics has develop some methods that result in estimators with desirable properties. D.3.4 Maximum Likelihood Estimation Given a random sample \\(X_1, X_2, \\ldots, X_n\\) from a population with parameter \\(\\theta\\) and density or mass \\(f(x; \\theta)\\), we define the likelihood as \\[ \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) \\triangleq f(x_1, x_2, \\ldots, x_n; \\theta) = \\prod_{i = 1}^n f(x_i; \\theta) \\] The Maximum Likelihood Estimator, \\(\\hat{\\theta}\\) \\[ \\hat{\\theta} \\triangleq \\underset{\\theta}{\\text{argmax}} \\ \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) = \\underset{\\theta}{\\text{argmax}} \\ \\log \\mathcal{L}(\\theta \\mid x_1, x_2, \\ldots x_n) \\] D.3.4.1 Invariance Principle If \\(\\hat{\\theta}\\) is the MLE of \\(\\theta\\) and the function \\(h(\\theta)\\) is continuous, then \\(h(\\hat{\\theta})\\) is the MLE of \\(h(\\theta)\\). D.3.5 Method of Moments While it is very unlikely that we will use the Method of Moments in STAT 432, you should still be aware of its existence. D.3.6 Empirical Distribution Function Consider a random variable \\(X\\) with CDF \\(F(k) = P(X &lt; k)\\) and an iid random sample \\(X_1, X_2, \\ldots, X_n\\). We can estimate \\(F(k)\\) using the Empirical Distribution Function (EDF), \\[ \\hat{F}(k) = \\frac{\\text{# of elements in sample} \\leq k}{n} = \\frac{1}{n} \\sum_{i = 1}^n I(x_i \\leq k) \\] where \\(I(x_i \\leq k)\\) is an indicator such that \\[ I(x_i \\leq k) = \\begin{cases} 1 &amp; \\text{if } x_i \\leq k \\\\ 0 &amp; \\text{if } x_i &gt; k \\end{cases} \\] Given a data vector in R that is assumed to be a random sample, say, y, and some value, say k, it is easy to calculate \\(\\hat{F}(k)\\). set.seed(66) y = rnorm(n = 25, mean = 6, sd = 2.6) # generate sample k = 4 # pick some k head(y) # check data ## [1] 12.042334 6.564140 7.087301 5.503419 5.181420 4.315569 # using the EDF mean(y &lt; k) ## [1] 0.2 # using an estimated normal distribution (not quite using the MLE) pnorm(q = k, mean = mean(y), sd = sd(y)) ## [1] 0.2088465 # using the true (but assumed unknown) CDF pnorm(q = k, mean = 6, sd = 2.6) ## [1] 0.2208782 Note that technically sd(x) does not return the MLE of \\(\\sigma\\) since it uses the unbiased estimator with a denominator of \\(n - 1\\) instead of \\(n\\), but we’re being lazy for the sake of some cleaner code. plot(ecdf(y), col.01line = &quot;white&quot;, verticals = TRUE, do.points = FALSE, xlim = c(0, 15), lwd = 2, lty = 1, ylab = &quot;F(y)&quot;, xlab = &quot;y&quot;, main = &quot;Comparing the EDF to The Truth and MLE&quot;) curve(pnorm(x, mean = 6, sd = 2.6), add = TRUE, xlim = c(0, 15), col = &quot;dodgerblue&quot;, lty = 2, lwd = 2) curve(pnorm(x, mean = mean(y), sd = sd(y)), add = TRUE, xlim = c(0, 15), col = &quot;darkorange&quot;, lty = 3, lwd = 2) legend(&quot;bottomright&quot;, legend = c(&quot;EDF&quot;, &quot;Truth&quot;, &quot;MLE&quot;), col = c(&quot;black&quot;, &quot;dodgerblue&quot;, &quot;darkorange&quot;), lty = 1:3, lwd = 2) grid() We have purposefully used a “small” sample size here so that the EDF is visibly a step function. Modify the code above to increase the sample size. You should notice that the three functions converge as the sample size increases. "],["references.html", "References", " References "]]
