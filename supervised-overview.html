<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 14 Supervised Learning Overview II | Basics of Statistical Learning</title>
  <meta name="description" content="Chapter 14 Supervised Learning Overview II | Basics of Statistical Learning" />
  <meta name="generator" content="bookdown 0.21.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 14 Supervised Learning Overview II | Basics of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://statisticallearning.org/" />
  
  
  <meta name="github-repo" content="daviddalpiaz/bsl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 14 Supervised Learning Overview II | Basics of Statistical Learning" />
  
  
  

<meta name="author" content="David Dalpiaz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ensemble-methods.html"/>
<link rel="next" href="additional-reading.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Basics of Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i><b>0.1</b> Who?</a>
<ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#readers"><i class="fa fa-check"></i><b>0.1.1</b> Readers</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#author"><i class="fa fa-check"></i><b>0.1.2</b> Author</a></li>
<li class="chapter" data-level="0.1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>0.1.3</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#what"><i class="fa fa-check"></i><b>0.2</b> What?</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#why"><i class="fa fa-check"></i><b>0.3</b> Why?</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#where"><i class="fa fa-check"></i><b>0.4</b> Where?</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#when"><i class="fa fa-check"></i><b>0.5</b> When?</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#how"><i class="fa fa-check"></i><b>0.6</b> How?</a>
<ul>
<li class="chapter" data-level="0.6.1" data-path="index.html"><a href="index.html#build-tools"><i class="fa fa-check"></i><b>0.6.1</b> Build Tools</a></li>
<li class="chapter" data-level="0.6.2" data-path="index.html"><a href="index.html#active-development"><i class="fa fa-check"></i><b>0.6.2</b> Active Development</a></li>
<li class="chapter" data-level="0.6.3" data-path="index.html"><a href="index.html#packages"><i class="fa fa-check"></i><b>0.6.3</b> Packages</a></li>
<li class="chapter" data-level="0.6.4" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>0.6.4</b> License</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ml-overview.html"><a href="ml-overview.html"><i class="fa fa-check"></i><b>1</b> Machine Learning Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ml-overview.html"><a href="ml-overview.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-tasks"><i class="fa fa-check"></i><b>1.2</b> Machine Learning Tasks</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ml-overview.html"><a href="ml-overview.html#supervised-learning"><i class="fa fa-check"></i><b>1.2.1</b> Supervised Learning</a></li>
<li class="chapter" data-level="1.2.2" data-path="ml-overview.html"><a href="ml-overview.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.2.2</b> Unsupervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ml-overview.html"><a href="ml-overview.html#open-questions"><i class="fa fa-check"></i><b>1.3</b> Open Questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#r-setup-and-source"><i class="fa fa-check"></i><b>2.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#explanation-versus-prediction"><i class="fa fa-check"></i><b>2.2</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#task-setup"><i class="fa fa-check"></i><b>2.3</b> Task Setup</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#mathematical-setup"><i class="fa fa-check"></i><b>2.4</b> Mathematical Setup</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-models"><i class="fa fa-check"></i><b>2.5</b> Linear Regression Models</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#using-lm"><i class="fa fa-check"></i><b>2.6</b> Using <code>lm()</code></a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression.html"><a href="linear-regression.html#the-predict-function"><i class="fa fa-check"></i><b>2.7</b> The <code>predict()</code> Function</a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression.html"><a href="linear-regression.html#data-splitting"><i class="fa fa-check"></i><b>2.8</b> Data Splitting</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression.html"><a href="linear-regression.html#regression-metrics"><i class="fa fa-check"></i><b>2.9</b> Regression Metrics</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="linear-regression.html"><a href="linear-regression.html#graphical-evaluation"><i class="fa fa-check"></i><b>2.9.1</b> Graphical Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="linear-regression.html"><a href="linear-regression.html#example-simple-simulated-data"><i class="fa fa-check"></i><b>2.10</b> Example: “Simple” Simulated Data</a></li>
<li class="chapter" data-level="2.11" data-path="linear-regression.html"><a href="linear-regression.html#example-diamonds-data"><i class="fa fa-check"></i><b>2.11</b> Example: Diamonds Data</a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression.html"><a href="linear-regression.html#example-credit-card-data"><i class="fa fa-check"></i><b>2.12</b> Example: Credit Card Data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html"><i class="fa fa-check"></i><b>3</b> Nonparametric Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#r-setup-and-source-1"><i class="fa fa-check"></i><b>3.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="3.2" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#mathematical-setup-1"><i class="fa fa-check"></i><b>3.2</b> Mathematical Setup</a></li>
<li class="chapter" data-level="3.3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="3.4" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#decision-trees"><i class="fa fa-check"></i><b>3.4</b> Decision Trees</a></li>
<li class="chapter" data-level="3.5" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#example-credit-card-data-1"><i class="fa fa-check"></i><b>3.5</b> Example: Credit Card Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>4</b> The Bias–Variance Tradeoff</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#r-setup-and-source-2"><i class="fa fa-check"></i><b>4.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="4.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#the-regression-setup"><i class="fa fa-check"></i><b>4.2</b> The Regression Setup</a></li>
<li class="chapter" data-level="4.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#reducible-and-irreducible-error"><i class="fa fa-check"></i><b>4.3</b> Reducible and Irreducible Error</a></li>
<li class="chapter" data-level="4.4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#bias-variance-decomposition"><i class="fa fa-check"></i><b>4.4</b> Bias-Variance Decomposition</a></li>
<li class="chapter" data-level="4.5" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#using-simulation-to-estimate-bias-and-variance"><i class="fa fa-check"></i><b>4.5</b> Using Simulation to Estimate Bias and Variance</a></li>
<li class="chapter" data-level="4.6" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#estimating-expected-prediction-error"><i class="fa fa-check"></i><b>4.6</b> Estimating Expected Prediction Error</a></li>
<li class="chapter" data-level="4.7" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#model-flexibility"><i class="fa fa-check"></i><b>4.7</b> Model Flexibility</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#linear-models"><i class="fa fa-check"></i><b>4.7.1</b> Linear Models</a></li>
<li class="chapter" data-level="4.7.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#k-nearest-neighbors-1"><i class="fa fa-check"></i><b>4.7.2</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.7.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#decision-trees-1"><i class="fa fa-check"></i><b>4.7.3</b> Decision Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-overview.html"><a href="regression-overview.html"><i class="fa fa-check"></i><b>5</b> Regression Overview</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regression-overview.html"><a href="regression-overview.html#the-goal"><i class="fa fa-check"></i><b>5.1</b> The Goal</a></li>
<li class="chapter" data-level="5.2" data-path="regression-overview.html"><a href="regression-overview.html#general-strategy"><i class="fa fa-check"></i><b>5.2</b> General Strategy</a></li>
<li class="chapter" data-level="5.3" data-path="regression-overview.html"><a href="regression-overview.html#aglorithms"><i class="fa fa-check"></i><b>5.3</b> Aglorithms</a></li>
<li class="chapter" data-level="5.4" data-path="regression-overview.html"><a href="regression-overview.html#model-flexibility-1"><i class="fa fa-check"></i><b>5.4</b> Model Flexibility</a></li>
<li class="chapter" data-level="5.5" data-path="regression-overview.html"><a href="regression-overview.html#overfitting"><i class="fa fa-check"></i><b>5.5</b> Overfitting</a></li>
<li class="chapter" data-level="5.6" data-path="regression-overview.html"><a href="regression-overview.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.6</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="5.7" data-path="regression-overview.html"><a href="regression-overview.html#no-free-lunch"><i class="fa fa-check"></i><b>5.7</b> No Free Lunch</a></li>
<li class="chapter" data-level="5.8" data-path="regression-overview.html"><a href="regression-overview.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>5.8</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>6</b> Classification</a>
<ul>
<li class="chapter" data-level="6.1" data-path="classification.html"><a href="classification.html#r-setup-and-source-3"><i class="fa fa-check"></i><b>6.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="6.2" data-path="classification.html"><a href="classification.html#data-setup"><i class="fa fa-check"></i><b>6.2</b> Data Setup</a></li>
<li class="chapter" data-level="6.3" data-path="classification.html"><a href="classification.html#mathematical-setup-2"><i class="fa fa-check"></i><b>6.3</b> Mathematical Setup</a></li>
<li class="chapter" data-level="6.4" data-path="classification.html"><a href="classification.html#example"><i class="fa fa-check"></i><b>6.4</b> Example</a></li>
<li class="chapter" data-level="6.5" data-path="classification.html"><a href="classification.html#bayes-classifier"><i class="fa fa-check"></i><b>6.5</b> Bayes Classifier</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="classification.html"><a href="classification.html#bayes-error-rate"><i class="fa fa-check"></i><b>6.5.1</b> Bayes Error Rate</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="classification.html"><a href="classification.html#building-a-classifier"><i class="fa fa-check"></i><b>6.6</b> Building a Classifier</a></li>
<li class="chapter" data-level="6.7" data-path="classification.html"><a href="classification.html#modeling"><i class="fa fa-check"></i><b>6.7</b> Modeling</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="classification.html"><a href="classification.html#linear-models-3"><i class="fa fa-check"></i><b>6.7.1</b> Linear Models</a></li>
<li class="chapter" data-level="6.7.2" data-path="classification.html"><a href="classification.html#k-nearest-neighbors-4"><i class="fa fa-check"></i><b>6.7.2</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="6.7.3" data-path="classification.html"><a href="classification.html#decision-trees-3"><i class="fa fa-check"></i><b>6.7.3</b> Decision Trees</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="classification.html"><a href="classification.html#classification-metrics"><i class="fa fa-check"></i><b>6.8</b> Classification Metrics</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="classification.html"><a href="classification.html#misclassification"><i class="fa fa-check"></i><b>6.8.1</b> Misclassification</a></li>
<li class="chapter" data-level="6.8.2" data-path="classification.html"><a href="classification.html#accuracy"><i class="fa fa-check"></i><b>6.8.2</b> Accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html"><i class="fa fa-check"></i><b>7</b> Nonparametric Classification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html#example-knn-on-simulated-data"><i class="fa fa-check"></i><b>7.1</b> Example: KNN on Simulated Data</a></li>
<li class="chapter" data-level="7.2" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html#example-decision-tree-on-penguin-data"><i class="fa fa-check"></i><b>7.2</b> Example: Decision Tree on Penguin Data</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Logistic Regression</a></li>
<li class="chapter" data-level="9" data-path="binary-classification.html"><a href="binary-classification.html"><i class="fa fa-check"></i><b>9</b> Binary Classification</a>
<ul>
<li class="chapter" data-level="9.1" data-path="binary-classification.html"><a href="binary-classification.html#r-setup-and-source-4"><i class="fa fa-check"></i><b>9.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="9.2" data-path="binary-classification.html"><a href="binary-classification.html#breast-cancer-data"><i class="fa fa-check"></i><b>9.2</b> Breast Cancer Data</a></li>
<li class="chapter" data-level="9.3" data-path="binary-classification.html"><a href="binary-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>9.3</b> Confusion Matrix</a></li>
<li class="chapter" data-level="9.4" data-path="binary-classification.html"><a href="binary-classification.html#binary-classification-metrics"><i class="fa fa-check"></i><b>9.4</b> Binary Classification Metrics</a></li>
<li class="chapter" data-level="9.5" data-path="binary-classification.html"><a href="binary-classification.html#probability-cutoff"><i class="fa fa-check"></i><b>9.5</b> Probability Cutoff</a></li>
<li class="chapter" data-level="9.6" data-path="binary-classification.html"><a href="binary-classification.html#r-packages-and-function"><i class="fa fa-check"></i><b>9.6</b> R Packages and Function</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="generative.html"><a href="generative.html"><i class="fa fa-check"></i><b>10</b> Generative Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="generative.html"><a href="generative.html#r-setup-and-source-5"><i class="fa fa-check"></i><b>10.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="10.2" data-path="generative.html"><a href="generative.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>10.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="10.3" data-path="generative.html"><a href="generative.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>10.3</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="10.4" data-path="generative.html"><a href="generative.html#naive-bayes"><i class="fa fa-check"></i><b>10.4</b> Naive Bayes</a></li>
<li class="chapter" data-level="10.5" data-path="generative.html"><a href="generative.html#categorical-features"><i class="fa fa-check"></i><b>10.5</b> Categorical Features</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>11</b> Cross-Validation</a></li>
<li class="chapter" data-level="12" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>12</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.1" data-path="regularization.html"><a href="regularization.html#ridge-regression"><i class="fa fa-check"></i><b>12.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="12.2" data-path="regularization.html"><a href="regularization.html#lasso"><i class="fa fa-check"></i><b>12.2</b> Lasso</a></li>
<li class="chapter" data-level="12.3" data-path="regularization.html"><a href="regularization.html#broom"><i class="fa fa-check"></i><b>12.3</b> <code>broom</code></a></li>
<li class="chapter" data-level="12.4" data-path="regularization.html"><a href="regularization.html#simulated-data-p-n"><i class="fa fa-check"></i><b>12.4</b> Simulated Data, <span class="math inline">\(p &gt; n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>13</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>13.1</b> Bagging</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#reading"><i class="fa fa-check"></i><b>13.1.1</b> Reading</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>13.2</b> Random Forest</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#reading-1"><i class="fa fa-check"></i><b>13.2.1</b> Reading</a></li>
<li class="chapter" data-level="13.2.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#video"><i class="fa fa-check"></i><b>13.2.2</b> Video</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>13.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#reading-2"><i class="fa fa-check"></i><b>13.3.1</b> Reading</a></li>
<li class="chapter" data-level="13.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#video-1"><i class="fa fa-check"></i><b>13.3.2</b> Video</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="supervised-overview.html"><a href="supervised-overview.html"><i class="fa fa-check"></i><b>14</b> Supervised Learning Overview II</a>
<ul>
<li class="chapter" data-level="14.1" data-path="supervised-overview.html"><a href="supervised-overview.html#classification-2"><i class="fa fa-check"></i><b>14.1</b> Classification</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="supervised-overview.html"><a href="supervised-overview.html#tuning"><i class="fa fa-check"></i><b>14.1.1</b> Tuning</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="supervised-overview.html"><a href="supervised-overview.html#regression-1"><i class="fa fa-check"></i><b>14.2</b> Regression</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="supervised-overview.html"><a href="supervised-overview.html#methods"><i class="fa fa-check"></i><b>14.2.1</b> Methods</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="supervised-overview.html"><a href="supervised-overview.html#external-links"><i class="fa fa-check"></i><b>14.3</b> External Links</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="additional-reading.html"><a href="additional-reading.html"><i class="fa fa-check"></i><b>A</b> Additional Reading</a>
<ul>
<li class="chapter" data-level="A.1" data-path="additional-reading.html"><a href="additional-reading.html#books"><i class="fa fa-check"></i><b>A.1</b> Books</a></li>
<li class="chapter" data-level="A.2" data-path="additional-reading.html"><a href="additional-reading.html#papers"><i class="fa fa-check"></i><b>A.2</b> Papers</a></li>
<li class="chapter" data-level="A.3" data-path="additional-reading.html"><a href="additional-reading.html#blog-posts"><i class="fa fa-check"></i><b>A.3</b> Blog Posts</a></li>
<li class="chapter" data-level="A.4" data-path="additional-reading.html"><a href="additional-reading.html#miscellaneous"><i class="fa fa-check"></i><b>A.4</b> Miscellaneous</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="computing.html"><a href="computing.html"><i class="fa fa-check"></i><b>B</b> Computing</a>
<ul>
<li class="chapter" data-level="B.1" data-path="computing.html"><a href="computing.html#reading-3"><i class="fa fa-check"></i><b>B.1</b> Reading</a></li>
<li class="chapter" data-level="B.2" data-path="computing.html"><a href="computing.html#additional-resources"><i class="fa fa-check"></i><b>B.2</b> Additional Resources</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="computing.html"><a href="computing.html#r"><i class="fa fa-check"></i><b>B.2.1</b> R</a></li>
<li class="chapter" data-level="B.2.2" data-path="computing.html"><a href="computing.html#rstudio"><i class="fa fa-check"></i><b>B.2.2</b> RStudio</a></li>
<li class="chapter" data-level="B.2.3" data-path="computing.html"><a href="computing.html#r-markdown"><i class="fa fa-check"></i><b>B.2.3</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="computing.html"><a href="computing.html#stat-432-idioms"><i class="fa fa-check"></i><b>B.3</b> STAT 432 Idioms</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="computing.html"><a href="computing.html#dont-restore-old-workspaces"><i class="fa fa-check"></i><b>B.3.1</b> Don’t Restore Old Workspaces</a></li>
<li class="chapter" data-level="B.3.2" data-path="computing.html"><a href="computing.html#r-versions"><i class="fa fa-check"></i><b>B.3.2</b> R Versions</a></li>
<li class="chapter" data-level="B.3.3" data-path="computing.html"><a href="computing.html#packages-1"><i class="fa fa-check"></i><b>B.3.3</b> Packages</a></li>
<li class="chapter" data-level="B.3.4" data-path="computing.html"><a href="computing.html#code-style"><i class="fa fa-check"></i><b>B.3.4</b> Code Style</a></li>
<li class="chapter" data-level="B.3.5" data-path="computing.html"><a href="computing.html#reference-style"><i class="fa fa-check"></i><b>B.3.5</b> Reference Style</a></li>
<li class="chapter" data-level="B.3.6" data-path="computing.html"><a href="computing.html#stat-432-r-style-overrides"><i class="fa fa-check"></i><b>B.3.6</b> STAT 432 R Style Overrides</a></li>
<li class="chapter" data-level="B.3.7" data-path="computing.html"><a href="computing.html#stat-432-r-markdown-style"><i class="fa fa-check"></i><b>B.3.7</b> STAT 432 R Markdown Style</a></li>
<li class="chapter" data-level="B.3.8" data-path="computing.html"><a href="computing.html#style-heuristics"><i class="fa fa-check"></i><b>B.3.8</b> Style Heuristics</a></li>
<li class="chapter" data-level="B.3.9" data-path="computing.html"><a href="computing.html#objects-and-functions"><i class="fa fa-check"></i><b>B.3.9</b> Objects and Functions</a></li>
<li class="chapter" data-level="B.3.10" data-path="computing.html"><a href="computing.html#print-versus-return"><i class="fa fa-check"></i><b>B.3.10</b> Print versus Return</a></li>
<li class="chapter" data-level="B.3.11" data-path="computing.html"><a href="computing.html#help"><i class="fa fa-check"></i><b>B.3.11</b> Help</a></li>
<li class="chapter" data-level="B.3.12" data-path="computing.html"><a href="computing.html#keyboard-shortcuts"><i class="fa fa-check"></i><b>B.3.12</b> Keyboard Shortcuts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>C</b> Probability</a>
<ul>
<li class="chapter" data-level="C.1" data-path="probability.html"><a href="probability.html#reading-4"><i class="fa fa-check"></i><b>C.1</b> Reading</a></li>
<li class="chapter" data-level="C.2" data-path="probability.html"><a href="probability.html#probability-models"><i class="fa fa-check"></i><b>C.2</b> Probability Models</a></li>
<li class="chapter" data-level="C.3" data-path="probability.html"><a href="probability.html#probability-axioms"><i class="fa fa-check"></i><b>C.3</b> Probability Axioms</a></li>
<li class="chapter" data-level="C.4" data-path="probability.html"><a href="probability.html#probability-rules"><i class="fa fa-check"></i><b>C.4</b> Probability Rules</a></li>
<li class="chapter" data-level="C.5" data-path="probability.html"><a href="probability.html#random-variables"><i class="fa fa-check"></i><b>C.5</b> Random Variables</a>
<ul>
<li class="chapter" data-level="C.5.1" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>C.5.1</b> Distributions</a></li>
<li class="chapter" data-level="C.5.2" data-path="probability.html"><a href="probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>C.5.2</b> Discrete Random Variables</a></li>
<li class="chapter" data-level="C.5.3" data-path="probability.html"><a href="probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>C.5.3</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="C.5.4" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>C.5.4</b> Distributions in R</a></li>
<li class="chapter" data-level="C.5.5" data-path="probability.html"><a href="probability.html#several-random-variables"><i class="fa fa-check"></i><b>C.5.5</b> Several Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="probability.html"><a href="probability.html#expectations"><i class="fa fa-check"></i><b>C.6</b> Expectations</a></li>
<li class="chapter" data-level="C.7" data-path="probability.html"><a href="probability.html#likelihood"><i class="fa fa-check"></i><b>C.7</b> Likelihood</a></li>
<li class="chapter" data-level="C.8" data-path="probability.html"><a href="probability.html#additional-references"><i class="fa fa-check"></i><b>C.8</b> Additional References</a>
<ul>
<li class="chapter" data-level="C.8.1" data-path="probability.html"><a href="probability.html#videos"><i class="fa fa-check"></i><b>C.8.1</b> Videos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>D</b> Statistics</a>
<ul>
<li class="chapter" data-level="D.1" data-path="statistics.html"><a href="statistics.html#reading-5"><i class="fa fa-check"></i><b>D.1</b> Reading</a></li>
<li class="chapter" data-level="D.2" data-path="statistics.html"><a href="statistics.html#statistics-1"><i class="fa fa-check"></i><b>D.2</b> Statistics</a></li>
<li class="chapter" data-level="D.3" data-path="statistics.html"><a href="statistics.html#estimators"><i class="fa fa-check"></i><b>D.3</b> Estimators</a>
<ul>
<li class="chapter" data-level="D.3.1" data-path="statistics.html"><a href="statistics.html#properties"><i class="fa fa-check"></i><b>D.3.1</b> Properties</a></li>
<li class="chapter" data-level="D.3.2" data-path="statistics.html"><a href="statistics.html#example-mse-of-an-estimator"><i class="fa fa-check"></i><b>D.3.2</b> Example: MSE of an Estimator</a></li>
<li class="chapter" data-level="D.3.3" data-path="statistics.html"><a href="statistics.html#estimation-methods"><i class="fa fa-check"></i><b>D.3.3</b> Estimation Methods</a></li>
<li class="chapter" data-level="D.3.4" data-path="statistics.html"><a href="statistics.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>D.3.4</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="D.3.5" data-path="statistics.html"><a href="statistics.html#method-of-moments"><i class="fa fa-check"></i><b>D.3.5</b> Method of Moments</a></li>
<li class="chapter" data-level="D.3.6" data-path="statistics.html"><a href="statistics.html#empirical-distribution-function"><i class="fa fa-check"></i><b>D.3.6</b> Empirical Distribution Function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://daviddalpiaz.org" target="blank">&copy; 2020 David Dalpiaz</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basics of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised-learning-overview-ii" class="section level1" number="14">
<h1><span class="header-section-number">Chapter 14</span> Supervised Learning Overview II</h1>
<p>This chapter is currently just a bit of a placeholder that introduces <code>caret</code> as a systematized way of doing supervised learning.</p>
<hr />
<p>Now that we have seen a number of classification and regression methods, and introduced cross-validation, we see the general outline of a predictive analysis:</p>
<ul>
<li>Test-train split the available data
<ul>
<li>Consider a learning method(s)
<ul>
<li>Decide on a set of candidate models (specify possible tuning parameters for each method)</li>
<li>Use resampling to find the “best model” by choosing the values of the tuning parameters</li>
</ul></li>
<li>Use chosen model to make predictions</li>
<li>Calculate relevant metrics on the test data</li>
</ul></li>
</ul>
<p>At face value it would seem like it should be easy to repeat this process for a number of different methods, however we have run into a number of difficulties attempting to do so with <code>R</code>.</p>
<ul>
<li>The <code>predict()</code> function seems to have a different behavior for each new method we see.</li>
<li>Many methods have different cross-validation functions, or worse yet, no built-in process for cross-validation. (We’ve been writing code ourselves to perform cross-validation.)</li>
<li>Not all methods expect the same data format. Some methods do not use formula syntax.</li>
<li>Different methods have different handling of categorical predictors. Some methods cannot handle factor variables.</li>
</ul>
<p>Thankfully, the <code>R</code> community has essentially provided a silver bullet for these issues, the <a href="http://topepo.github.io/caret/"><code>caret</code></a> package. Returning to the above list, we will see that a number of these tasks are directly addressed in the <code>caret</code> package.</p>
<ul>
<li>Test-train split the available data
<ul>
<li><code>createDataPartition()</code> will take the place of our manual data splitting. It will also do some extra work to ensure that the train and test samples are somewhat similar, especially with respect to the distribution of the response variable.</li>
</ul></li>
<li>Specify possible tuning parameters for method
<ul>
<li><code>expand.grid()</code> is not a function in <code>caret</code>, but we will get in the habit of using it to specify a grid of tuning parameters.</li>
</ul></li>
<li>Use resampling to find the “best model” by choosing the values of the tuning parameters
<ul>
<li><code>trainControl()</code> will specify the resampling scheme</li>
<li><code>train()</code> is the workhorse of <code>caret</code>. It takes the following information then trains (tunes) the requested model:
<ul>
<li><code>form</code>, a formula, such as <code>y ~ .</code>
<ul>
<li>This specifies the response and which predictors (or transformations of) should be used.</li>
</ul></li>
<li><code>data</code>, the data used for training</li>
<li><code>trControl</code> which specifies the resampling scheme, that is, how cross-validation should be performed to find the best values of the tuning parameters</li>
<li><code>preProcess</code> which allows for specification of data pre-processing such as centering and scaling</li>
<li><code>method</code>, a statistical learning method from <a href="https://topepo.github.io/caret/available-models.html">a long list of available models</a></li>
<li><code>tuneGrid</code> which specifies the tuning parameters to train over</li>
</ul></li>
</ul></li>
<li>Use chosen model to make predictions
<ul>
<li><code>predict()</code> used on objects of type <code>train</code> will be truly magical!</li>
</ul></li>
</ul>
<div id="classification-2" class="section level2" number="14.1">
<h2><span class="header-section-number">14.1</span> Classification</h2>
<p>To illustrate <code>caret</code>, first for classification, we will use the <code>Default</code> data from the <code>ISLR</code> package.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="supervised-overview.html#cb426-1"></a><span class="kw">data</span>(Default, <span class="dt">package =</span> <span class="st">&quot;ISLR&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb427-1"><a href="supervised-overview.html#cb427-1"></a><span class="kw">library</span>(caret)</span></code></pre></div>
<p>We first test-train split the data using <code>createDataPartition</code>. Here we are using 75% of the data for training.</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="supervised-overview.html#cb428-1"></a><span class="kw">set.seed</span>(<span class="dv">430</span>)</span>
<span id="cb428-2"><a href="supervised-overview.html#cb428-2"></a>default_idx =<span class="st"> </span><span class="kw">createDataPartition</span>(Default<span class="op">$</span>default, <span class="dt">p =</span> <span class="fl">0.75</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>)</span>
<span id="cb428-3"><a href="supervised-overview.html#cb428-3"></a>default_trn =<span class="st"> </span>Default[default_idx, ]</span>
<span id="cb428-4"><a href="supervised-overview.html#cb428-4"></a>default_tst =<span class="st"> </span>Default[<span class="op">-</span>default_idx, ]</span></code></pre></div>
<p>At first glance, it might appear as if the use of <code>createDataPartition()</code> is no different than our previous use of <code>sample()</code>. However, <code>createDataPartition()</code> tries to ensure a split that has a similar distribution of the supplied variable in both datasets. See the documentation for details.</p>
<p>After splitting the data, we can begin training a number of models. We begin with a simple additive logistic regression.</p>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb429-1"><a href="supervised-overview.html#cb429-1"></a>default_glm_mod =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb429-2"><a href="supervised-overview.html#cb429-2"></a>  <span class="dt">form =</span> default <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb429-3"><a href="supervised-overview.html#cb429-3"></a>  <span class="dt">data =</span> default_trn,</span>
<span id="cb429-4"><a href="supervised-overview.html#cb429-4"></a>  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>),</span>
<span id="cb429-5"><a href="supervised-overview.html#cb429-5"></a>  <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,</span>
<span id="cb429-6"><a href="supervised-overview.html#cb429-6"></a>  <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span></span>
<span id="cb429-7"><a href="supervised-overview.html#cb429-7"></a>)</span></code></pre></div>
<p>Here, we have supplied four arguments to the <code>train()</code> function form the <code>caret</code> package.</p>
<ul>
<li><code>form = default ~ .</code> specifies the <code>default</code> variable as the response. It also indicates that all available predictors should be used.</li>
<li><code>data = default_trn</code> specifies that training will be down with the <code>default_trn</code> data</li>
<li><code>trControl = trainControl(method = "cv", number = 5)</code> specifies that we will be using 5-fold cross-validation.</li>
<li><code>method = glm</code> specifies that we will fit a generalized linear model.</li>
</ul>
<p>The <code>method</code> essentially specifies both the model (and more specifically the function to fit said model in <code>R</code>) and package that will be used. The <code>train()</code> function is essentially a wrapper around whatever method we chose. In this case, the function is the base <code>R</code> function <code>glm()</code>, so no additional package is required. When a method requires a function from a certain package, that package will need to be installed. See the <a href="https://topepo.github.io/caret/available-models.html">list of available models</a> for package information.</p>
<p>The list that we have passed to the <code>trControl</code> argument is created using the <code>trainControl()</code> function from <code>caret</code>. The <code>trainControl()</code> function is a powerful tool for specifying a number of the training choices required by <code>train()</code>, in particular the resampling scheme.</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="supervised-overview.html#cb430-1"></a><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]</span></code></pre></div>
<pre><code>## $method
## [1] &quot;cv&quot;
## 
## $number
## [1] 5
## 
## $repeats
## [1] NA</code></pre>
<p>Here we see just the first three elements of this list, which are related to how the resampling will be done. These are the three elements that we will be most interested in. Here, only the first two are relevant.</p>
<ul>
<li><code>method</code> specifies how resampling will be done. Examples include <code>cv</code>, <code>boot</code>, <code>LOOCV</code>, <code>repeatedcv</code>, and <code>oob</code>.</li>
<li><code>number</code> specifies the number of times resampling should be done for methods that require resample, such as, <code>cv</code> and <code>boot</code>.</li>
<li><code>repeats</code> specifies the number of times to repeat resampling for methods such as <code>repeatedcv</code></li>
</ul>
<p>For details on the full capabilities of this function, see the relevant documentation. The out-of-bag, <code>oob</code> which is a sort of automatic resampling for certain statistical learning methods, will be introduced later.</p>
<p>We’ve also passed an additional argument of <code>"binomial"</code> to <code>family</code>. This isn’t actually an argument for <code>train()</code>, but an additional argument for the method <code>glm</code>. In actuality, we don’t need to specify the family. Since <code>default</code> is a factor variable, <code>caret</code> automatically detects that we are trying to perform classification, and would automatically use <code>family = "binomial"</code>. This isn’t the case if we were simply using <code>glm()</code>.</p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="supervised-overview.html#cb432-1"></a>default_glm_mod</span></code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 7501 samples
##    3 predictor
##    2 classes: &#39;No&#39;, &#39;Yes&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 6001, 6000, 6001, 6001, 6001 
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.9728038  0.4248613</code></pre>
<p>Called the stored <code>train()</code> object summarizes the training that we have done. We see that we used 7501 observations that had a binary class response and three predictors. We have not done any data pre-processing, and have utilized 5-fold cross-validation. The cross-validated accuracy is reported. Note that, <code>caret</code> is an optimist, and prefers to report accuracy (proportion of correct classifications) instead of the error that we often considered before (proportion of incorrect classifications).</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="supervised-overview.html#cb434-1"></a><span class="kw">names</span>(default_glm_mod)</span></code></pre></div>
<pre><code>##  [1] &quot;method&quot;       &quot;modelInfo&quot;    &quot;modelType&quot;    &quot;results&quot;      &quot;pred&quot;        
##  [6] &quot;bestTune&quot;     &quot;call&quot;         &quot;dots&quot;         &quot;metric&quot;       &quot;control&quot;     
## [11] &quot;finalModel&quot;   &quot;preProcess&quot;   &quot;trainingData&quot; &quot;resample&quot;     &quot;resampledCM&quot; 
## [16] &quot;perfNames&quot;    &quot;maximize&quot;     &quot;yLimits&quot;      &quot;times&quot;        &quot;levels&quot;      
## [21] &quot;terms&quot;        &quot;coefnames&quot;    &quot;contrasts&quot;    &quot;xlevels&quot;</code></pre>
<p>We see that there is a wealth of information stored in the list returned by <code>train()</code>. Two elements that we will often be interested in are <code>results</code> and <code>finalModel</code>.</p>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="supervised-overview.html#cb436-1"></a>default_glm_mod<span class="op">$</span>results</span></code></pre></div>
<pre><code>##   parameter  Accuracy     Kappa  AccuracySD    KappaSD
## 1      none 0.9728038 0.4248613 0.002956624 0.07544475</code></pre>
<p>The <code>resutls</code> show some more detailed results, in particular <code>AccuracySD</code> which gives us an estimate of the uncertainty in our accuracy estimate.</p>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="supervised-overview.html#cb438-1"></a>default_glm_mod<span class="op">$</span>finalModel</span></code></pre></div>
<pre><code>## 
## Call:  NULL
## 
## Coefficients:
## (Intercept)   studentYes      balance       income  
##  -1.070e+01   -6.992e-01    5.676e-03    4.383e-07  
## 
## Degrees of Freedom: 7500 Total (i.e. Null);  7497 Residual
## Null Deviance:       2192 
## Residual Deviance: 1186  AIC: 1194</code></pre>
<p>The <code>finalModel</code> is a model object, in this case, the object returned from <code>glm()</code>. This final model, is fit to all of the supplied training data. This model object is often used when we call certain relevant functions on the object returned by <code>train()</code>, such as <code>summary()</code></p>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="supervised-overview.html#cb440-1"></a><span class="kw">summary</span>(default_glm_mod)</span></code></pre></div>
<pre><code>## 
## Call:
## NULL
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1317  -0.1420  -0.0568  -0.0210   3.7348  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.070e+01  5.607e-01 -19.079  &lt; 2e-16 ***
## studentYes  -6.992e-01  2.708e-01  -2.582  0.00984 ** 
## balance      5.676e-03  2.644e-04  21.471  &lt; 2e-16 ***
## income       4.383e-07  9.389e-06   0.047  0.96276    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2192.2  on 7500  degrees of freedom
## Residual deviance: 1185.8  on 7497  degrees of freedom
## AIC: 1193.8
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p>We see that this summary is what we had seen previously from objects of type <code>glm</code>.</p>
<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb442-1"><a href="supervised-overview.html#cb442-1"></a>calc_acc =<span class="st"> </span><span class="cf">function</span>(actual, predicted) {</span>
<span id="cb442-2"><a href="supervised-overview.html#cb442-2"></a>  <span class="kw">mean</span>(actual <span class="op">==</span><span class="st"> </span>predicted)</span>
<span id="cb442-3"><a href="supervised-overview.html#cb442-3"></a>}</span></code></pre></div>
<p>To obtain test accuracy, we will need to make predictions on the test data. With the object returned by <code>train()</code>, this is extremely easy.</p>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb443-1"><a href="supervised-overview.html#cb443-1"></a><span class="kw">head</span>(<span class="kw">predict</span>(default_glm_mod, <span class="dt">newdata =</span> default_tst))</span></code></pre></div>
<pre><code>## [1] No No No No No No
## Levels: No Yes</code></pre>
<p>We see that by default, the <code>predict()</code> function is returning classifications. This will be true no matter what method we use!</p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb445-1"><a href="supervised-overview.html#cb445-1"></a><span class="co"># test acc</span></span>
<span id="cb445-2"><a href="supervised-overview.html#cb445-2"></a><span class="kw">calc_acc</span>(<span class="dt">actual =</span> default_tst<span class="op">$</span>default,</span>
<span id="cb445-3"><a href="supervised-overview.html#cb445-3"></a>         <span class="dt">predicted =</span> <span class="kw">predict</span>(default_glm_mod, <span class="dt">newdata =</span> default_tst))</span></code></pre></div>
<pre><code>## [1] 0.9735894</code></pre>
<p>If instead of the default behavior of returning classifications, we instead wanted predicted probabilities, we simply specify <code>type = "prob"</code>.</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="supervised-overview.html#cb447-1"></a><span class="co"># get probs</span></span>
<span id="cb447-2"><a href="supervised-overview.html#cb447-2"></a><span class="kw">head</span>(<span class="kw">predict</span>(default_glm_mod, <span class="dt">newdata =</span> default_trn, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>))</span></code></pre></div>
<pre><code>##           No          Yes
## 2  0.9988332 1.166819e-03
## 4  0.9995369 4.630821e-04
## 7  0.9975279 2.472097e-03
## 8  0.9988855 1.114516e-03
## 10 0.9999771 2.290522e-05
## 11 0.9999887 1.134693e-05</code></pre>
<p>Notice that this returns the probabilities for all possible classes, in this case <code>No</code> and <code>Yes</code>. Again, this will be true for all methods! This is especially useful for multi-class data!.</p>
<div id="tuning" class="section level3" number="14.1.1">
<h3><span class="header-section-number">14.1.1</span> Tuning</h3>
<p>Since logistic regression has no tuning parameters, we haven’t really highlighted the full potential of <code>caret</code>. We’ve essentially used it to obtain cross-validated results, and for the more well-behaved <code>predict()</code> function. These are excellent improvements over our previous methods, but the real power of <code>caret</code> is its ability to provide a framework for tuning model.</p>
<p>To illustrate tuning, we now use <code>knn</code> as our method, which performs <span class="math inline">\(k\)</span>-nearest neighbors.</p>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="supervised-overview.html#cb449-1"></a>default_knn_mod =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb449-2"><a href="supervised-overview.html#cb449-2"></a>  default <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb449-3"><a href="supervised-overview.html#cb449-3"></a>  <span class="dt">data =</span> default_trn,</span>
<span id="cb449-4"><a href="supervised-overview.html#cb449-4"></a>  <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb449-5"><a href="supervised-overview.html#cb449-5"></a>  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>)</span>
<span id="cb449-6"><a href="supervised-overview.html#cb449-6"></a>)</span></code></pre></div>
<p>First, note that we are using formula syntax here, where previously we needed to create separate response and predictors matrices. Also, we’re using a factor variable as a predictor, and <code>caret</code> seems to be taking care of this automatically.</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="supervised-overview.html#cb450-1"></a>default_knn_mod</span></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 7501 samples
##    3 predictor
##    2 classes: &#39;No&#39;, &#39;Yes&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 6001, 6000, 6001, 6001, 6001 
## Resampling results across tuning parameters:
## 
##   k  Accuracy   Kappa    
##   5  0.9677377  0.2125623
##   7  0.9664047  0.1099835
##   9  0.9680044  0.1223319
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 9.</code></pre>
<p>Here we are again using 5-fold cross-validation and no pre-processing. Notice that we now have multiple results, for <code>k = 5</code>, <code>k = 7</code>, and <code>k = 9</code>.</p>
<p>Let’s modifying this training by introducing pre-processing, and specifying our own tuning parameters, instead of the default values above.</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="supervised-overview.html#cb452-1"></a>default_knn_mod =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb452-2"><a href="supervised-overview.html#cb452-2"></a>  default <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb452-3"><a href="supervised-overview.html#cb452-3"></a>  <span class="dt">data =</span> default_trn,</span>
<span id="cb452-4"><a href="supervised-overview.html#cb452-4"></a>  <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb452-5"><a href="supervised-overview.html#cb452-5"></a>  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>),</span>
<span id="cb452-6"><a href="supervised-overview.html#cb452-6"></a>  <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</span>
<span id="cb452-7"><a href="supervised-overview.html#cb452-7"></a>  <span class="dt">tuneGrid =</span> <span class="kw">expand.grid</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">101</span>, <span class="dt">by =</span> <span class="dv">2</span>))</span>
<span id="cb452-8"><a href="supervised-overview.html#cb452-8"></a>)</span></code></pre></div>
<p>Here, we’ve specified that we would like to center and scale the data. Essentially transforming each predictor to have mean 0 and variance 1. The documentation on the <code>preProcess()</code> function provides examples of additional possible pre-processing. IN our call to <code>train()</code> we’re essentially specifying how we would like this function applied to our data.</p>
<p>We’ve also provided a “tuning grid,” in this case, the values of <code>k</code> to try. The <code>tuneGrid</code> argument expects a data frame, which <code>expand.grid()</code> returns. We don’t actually need <code>expand.grid()</code> for this example, but it will be a useful habit to develop when we move to methods with multiple tuning parameters.</p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a href="supervised-overview.html#cb453-1"></a><span class="kw">head</span>(default_knn_mod<span class="op">$</span>results, <span class="dv">5</span>)</span></code></pre></div>
<pre><code>##   k  Accuracy     Kappa  AccuracySD    KappaSD
## 1 1 0.9544051 0.2843363 0.006566472 0.08989820
## 2 3 0.9662702 0.3633701 0.004317478 0.09324199
## 3 5 0.9710698 0.4122812 0.004076929 0.11219277
## 4 7 0.9712032 0.4031654 0.003576867 0.09624856
## 5 9 0.9721364 0.4250236 0.003987725 0.09388466</code></pre>
<p>Since how we have a large number of results, display the entire <code>results</code> would create a lot of clutter. Instead, we can plot the tuning results by calling <code>plot()</code> on the object returned by <code>train()</code>.</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a href="supervised-overview.html#cb455-1"></a><span class="kw">plot</span>(default_knn_mod)</span></code></pre></div>
<p><img src="supervised-overview_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>By default, <code>caret</code> utilizes the <code>lattice</code> graphics package to create these plots. Recently, additional support for <code>ggplot2</code> style graphics has been added for some plots.</p>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="supervised-overview.html#cb456-1"></a><span class="kw">ggplot</span>(default_knn_mod) <span class="op">+</span><span class="st"> </span><span class="kw">theme_bw</span>()</span></code></pre></div>
<p><img src="supervised-overview_files/figure-html/unnamed-chunk-20-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Now that we are dealing with a tuning parameter, <code>train()</code> determines the best value of those considered, by default selecting the best (highest cross-validated) accuracy, and returning that value as <code>bestTune</code>.</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="supervised-overview.html#cb457-1"></a>default_knn_mod<span class="op">$</span>bestTune</span></code></pre></div>
<pre><code>##    k
## 6 11</code></pre>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb459-1"><a href="supervised-overview.html#cb459-1"></a>get_best_result =<span class="st"> </span><span class="cf">function</span>(caret_fit) {</span>
<span id="cb459-2"><a href="supervised-overview.html#cb459-2"></a>  best =<span class="st"> </span><span class="kw">which</span>(<span class="kw">rownames</span>(caret_fit<span class="op">$</span>results) <span class="op">==</span><span class="st"> </span><span class="kw">rownames</span>(caret_fit<span class="op">$</span>bestTune))</span>
<span id="cb459-3"><a href="supervised-overview.html#cb459-3"></a>  best_result =<span class="st"> </span>caret_fit<span class="op">$</span>results[best, ]</span>
<span id="cb459-4"><a href="supervised-overview.html#cb459-4"></a>  <span class="kw">rownames</span>(best_result) =<span class="st"> </span><span class="ot">NULL</span></span>
<span id="cb459-5"><a href="supervised-overview.html#cb459-5"></a>  best_result</span>
<span id="cb459-6"><a href="supervised-overview.html#cb459-6"></a>}</span></code></pre></div>
<p>Sometimes it will be useful to obtain the results for only that value. The above function does this automatically.</p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="supervised-overview.html#cb460-1"></a><span class="kw">get_best_result</span>(default_knn_mod)</span></code></pre></div>
<pre><code>##    k Accuracy     Kappa  AccuracySD    KappaSD
## 1 11  0.97227 0.4225433 0.002654708 0.07773239</code></pre>
<p>While we did fit a large number of models, the “best” model is stored in <code>finalModel</code>. After this model was determined to be the best via cross-validation, it is then fit to the entire training dataset.</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb462-1"><a href="supervised-overview.html#cb462-1"></a>default_knn_mod<span class="op">$</span>finalModel</span></code></pre></div>
<pre><code>## 11-nearest neighbor model
## Training set outcome distribution:
## 
##   No  Yes 
## 7251  250</code></pre>
<p>With this model, we can again make predictions and obtain predicted probabilities.</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb464-1"><a href="supervised-overview.html#cb464-1"></a><span class="kw">head</span>(<span class="kw">predict</span>(default_knn_mod, <span class="dt">newdata =</span> default_tst, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>))</span></code></pre></div>
<pre><code>##          No        Yes
## 1 1.0000000 0.00000000
## 2 1.0000000 0.00000000
## 3 1.0000000 0.00000000
## 4 1.0000000 0.00000000
## 5 0.9090909 0.09090909
## 6 0.9090909 0.09090909</code></pre>
<p>As an example of a multi-class response consider the following three models fit to the the <code>iris</code> data. Note that the first model is essentially “multinomial logistic regression,” but you might notice it also has a tuning parameter now. (Spoiler: It’s actually a neural network, so you’ll need the <code>nnet</code> package.)</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="supervised-overview.html#cb466-1"></a>iris_log_mod =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb466-2"><a href="supervised-overview.html#cb466-2"></a>  Species <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb466-3"><a href="supervised-overview.html#cb466-3"></a>  <span class="dt">data =</span> iris,</span>
<span id="cb466-4"><a href="supervised-overview.html#cb466-4"></a>  <span class="dt">method =</span> <span class="st">&quot;multinom&quot;</span>,</span>
<span id="cb466-5"><a href="supervised-overview.html#cb466-5"></a>  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>),</span>
<span id="cb466-6"><a href="supervised-overview.html#cb466-6"></a>  <span class="dt">trace =</span> <span class="ot">FALSE</span></span>
<span id="cb466-7"><a href="supervised-overview.html#cb466-7"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="supervised-overview.html#cb467-1"></a>iris_knn_mod =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb467-2"><a href="supervised-overview.html#cb467-2"></a>  Species <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb467-3"><a href="supervised-overview.html#cb467-3"></a>  <span class="dt">data =</span> iris,</span>
<span id="cb467-4"><a href="supervised-overview.html#cb467-4"></a>  <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb467-5"><a href="supervised-overview.html#cb467-5"></a>  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>),</span>
<span id="cb467-6"><a href="supervised-overview.html#cb467-6"></a>  <span class="dt">preProcess =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</span>
<span id="cb467-7"><a href="supervised-overview.html#cb467-7"></a>  <span class="dt">tuneGrid =</span> <span class="kw">expand.grid</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">21</span>, <span class="dt">by =</span> <span class="dv">2</span>))</span>
<span id="cb467-8"><a href="supervised-overview.html#cb467-8"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="supervised-overview.html#cb468-1"></a>iris_qda_mod =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb468-2"><a href="supervised-overview.html#cb468-2"></a>  Species <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb468-3"><a href="supervised-overview.html#cb468-3"></a>  <span class="dt">data =</span> iris,</span>
<span id="cb468-4"><a href="supervised-overview.html#cb468-4"></a>  <span class="dt">method =</span> <span class="st">&quot;qda&quot;</span>,</span>
<span id="cb468-5"><a href="supervised-overview.html#cb468-5"></a>  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>)</span>
<span id="cb468-6"><a href="supervised-overview.html#cb468-6"></a>)</span></code></pre></div>
<p>We can obtain predicted probabilities with these three models. Notice that they give the predicted probability for <strong>each</strong> class, using the same syntax for each model.</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="supervised-overview.html#cb469-1"></a><span class="kw">head</span>(<span class="kw">predict</span>(iris_log_mod, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>))</span></code></pre></div>
<pre><code>##      setosa   versicolor    virginica
## 1 1.0000000 1.526406e-09 2.716417e-36
## 2 0.9999996 3.536476e-07 2.883729e-32
## 3 1.0000000 4.443506e-08 6.103424e-34
## 4 0.9999968 3.163905e-06 7.117010e-31
## 5 1.0000000 1.102983e-09 1.289946e-36
## 6 1.0000000 3.521573e-10 1.344907e-35</code></pre>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="supervised-overview.html#cb471-1"></a><span class="kw">head</span>(<span class="kw">predict</span>(iris_knn_mod, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>))</span></code></pre></div>
<pre><code>##   setosa versicolor virginica
## 1      1          0         0
## 2      1          0         0
## 3      1          0         0
## 4      1          0         0
## 5      1          0         0
## 6      1          0         0</code></pre>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="supervised-overview.html#cb473-1"></a><span class="kw">head</span>(<span class="kw">predict</span>(iris_qda_mod, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>))</span></code></pre></div>
<pre><code>##   setosa   versicolor    virginica
## 1      1 4.918517e-26 2.981541e-41
## 2      1 7.655808e-19 1.311032e-34
## 3      1 1.552279e-21 3.380440e-36
## 4      1 8.300396e-19 8.541858e-32
## 5      1 3.365614e-27 2.010147e-41
## 6      1 1.472533e-26 1.271928e-40</code></pre>
</div>
</div>
<div id="regression-1" class="section level2" number="14.2">
<h2><span class="header-section-number">14.2</span> Regression</h2>
<p>To illustrate the use of <code>caret</code> for regression, we’ll consider some simulated data.</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="supervised-overview.html#cb475-1"></a>gen_some_data =<span class="st"> </span><span class="cf">function</span>(<span class="dt">n_obs =</span> <span class="dv">50</span>) {</span>
<span id="cb475-2"><a href="supervised-overview.html#cb475-2"></a>  x1 =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dt">length.out =</span> n_obs)</span>
<span id="cb475-3"><a href="supervised-overview.html#cb475-3"></a>  x2 =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> n_obs, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">2</span>)</span>
<span id="cb475-4"><a href="supervised-overview.html#cb475-4"></a>  x3 =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;C&quot;</span>), <span class="dt">size =</span> n_obs, <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb475-5"><a href="supervised-overview.html#cb475-5"></a>  x4 =<span class="st"> </span><span class="kw">round</span>(<span class="kw">runif</span>(<span class="dt">n =</span> n_obs, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">5</span>), <span class="dv">1</span>)</span>
<span id="cb475-6"><a href="supervised-overview.html#cb475-6"></a>  x5 =<span class="st"> </span><span class="kw">round</span>(<span class="kw">runif</span>(<span class="dt">n =</span> n_obs, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">5</span>), <span class="dv">0</span>)</span>
<span id="cb475-7"><a href="supervised-overview.html#cb475-7"></a>  y =<span class="st"> </span><span class="kw">round</span>(x1 <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x2 <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(x3 <span class="op">==</span><span class="st"> &quot;B&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n_obs), <span class="dv">3</span>)</span>
<span id="cb475-8"><a href="supervised-overview.html#cb475-8"></a>  <span class="kw">data.frame</span>(y, x1, x2, x3, x4, x5)</span>
<span id="cb475-9"><a href="supervised-overview.html#cb475-9"></a>}</span></code></pre></div>
<p>We first simulate a train and test dataset.</p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb476-1"><a href="supervised-overview.html#cb476-1"></a><span class="kw">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb476-2"><a href="supervised-overview.html#cb476-2"></a>sim_trn =<span class="st"> </span><span class="kw">gen_some_data</span>(<span class="dt">n_obs =</span> <span class="dv">500</span>)</span>
<span id="cb476-3"><a href="supervised-overview.html#cb476-3"></a>sim_tst =<span class="st"> </span><span class="kw">gen_some_data</span>(<span class="dt">n_obs =</span> <span class="dv">5000</span>)</span></code></pre></div>
<p>Fitting <code>knn</code> works nearly identically to its use for classification. Really, the only difference here is that we have a numeric response, which <code>caret</code> understands to be a regression problem.</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="supervised-overview.html#cb477-1"></a>sim_knn_mod =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb477-2"><a href="supervised-overview.html#cb477-2"></a>  y <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb477-3"><a href="supervised-overview.html#cb477-3"></a>  <span class="dt">data =</span> sim_trn,</span>
<span id="cb477-4"><a href="supervised-overview.html#cb477-4"></a>  <span class="dt">method =</span> <span class="st">&quot;knn&quot;</span>,</span>
<span id="cb477-5"><a href="supervised-overview.html#cb477-5"></a>  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>),</span>
<span id="cb477-6"><a href="supervised-overview.html#cb477-6"></a>  <span class="co"># preProcess = c(&quot;center&quot;, &quot;scale&quot;),</span></span>
<span id="cb477-7"><a href="supervised-overview.html#cb477-7"></a>  <span class="dt">tuneGrid =</span> <span class="kw">expand.grid</span>(<span class="dt">k =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">31</span>, <span class="dt">by =</span> <span class="dv">2</span>))</span>
<span id="cb477-8"><a href="supervised-overview.html#cb477-8"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb478-1"><a href="supervised-overview.html#cb478-1"></a>sim_knn_mod<span class="op">$</span>modelType</span></code></pre></div>
<pre><code>## [1] &quot;Regression&quot;</code></pre>
<p>Notice that we’ve commented out the line to perform pre-processing. Can you figure out why?</p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="supervised-overview.html#cb480-1"></a><span class="kw">get_best_result</span>(sim_knn_mod)</span></code></pre></div>
<pre><code>##   k   RMSE  Rsquared      MAE   RMSESD  RsquaredSD     MAESD
## 1 7 3.6834 0.9863968 2.646447 0.587191 0.002630169 0.3388258</code></pre>
<p>A few things to notice in the results. In addition to the usual RMSE, which is be used to determine the best model, we also have MAE, the <a href="https://en.wikipedia.org/wiki/Mean_absolute_error">mean absolute error</a>. We are also given standard deviations of both of these metrics.</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="supervised-overview.html#cb482-1"></a><span class="kw">plot</span>(sim_knn_mod)</span></code></pre></div>
<p><img src="supervised-overview_files/figure-html/unnamed-chunk-35-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The following plot adds error bars to RMSE estimate for each <code>k</code>.</p>
<p><img src="supervised-overview_files/figure-html/unnamed-chunk-36-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Sometimes, instead of simply picking the model with the best RMSE (or accuracy), we pick the simplest model within one standard error of the model with the best RMSE. Here then, we would consider <code>k = 11</code> instead of <code>k = 7</code> since there isn’t a statistically significant difference. This is potentially a very good idea in practice. By picking a a simpler model, we are essentially at less risk of overfitting, especially since in practice, future data may be slightly different than the data that we are training on. If you’re trying to win a Kaggle competition, this might not be as useful, since often the test and train data come from the exact same source.</p>
<ul>
<li>TODO: additional details about 1-SE rule.</li>
</ul>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="supervised-overview.html#cb483-1"></a>calc_rmse =<span class="st"> </span><span class="cf">function</span>(actual, predicted) {</span>
<span id="cb483-2"><a href="supervised-overview.html#cb483-2"></a>  <span class="kw">sqrt</span>(<span class="kw">mean</span>((actual <span class="op">-</span><span class="st"> </span>predicted) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</span>
<span id="cb483-3"><a href="supervised-overview.html#cb483-3"></a>}</span></code></pre></div>
<p>Since we simulated this data, we have a rather large test dataset. This allows us to compare our cross-validation error estimate, to an estimate using (an impractically large) test set.</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="supervised-overview.html#cb484-1"></a><span class="kw">get_best_result</span>(sim_knn_mod)<span class="op">$</span>RMSE</span></code></pre></div>
<pre><code>## [1] 3.6834</code></pre>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="supervised-overview.html#cb486-1"></a><span class="kw">calc_rmse</span>(<span class="dt">actual =</span> sim_tst<span class="op">$</span>y,</span>
<span id="cb486-2"><a href="supervised-overview.html#cb486-2"></a>          <span class="dt">predicted =</span> <span class="kw">predict</span>(sim_knn_mod, sim_tst))</span></code></pre></div>
<pre><code>## [1] 3.412332</code></pre>
<p>Here we see that the cross-validated RMSE is a bit of an overestimate, but still rather close to the test error. The real question is, are either of these any good? Is this model predicting well? No! Notice that we simulated this data with an error standard deviation of 1!</p>
<div id="methods" class="section level3" number="14.2.1">
<h3><span class="header-section-number">14.2.1</span> Methods</h3>
<p>Now that <code>caret</code> has given us a pipeline for a predictive analysis, we can very quickly and easily test new methods. For example, in an upcoming chapter we will discuss boosted tree models, but now that we understand how to use <code>caret</code>, in order to <em>use</em> a boosted tree model, we simply need to know the “method” to do so, which in this case is <code>gbm</code>. Beyond knowing that the method exists, we just need to know its tuning parameters, in this case, there are four. We actually could get away with knowing nothing about them, and simply specify a <code>tuneLength</code>, then <code>caret</code> would automatically try some reasonable values.</p>
<p>Instead, we could <a href="https://topepo.github.io/caret/train-models-by-tag.html">read the <code>caret</code> documentation</a> to find the tuning parameters, as well as the required packages. For now, we’ll simply use the following tuning grid. In later chapters, we’ll discuss how these effect the model.</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="supervised-overview.html#cb488-1"></a>gbm_grid =<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">interaction.depth =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>),</span>
<span id="cb488-2"><a href="supervised-overview.html#cb488-2"></a>                       <span class="dt">n.trees =</span> (<span class="dv">1</span><span class="op">:</span><span class="dv">30</span>) <span class="op">*</span><span class="st"> </span><span class="dv">100</span>,</span>
<span id="cb488-3"><a href="supervised-overview.html#cb488-3"></a>                       <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.3</span>),</span>
<span id="cb488-4"><a href="supervised-overview.html#cb488-4"></a>                       <span class="dt">n.minobsinnode =</span> <span class="dv">20</span>)</span>
<span id="cb488-5"><a href="supervised-overview.html#cb488-5"></a><span class="kw">head</span>(gbm_grid)</span></code></pre></div>
<pre><code>##   interaction.depth n.trees shrinkage n.minobsinnode
## 1                 1     100       0.1             20
## 2                 2     100       0.1             20
## 3                 3     100       0.1             20
## 4                 1     200       0.1             20
## 5                 2     200       0.1             20
## 6                 3     200       0.1             20</code></pre>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="supervised-overview.html#cb490-1"></a><span class="kw">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb490-2"><a href="supervised-overview.html#cb490-2"></a>sim_gbm_mod =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb490-3"><a href="supervised-overview.html#cb490-3"></a>  y <span class="op">~</span><span class="st"> </span>.,</span>
<span id="cb490-4"><a href="supervised-overview.html#cb490-4"></a>  <span class="dt">data =</span> sim_trn,</span>
<span id="cb490-5"><a href="supervised-overview.html#cb490-5"></a>  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>),</span>
<span id="cb490-6"><a href="supervised-overview.html#cb490-6"></a>  <span class="dt">method =</span> <span class="st">&quot;gbm&quot;</span>,</span>
<span id="cb490-7"><a href="supervised-overview.html#cb490-7"></a>  <span class="dt">tuneGrid =</span> gbm_grid, </span>
<span id="cb490-8"><a href="supervised-overview.html#cb490-8"></a>  <span class="dt">verbose =</span> <span class="ot">FALSE</span></span>
<span id="cb490-9"><a href="supervised-overview.html#cb490-9"></a>)</span></code></pre></div>
<p>We added <code>verbose = FALSE</code> to the <code>train()</code> call to suppress some of the intermediate output of the <code>gbm</code> fitting procedure.</p>
<p>How this training is happening is a bit of a mystery to us right now. What is this method? How does it deal with the factor variable as a predictor? We’ll answer these questions later, for now, we do know how to evaluate how well the method is working.</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="supervised-overview.html#cb491-1"></a>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">head</span>(sim_gbm_mod<span class="op">$</span>results), <span class="dt">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<table style="width:100%;">
<colgroup>
<col width="4%" />
<col width="10%" />
<col width="18%" />
<col width="15%" />
<col width="8%" />
<col width="6%" />
<col width="9%" />
<col width="6%" />
<col width="7%" />
<col width="11%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">shrinkage</th>
<th align="right">interaction.depth</th>
<th align="right">n.minobsinnode</th>
<th align="right">n.trees</th>
<th align="right">RMSE</th>
<th align="right">Rsquared</th>
<th align="right">MAE</th>
<th align="right">RMSESD</th>
<th align="right">RsquaredSD</th>
<th align="right">MAESD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="right">0.1</td>
<td align="right">1</td>
<td align="right">20</td>
<td align="right">100</td>
<td align="right">2.151</td>
<td align="right">0.995</td>
<td align="right">1.569</td>
<td align="right">0.321</td>
<td align="right">0.001</td>
<td align="right">0.185</td>
</tr>
<tr class="even">
<td align="left">91</td>
<td align="right">0.3</td>
<td align="right">1</td>
<td align="right">20</td>
<td align="right">100</td>
<td align="right">2.800</td>
<td align="right">0.991</td>
<td align="right">2.068</td>
<td align="right">0.180</td>
<td align="right">0.001</td>
<td align="right">0.183</td>
</tr>
<tr class="odd">
<td align="left">31</td>
<td align="right">0.1</td>
<td align="right">2</td>
<td align="right">20</td>
<td align="right">100</td>
<td align="right">1.996</td>
<td align="right">0.996</td>
<td align="right">1.439</td>
<td align="right">0.286</td>
<td align="right">0.001</td>
<td align="right">0.165</td>
</tr>
<tr class="even">
<td align="left">121</td>
<td align="right">0.3</td>
<td align="right">2</td>
<td align="right">20</td>
<td align="right">100</td>
<td align="right">2.377</td>
<td align="right">0.994</td>
<td align="right">1.784</td>
<td align="right">0.285</td>
<td align="right">0.001</td>
<td align="right">0.191</td>
</tr>
<tr class="odd">
<td align="left">61</td>
<td align="right">0.1</td>
<td align="right">3</td>
<td align="right">20</td>
<td align="right">100</td>
<td align="right">1.967</td>
<td align="right">0.996</td>
<td align="right">1.410</td>
<td align="right">0.325</td>
<td align="right">0.001</td>
<td align="right">0.148</td>
</tr>
<tr class="even">
<td align="left">151</td>
<td align="right">0.3</td>
<td align="right">3</td>
<td align="right">20</td>
<td align="right">100</td>
<td align="right">2.166</td>
<td align="right">0.995</td>
<td align="right">1.620</td>
<td align="right">0.150</td>
<td align="right">0.001</td>
<td align="right">0.135</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="supervised-overview.html#cb492-1"></a><span class="kw">plot</span>(sim_gbm_mod)</span></code></pre></div>
<p><img src="supervised-overview_files/figure-html/unnamed-chunk-43-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="supervised-overview.html#cb493-1"></a>sim_gbm_mod<span class="op">$</span>bestTune</span></code></pre></div>
<pre><code>##    n.trees interaction.depth shrinkage n.minobsinnode
## 35     500                 2       0.1             20</code></pre>
<p>Here we obtain the set of tuning parameters that performed best. Based on the above plot, do you think we considered enough possible tuning parameters?</p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="supervised-overview.html#cb495-1"></a><span class="kw">get_best_result</span>(sim_gbm_mod)</span></code></pre></div>
<pre><code>##   shrinkage interaction.depth n.minobsinnode n.trees     RMSE  Rsquared
## 1       0.1                 2             20     500 1.854777 0.9962175
##        MAE    RMSESD   RsquaredSD     MAESD
## 1 1.346841 0.2556658 0.0007945475 0.1555363</code></pre>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="supervised-overview.html#cb497-1"></a><span class="kw">calc_rmse</span>(<span class="dt">actual =</span> sim_tst<span class="op">$</span>y,</span>
<span id="cb497-2"><a href="supervised-overview.html#cb497-2"></a>          <span class="dt">predicted =</span> <span class="kw">predict</span>(sim_gbm_mod, sim_tst))</span></code></pre></div>
<pre><code>## [1] 1.568517</code></pre>
<p>Again, the cross-validated result is overestimating the error a bit. Also, this model is a big improvement over the <code>knn</code> model, but we can still do better.</p>
<div class="sourceCode" id="cb499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb499-1"><a href="supervised-overview.html#cb499-1"></a>sim_lm_mod =<span class="st"> </span><span class="kw">train</span>(</span>
<span id="cb499-2"><a href="supervised-overview.html#cb499-2"></a>  y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x1, <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">poly</span>(x2, <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>x3,</span>
<span id="cb499-3"><a href="supervised-overview.html#cb499-3"></a>  <span class="dt">data =</span> sim_trn,</span>
<span id="cb499-4"><a href="supervised-overview.html#cb499-4"></a>  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb499-5"><a href="supervised-overview.html#cb499-5"></a>  <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>)</span>
<span id="cb499-6"><a href="supervised-overview.html#cb499-6"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="supervised-overview.html#cb500-1"></a>sim_lm_mod<span class="op">$</span>finalModel</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Coefficients:
##    (Intercept)  `poly(x1, 2)1`  `poly(x1, 2)2`  `poly(x2, 2)1`  `poly(x2, 2)2`  
##       34.75615       645.50804       167.12875        26.00951         6.86587  
##            x3B             x3C  
##        1.80700         0.07108</code></pre>
<p>Here we fit a good old linear model, except, we specify a very specific formula.</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="supervised-overview.html#cb502-1"></a>sim_lm_mod<span class="op">$</span>results<span class="op">$</span>RMSE</span></code></pre></div>
<pre><code>## [1] 1.046702</code></pre>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="supervised-overview.html#cb504-1"></a><span class="kw">calc_rmse</span>(<span class="dt">actual =</span> sim_tst<span class="op">$</span>y,</span>
<span id="cb504-2"><a href="supervised-overview.html#cb504-2"></a>          <span class="dt">predicted =</span> <span class="kw">predict</span>(sim_lm_mod, sim_tst))</span></code></pre></div>
<pre><code>## [1] 1.035896</code></pre>
<p>This model dominates the previous two. The <code>gbm</code> model does still have a big advantage. The <code>lm</code> model needed the correct form of the model, whereas <code>gbm</code> nearly learned it automatically!</p>
<p>This question of <em>which</em> variables should be included is where we will turn our focus next. We’ll consider both what variables are useful for prediction, and learn tools to asses how useful they are.</p>
</div>
</div>
<div id="external-links" class="section level2" number="14.3">
<h2><span class="header-section-number">14.3</span> External Links</h2>
<ul>
<li><a href="http://topepo.github.io/caret/index.html">The <code>caret</code> Package</a> - Reference documentation for the <code>caret</code> package in <code>bookdown</code> format.</li>
<li><a href="http://topepo.github.io/caret/available-models.html"><code>caret</code> Model List</a> - List of available models in <code>caret</code>.</li>
<li><a href="https://topepo.github.io/caret/train-models-by-tag.html"><code>caret</code> Model List, By Tag</a> - Gives information on tuning parameters and necessary packages.</li>
<li><a href="http://www.springer.com/us/book/9781461468486">Applied Predictive Modeling</a> - Book from the author of the <code>caret</code> package, Max Kuhn, as well as Kjell Johnson. Further discussion on use of statistical learning methods in practice.</li>
</ul>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="ensemble-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="additional-reading.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/daviddalpiaz/bsl/edit/master/supervised-overview.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
