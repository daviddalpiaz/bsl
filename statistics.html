<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>D Statistics | Basics of Statistical Learning</title>
  <meta name="description" content="D Statistics | Basics of Statistical Learning" />
  <meta name="generator" content="bookdown 0.20.3 and GitBook 2.6.7" />

  <meta property="og:title" content="D Statistics | Basics of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://statisticallearning.org/" />
  
  
  <meta name="github-repo" content="daviddalpiaz/bsl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="D Statistics | Basics of Statistical Learning" />
  
  
  

<meta name="author" content="David Dalpiaz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Basics of Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i><b>0.1</b> Who?</a><ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#readers"><i class="fa fa-check"></i><b>0.1.1</b> Readers</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#author"><i class="fa fa-check"></i><b>0.1.2</b> Author</a></li>
<li class="chapter" data-level="0.1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>0.1.3</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#what"><i class="fa fa-check"></i><b>0.2</b> What?</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#why"><i class="fa fa-check"></i><b>0.3</b> Why?</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#where"><i class="fa fa-check"></i><b>0.4</b> Where?</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#when"><i class="fa fa-check"></i><b>0.5</b> When?</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#how"><i class="fa fa-check"></i><b>0.6</b> How?</a><ul>
<li class="chapter" data-level="0.6.1" data-path="index.html"><a href="index.html#build-tools"><i class="fa fa-check"></i><b>0.6.1</b> Build Tools</a></li>
<li class="chapter" data-level="0.6.2" data-path="index.html"><a href="index.html#active-development"><i class="fa fa-check"></i><b>0.6.2</b> Active Development</a></li>
<li class="chapter" data-level="0.6.3" data-path="index.html"><a href="index.html#packages"><i class="fa fa-check"></i><b>0.6.3</b> Packages</a></li>
<li class="chapter" data-level="0.6.4" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>0.6.4</b> License</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ml-overview.html"><a href="ml-overview.html"><i class="fa fa-check"></i><b>1</b> Machine Learning Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="ml-overview.html"><a href="ml-overview.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-tasks"><i class="fa fa-check"></i><b>1.2</b> Machine Learning Tasks</a><ul>
<li class="chapter" data-level="1.2.1" data-path="ml-overview.html"><a href="ml-overview.html#supervised-learning"><i class="fa fa-check"></i><b>1.2.1</b> Supervised Learning</a></li>
<li class="chapter" data-level="1.2.2" data-path="ml-overview.html"><a href="ml-overview.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.2.2</b> Unsupervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ml-overview.html"><a href="ml-overview.html#open-questions"><i class="fa fa-check"></i><b>1.3</b> Open Questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#r-setup-and-source"><i class="fa fa-check"></i><b>2.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#explanation-versus-prediction"><i class="fa fa-check"></i><b>2.2</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#task-setup"><i class="fa fa-check"></i><b>2.3</b> Task Setup</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#mathematical-setup"><i class="fa fa-check"></i><b>2.4</b> Mathematical Setup</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-models"><i class="fa fa-check"></i><b>2.5</b> Linear Regression Models</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#using-lm"><i class="fa fa-check"></i><b>2.6</b> Using <code>lm()</code></a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression.html"><a href="linear-regression.html#the-predict-function"><i class="fa fa-check"></i><b>2.7</b> The <code>predict()</code> Function</a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression.html"><a href="linear-regression.html#data-splitting"><i class="fa fa-check"></i><b>2.8</b> Data Splitting</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression.html"><a href="linear-regression.html#regression-metrics"><i class="fa fa-check"></i><b>2.9</b> Regression Metrics</a><ul>
<li class="chapter" data-level="2.9.1" data-path="linear-regression.html"><a href="linear-regression.html#graphical-evaluation"><i class="fa fa-check"></i><b>2.9.1</b> Graphical Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="linear-regression.html"><a href="linear-regression.html#example-simple-simulated-data"><i class="fa fa-check"></i><b>2.10</b> Example: “Simple” Simulated Data</a></li>
<li class="chapter" data-level="2.11" data-path="linear-regression.html"><a href="linear-regression.html#example-diamonds-data"><i class="fa fa-check"></i><b>2.11</b> Example: Diamonds Data</a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression.html"><a href="linear-regression.html#example-credit-card-data"><i class="fa fa-check"></i><b>2.12</b> Example: Credit Card Data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html"><i class="fa fa-check"></i><b>3</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#r-setup-and-source-1"><i class="fa fa-check"></i><b>3.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="3.2" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#mathematical-setup-1"><i class="fa fa-check"></i><b>3.2</b> Mathematical Setup</a></li>
<li class="chapter" data-level="3.3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="3.4" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#decision-trees"><i class="fa fa-check"></i><b>3.4</b> Decision Trees</a></li>
<li class="chapter" data-level="3.5" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#example-credit-card-data-1"><i class="fa fa-check"></i><b>3.5</b> Example: Credit Card Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>4</b> The Bias–Variance Tradeoff</a><ul>
<li class="chapter" data-level="4.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#r-setup-and-source-2"><i class="fa fa-check"></i><b>4.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="4.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#the-regression-setup"><i class="fa fa-check"></i><b>4.2</b> The Regression Setup</a></li>
<li class="chapter" data-level="4.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#reducible-and-irreducible-error"><i class="fa fa-check"></i><b>4.3</b> Reducible and Irreducible Error</a></li>
<li class="chapter" data-level="4.4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#bias-variance-decomposition"><i class="fa fa-check"></i><b>4.4</b> Bias-Variance Decomposition</a></li>
<li class="chapter" data-level="4.5" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#using-simulation-to-estimate-bias-and-variance"><i class="fa fa-check"></i><b>4.5</b> Using Simulation to Estimate Bias and Variance</a></li>
<li class="chapter" data-level="4.6" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#estimating-expected-prediction-error"><i class="fa fa-check"></i><b>4.6</b> Estimating Expected Prediction Error</a></li>
<li class="chapter" data-level="4.7" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#model-flexibility"><i class="fa fa-check"></i><b>4.7</b> Model Flexibility</a><ul>
<li class="chapter" data-level="4.7.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#linear-models"><i class="fa fa-check"></i><b>4.7.1</b> Linear Models</a></li>
<li class="chapter" data-level="4.7.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#k-nearest-neighbors-1"><i class="fa fa-check"></i><b>4.7.2</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.7.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#decision-trees-1"><i class="fa fa-check"></i><b>4.7.3</b> Decision Trees</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="additional-reading.html"><a href="additional-reading.html"><i class="fa fa-check"></i><b>A</b> Additional Reading</a><ul>
<li class="chapter" data-level="A.1" data-path="additional-reading.html"><a href="additional-reading.html#books"><i class="fa fa-check"></i><b>A.1</b> Books</a></li>
<li class="chapter" data-level="A.2" data-path="additional-reading.html"><a href="additional-reading.html#papers"><i class="fa fa-check"></i><b>A.2</b> Papers</a></li>
<li class="chapter" data-level="A.3" data-path="additional-reading.html"><a href="additional-reading.html#blog-posts"><i class="fa fa-check"></i><b>A.3</b> Blog Posts</a></li>
<li class="chapter" data-level="A.4" data-path="additional-reading.html"><a href="additional-reading.html#miscellaneous"><i class="fa fa-check"></i><b>A.4</b> Miscellaneous</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="computing.html"><a href="computing.html"><i class="fa fa-check"></i><b>B</b> Computing</a><ul>
<li class="chapter" data-level="B.1" data-path="computing.html"><a href="computing.html#reading"><i class="fa fa-check"></i><b>B.1</b> Reading</a></li>
<li class="chapter" data-level="B.2" data-path="computing.html"><a href="computing.html#additional-resources"><i class="fa fa-check"></i><b>B.2</b> Additional Resources</a><ul>
<li class="chapter" data-level="B.2.1" data-path="computing.html"><a href="computing.html#r"><i class="fa fa-check"></i><b>B.2.1</b> R</a></li>
<li class="chapter" data-level="B.2.2" data-path="computing.html"><a href="computing.html#rstudio"><i class="fa fa-check"></i><b>B.2.2</b> RStudio</a></li>
<li class="chapter" data-level="B.2.3" data-path="computing.html"><a href="computing.html#r-markdown"><i class="fa fa-check"></i><b>B.2.3</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="computing.html"><a href="computing.html#stat-432-idioms"><i class="fa fa-check"></i><b>B.3</b> STAT 432 Idioms</a><ul>
<li class="chapter" data-level="B.3.1" data-path="computing.html"><a href="computing.html#dont-restore-old-workspaces"><i class="fa fa-check"></i><b>B.3.1</b> Don’t Restore Old Workspaces</a></li>
<li class="chapter" data-level="B.3.2" data-path="computing.html"><a href="computing.html#r-versions"><i class="fa fa-check"></i><b>B.3.2</b> R Versions</a></li>
<li class="chapter" data-level="B.3.3" data-path="computing.html"><a href="computing.html#packages-1"><i class="fa fa-check"></i><b>B.3.3</b> Packages</a></li>
<li class="chapter" data-level="B.3.4" data-path="computing.html"><a href="computing.html#code-style"><i class="fa fa-check"></i><b>B.3.4</b> Code Style</a></li>
<li class="chapter" data-level="B.3.5" data-path="computing.html"><a href="computing.html#reference-style"><i class="fa fa-check"></i><b>B.3.5</b> Reference Style</a></li>
<li class="chapter" data-level="B.3.6" data-path="computing.html"><a href="computing.html#stat-432-r-style-overrides"><i class="fa fa-check"></i><b>B.3.6</b> STAT 432 R Style Overrides</a></li>
<li class="chapter" data-level="B.3.7" data-path="computing.html"><a href="computing.html#stat-432-r-markdown-style"><i class="fa fa-check"></i><b>B.3.7</b> STAT 432 R Markdown Style</a></li>
<li class="chapter" data-level="B.3.8" data-path="computing.html"><a href="computing.html#style-heuristics"><i class="fa fa-check"></i><b>B.3.8</b> Style Heuristics</a></li>
<li class="chapter" data-level="B.3.9" data-path="computing.html"><a href="computing.html#objects-and-functions"><i class="fa fa-check"></i><b>B.3.9</b> Objects and Functions</a></li>
<li class="chapter" data-level="B.3.10" data-path="computing.html"><a href="computing.html#print-versus-return"><i class="fa fa-check"></i><b>B.3.10</b> Print versus Return</a></li>
<li class="chapter" data-level="B.3.11" data-path="computing.html"><a href="computing.html#help"><i class="fa fa-check"></i><b>B.3.11</b> Help</a></li>
<li class="chapter" data-level="B.3.12" data-path="computing.html"><a href="computing.html#keyboard-shortcuts"><i class="fa fa-check"></i><b>B.3.12</b> Keyboard Shortcuts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>C</b> Probability</a><ul>
<li class="chapter" data-level="C.1" data-path="probability.html"><a href="probability.html#reading-1"><i class="fa fa-check"></i><b>C.1</b> Reading</a></li>
<li class="chapter" data-level="C.2" data-path="probability.html"><a href="probability.html#probability-models"><i class="fa fa-check"></i><b>C.2</b> Probability Models</a></li>
<li class="chapter" data-level="C.3" data-path="probability.html"><a href="probability.html#probability-axioms"><i class="fa fa-check"></i><b>C.3</b> Probability Axioms</a></li>
<li class="chapter" data-level="C.4" data-path="probability.html"><a href="probability.html#probability-rules"><i class="fa fa-check"></i><b>C.4</b> Probability Rules</a></li>
<li class="chapter" data-level="C.5" data-path="probability.html"><a href="probability.html#random-variables"><i class="fa fa-check"></i><b>C.5</b> Random Variables</a><ul>
<li class="chapter" data-level="C.5.1" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>C.5.1</b> Distributions</a></li>
<li class="chapter" data-level="C.5.2" data-path="probability.html"><a href="probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>C.5.2</b> Discrete Random Variables</a></li>
<li class="chapter" data-level="C.5.3" data-path="probability.html"><a href="probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>C.5.3</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="C.5.4" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>C.5.4</b> Distributions in R</a></li>
<li class="chapter" data-level="C.5.5" data-path="probability.html"><a href="probability.html#several-random-variables"><i class="fa fa-check"></i><b>C.5.5</b> Several Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="probability.html"><a href="probability.html#expectations"><i class="fa fa-check"></i><b>C.6</b> Expectations</a></li>
<li class="chapter" data-level="C.7" data-path="probability.html"><a href="probability.html#likelihood"><i class="fa fa-check"></i><b>C.7</b> Likelihood</a></li>
<li class="chapter" data-level="C.8" data-path="probability.html"><a href="probability.html#additional-references"><i class="fa fa-check"></i><b>C.8</b> Additional References</a><ul>
<li class="chapter" data-level="C.8.1" data-path="probability.html"><a href="probability.html#videos"><i class="fa fa-check"></i><b>C.8.1</b> Videos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>D</b> Statistics</a><ul>
<li class="chapter" data-level="D.1" data-path="statistics.html"><a href="statistics.html#reading-2"><i class="fa fa-check"></i><b>D.1</b> Reading</a></li>
<li class="chapter" data-level="D.2" data-path="statistics.html"><a href="statistics.html#statistics-1"><i class="fa fa-check"></i><b>D.2</b> Statistics</a></li>
<li class="chapter" data-level="D.3" data-path="statistics.html"><a href="statistics.html#estimators"><i class="fa fa-check"></i><b>D.3</b> Estimators</a><ul>
<li class="chapter" data-level="D.3.1" data-path="statistics.html"><a href="statistics.html#properties"><i class="fa fa-check"></i><b>D.3.1</b> Properties</a></li>
<li class="chapter" data-level="D.3.2" data-path="statistics.html"><a href="statistics.html#example-mse-of-an-estimator"><i class="fa fa-check"></i><b>D.3.2</b> Example: MSE of an Estimator</a></li>
<li class="chapter" data-level="D.3.3" data-path="statistics.html"><a href="statistics.html#estimation-methods"><i class="fa fa-check"></i><b>D.3.3</b> Estimation Methods</a></li>
<li class="chapter" data-level="D.3.4" data-path="statistics.html"><a href="statistics.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>D.3.4</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="D.3.5" data-path="statistics.html"><a href="statistics.html#method-of-moments"><i class="fa fa-check"></i><b>D.3.5</b> Method of Moments</a></li>
<li class="chapter" data-level="D.3.6" data-path="statistics.html"><a href="statistics.html#empirical-distribution-function"><i class="fa fa-check"></i><b>D.3.6</b> Empirical Distribution Function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://daviddalpiaz.org" target="blank">&copy; 2020 David Dalpiaz</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basics of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistics" class="section level1">
<h1><span class="header-section-number">D</span> Statistics</h1>
<p>STAT 432 is a course about statistics, in particular, some specific statistics. To discuss the statistics of interest in STAT 432, we will need some general concepts about statistics.</p>
<!-- - TODO: Where we are going, estimating conditional means and distributions. -->
<!-- - TODO: estimation = learning. "learning from data." what are we learning about? often parameters. -->
<div id="reading-2" class="section level2">
<h2><span class="header-section-number">D.1</span> Reading</h2>
<ul>
<li><strong>Reference:</strong> <a href="http://stat400.org">STAT 400 @ UIUC: Notes and Homework</a></li>
<li><strong>Reference:</strong> <a href="https://daviddalpiaz.github.io/stat3202-sp19/notes/fitting.html">STAT 3202 @ OSU: Fitting a Probability Model</a></li>
<li><strong>Reference:</strong> <a href="https://online.stat.psu.edu/stat414/node/213/">STAT 415 @ PSU: Notes</a></li>
</ul>
</div>
<div id="statistics-1" class="section level2">
<h2><span class="header-section-number">D.2</span> Statistics</h2>
<p>In short: a <strong>statistic</strong> is a <em>function of (sample) data</em>. (This mirrors <strong>parameters</strong> being <em>functions of (population) distributions</em>. In SAT terminology, statistics : data :: parameter : distribution.)</p>
<p>Consider a random variable <span class="math inline">\(X\)</span>, with PDF <span class="math inline">\(f(x)\)</span> which defines the distribution of <span class="math inline">\(X\)</span>. Now consider the parameter <span class="math inline">\(\mu\)</span>, which we usually refer to as the mean of a distribution. We use <span class="math inline">\(\mu_X\)</span> to note that <span class="math inline">\(\mu\)</span> is dependent on the distribution of <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[
\mu_X = \text{E}[X] = \int_{-\infty}^{\infty}xf(x)dx
\]</span></p>
<p>Note that this expression is a <em>function</em> of <span class="math inline">\(f(x)\)</span>. When we change the distribution of <span class="math inline">\(X\)</span>, that is, it has a different <span class="math inline">\(f(x)\)</span>, that effects <span class="math inline">\(\mu\)</span>.</p>
<p>Now, given a random sample <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>, define a statistic,</p>
<p><span class="math display">\[
\hat{\mu}(x_1, x_2, \ldots, x_n) = \frac{1}{n}\sum_{i = 1}^{n}x_i
\]</span></p>
<p>Often, we will simplify notation and instead simply write</p>
<p><span class="math display">\[
\hat{\mu} = \frac{1}{n}\sum_{i = 1}^{n}x_i
\]</span></p>
<p>and the fact that <span class="math inline">\(\hat{\mu}\)</span> is a function of the sample is implied. (You might also notice that this is the sample mean, which is often denoted by <span class="math inline">\(\bar{x}\)</span>.)</p>
<p>Another confusing aspect of statistics is that they are <em>random variables</em>! Sometimes we would write the above as</p>
<p><span class="math display">\[
\hat{\mu}(X_1, X_2, \ldots, X_n) = \frac{1}{n}\sum_{i = 1}^{n}X_i
\]</span></p>
<p>When written this way, we are emphasizing that the random sample has not yet be observed, thus is still random. When this is the case, we can investigate the properties of the statistic as a random variable. When the sample has been observed, we use <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> to note that we are inputting these observed values into a function, which outputs some value. (Sometimes we, and others, will be notationally sloppy and simply use lower case <span class="math inline">\(x\)</span> and you will be expected to understand via context if we are dealing with random variables or observed values of random variables. This is admittedly confusing.)</p>
<p>As a final note, suppose we observe some data</p>
<p><span class="math display">\[
x_1 = 2, x_2 = 1, x_3 =5
\]</span></p>
<p>and we calculate <span class="math inline">\(\hat{\mu}\)</span> given these values. We would obtain</p>
<p><span class="math display">\[
\hat{\mu} = \frac{8}{3} \approx 2.66
\]</span></p>
<p>Note that 2.66 is <strong>not a statistic</strong>. It is the <strong>value</strong> of a statistic given a particular set of data. The statistic is still <span class="math inline">\(\hat{\mu}\)</span> which has output the value 2.66. Statistics output values given some data.</p>
</div>
<div id="estimators" class="section level2">
<h2><span class="header-section-number">D.3</span> Estimators</h2>
<p>Estimators are just statistics with a purpose, that is, estimators are statistics that attempt to estimate some quantity of interest, usually some parameter. (In other words, <em>learn</em> from data.) Like statistics, estimators are functions of data that output values, which we call estimates.</p>
<div id="properties" class="section level3">
<h3><span class="header-section-number">D.3.1</span> Properties</h3>
<div class="figure">
<img src="img/bias-var.png" alt="Bias and Variance Visually Illustrated" />
<p class="caption">Bias and Variance Visually Illustrated</p>
</div>
<!-- - TODO: explain image above, source copyright or recreate. -->
<p>Because they are just statistics, estimators are simply functions of data. What makes an estimator <em>good</em>? Essentially, an estimator is good if it produces estimates that are close to the thing being estimated. The following properties help to better define this “closeness” as a function of the errors made by estimators.</p>
<p>To estimate some parameter <span class="math inline">\(\theta\)</span> we will consider some estimator <span class="math inline">\(\hat{\theta}\)</span>.</p>
<div id="bias" class="section level4">
<h4><span class="header-section-number">D.3.1.1</span> Bias</h4>
<p>The <strong>bias</strong> of an estimator defines the systematic error of the estimator, that is, how the estimator “misses” on average.</p>
<p><span class="math display">\[
\text{bias}\left[\hat{\theta}\right] \triangleq
\mathbb{E}\left[\hat{\theta}\right] - \theta
\]</span></p>
</div>
<div id="variance" class="section level4">
<h4><span class="header-section-number">D.3.1.2</span> Variance</h4>
<p>The <strong>variance</strong> of an estimator defines how close resulting estimates are to each other. (Assuming the estimated was repeated.)</p>
<p><span class="math display">\[
\text{var}\left[\hat{\theta}\right] \triangleq
\mathbb{E}\left[ \left( \hat{\theta} - \mathbb{E}\left[\hat{\theta}\right] \right)^2 \right]
\]</span></p>
</div>
<div id="mean-squared-error" class="section level4">
<h4><span class="header-section-number">D.3.1.3</span> Mean Squared Error</h4>
<p>The <strong>mean squared error</strong> (MSE) is exactly what the name suggests, it is the average squared error of the estimator. Interestingly, the MSE decomposes into terms related to the bias and the variance. We will return to this idea later for a detailed discussion in the context of machine learning.</p>
<p><span class="math display">\[
\text{MSE}\left[\hat{\theta}\right] \triangleq
\mathbb{E}\left[\left(\hat{\theta} - \theta\right)^2\right] =
\left(\text{bias}\left[\hat{\theta}\right]\right)^2 + 
\text{var}\left[\hat{\theta}\right]
\]</span></p>
</div>
<div id="consistency" class="section level4">
<h4><span class="header-section-number">D.3.1.4</span> Consistency</h4>
<p>An estimator <span class="math inline">\(\hat{\theta}_n\)</span> is said to be a <strong>consistent estimator</strong> of <span class="math inline">\(\theta\)</span> if, for any positive <span class="math inline">\(\epsilon\)</span>,</p>
<p><span class="math display">\[
\lim_{n \rightarrow \infty} P\left( \left| \hat{\theta}_n - \theta \right| \leq  \epsilon\right) =1
\]</span></p>
<p>or, equivalently,</p>
<p><span class="math display">\[
\lim_{n \rightarrow \infty} P\left( \left| \hat{\theta}_n - \theta \right| &gt;  \epsilon\right) =0
\]</span></p>
<p>We say that <span class="math inline">\(\hat{\theta}_n\)</span> <strong>converges in probability</strong> to <span class="math inline">\(\theta\)</span> and we write <span class="math inline">\(\hat{\theta}_n \overset P \rightarrow \theta\)</span>.</p>
</div>
</div>
<div id="example-mse-of-an-estimator" class="section level3">
<h3><span class="header-section-number">D.3.2</span> Example: MSE of an Estimator</h3>
<p>Consider <span class="math inline">\(X_1, X_2, X_3 \sim N(\mu, \sigma^2)\)</span>.</p>
<p>Define two estimators for the true mean, <span class="math inline">\(\mu\)</span>.</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n}\sum_{i = 1}^{3} X_i
\]</span></p>
<p><span class="math display">\[
\hat{\mu} = \frac{1}{4}X_1 + \frac{1}{5}X_2 + \frac{1}{6}X_3
\]</span></p>
<p>We will now calculate and compare the mean squared error of both <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\hat{\mu}\)</span> as estimators of <span class="math inline">\(\mu\)</span>.</p>
<p>First, recall from properties of the sample mean that</p>
<p><span class="math display">\[
\text{E}\left[\bar{X}\right] = \mu
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\text{var}\left[\bar{X}\right] = \frac{\sigma^2}{3}
\]</span></p>
<p>Thus we have</p>
<p><span class="math display">\[
\text{bias}\left[\bar{X}\right] = 
\mathbb{E}\left[\bar{X}\right] - \mu = 
\mu - \mu = 0
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
\text{MSE}\left[\bar{X}\right] \triangleq
\left(\text{bias}\left[\bar{X}\right]\right)^2 +
\text{var}\left[\bar{X}\right] =
0 + \frac{\sigma^2}{3} = 
\frac{\sigma^2}{3}
\]</span></p>
<p>Next,</p>
<p><span class="math display">\[
\text{E}\left[\hat{\mu}\right] = \frac{\mu}{4} + \frac{\mu}{5} + \frac{\mu}{6} = \frac{37}{60}\mu
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\text{var}\left[\hat{\mu}\right] = \frac{\sigma^2}{16} + \frac{\sigma^2}{25} + \frac{\sigma^2}{36} = \frac{469}{3600}\sigma^2
\]</span></p>
<p>Now we have</p>
<p><span class="math display">\[
\text{bias}\left[\hat{\mu}\right] = 
\mathbb{E}\left[\hat{\mu}\right] - \mu = 
\frac{37}{60}\mu - \mu = \frac{-23}{60}\mu
\]</span></p>
<p>Then finally we obtain the mean squared error for <span class="math inline">\(\hat{\mu}\)</span>,</p>
<p><span class="math display">\[
\text{MSE}\left[\hat{\mu}\right] \triangleq
\left(\text{bias}\left[\hat{\mu}\right]\right)^2 +
\text{var}\left[\hat{\mu}\right] =
\left( \frac{-23}{60}\mu \right)^2 +
\frac{469}{3600}\sigma^2
\]</span></p>
<p>Note that <span class="math inline">\(\text{MSE}\left[\hat{\mu}\right]\)</span> is small when <span class="math inline">\(\mu\)</span> is close to 0.</p>
</div>
<div id="estimation-methods" class="section level3">
<h3><span class="header-section-number">D.3.3</span> Estimation Methods</h3>
<p>So far we have discussed properties of estimators, but how do we <em>create</em> estimators? You <em>could</em> just define a bunch of estimators and then evaluate them to see what works best (an idea we will return to later in the context of ML) but (the field of) statistics has develop some methods that result in estimators with desirable properties.</p>
</div>
<div id="maximum-likelihood-estimation" class="section level3">
<h3><span class="header-section-number">D.3.4</span> Maximum Likelihood Estimation</h3>
<p>Given a random sample <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> from a population with parameter <span class="math inline">\(\theta\)</span> and density or mass <span class="math inline">\(f(x; \theta)\)</span>, we define the likelihood as</p>
<p><span class="math display">\[
\mathcal{L}(\theta \mid x_1, x_2, \ldots x_n) \triangleq f(x_1, x_2, \ldots, x_n; \theta) = \prod_{i = 1}^n f(x_i; \theta)
\]</span></p>
<p>The <strong>Maximum Likelihood Estimator</strong>, <span class="math inline">\(\hat{\theta}\)</span></p>
<p><span class="math display">\[
\hat{\theta} \triangleq \underset{\theta}{\text{argmax}} \ \mathcal{L}(\theta \mid x_1, x_2, \ldots x_n) = \underset{\theta}{\text{argmax}} \ \log \mathcal{L}(\theta \mid x_1, x_2, \ldots x_n)
\]</span></p>
<div id="invariance-principle" class="section level4">
<h4><span class="header-section-number">D.3.4.1</span> Invariance Principle</h4>
<p>If <span class="math inline">\(\hat{\theta}\)</span> is the MLE of <span class="math inline">\(\theta\)</span> and the function <span class="math inline">\(h(\theta)\)</span> is continuous, then <span class="math inline">\(h(\hat{\theta})\)</span> is the MLE of <span class="math inline">\(h(\theta)\)</span>.</p>
</div>
</div>
<div id="method-of-moments" class="section level3">
<h3><span class="header-section-number">D.3.5</span> Method of Moments</h3>
<p>While it is very unlikely that we will use the <a href="https://online.stat.psu.edu/stat414/node/193/">Method of Moments</a> in STAT 432, you should still be aware of its existence.</p>
</div>
<div id="empirical-distribution-function" class="section level3">
<h3><span class="header-section-number">D.3.6</span> Empirical Distribution Function</h3>
<p>Consider a random variable <span class="math inline">\(X\)</span> with CDF <span class="math inline">\(F(k) = P(X &lt; k)\)</span> and an iid random sample <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>. We can estimate <span class="math inline">\(F(k)\)</span> using the <a href="https://en.wikipedia.org/wiki/Empirical_distribution_function"><strong>Empirical Distribution Function</strong></a> (EDF),</p>
<p><span class="math display">\[
\hat{F}(k) = \frac{\text{# of elements in sample} \leq k}{n} = \frac{1}{n} \sum_{i = 1}^n I(x_i \leq k)
\]</span></p>
<p>where <span class="math inline">\(I(x_i \leq k)\)</span> is an indicator such that</p>
<p><span class="math display">\[
I(x_i \leq k) = 
\begin{cases} 
      1 &amp; \text{if } x_i \leq k \\
      0 &amp; \text{if } x_i &gt; k 
   \end{cases}
\]</span></p>
<!-- - TODO: Maybe consider this notation, but for now, gotta stop worrying about notation!!! -->
<!-- $$ -->
<!-- \mathbf{1}_{[x_i \le t]} -->
<!-- $$ -->
<!-- - TODO: Add properties? See All of Nonparametric Statistics. (Mean, Var, Convergence, Intervals)-->
<p>Given a data vector in R that is assumed to be a random sample, say, <code>y</code>, and some value, say <code>k</code>, it is easy to calculate <span class="math inline">\(\hat{F}(k)\)</span>.</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb226-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">66</span>)</a>
<a class="sourceLine" id="cb226-2" data-line-number="2">y =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">25</span>, <span class="dt">mean =</span> <span class="dv">6</span>, <span class="dt">sd =</span> <span class="fl">2.6</span>) <span class="co"># generate sample</span></a>
<a class="sourceLine" id="cb226-3" data-line-number="3">k =<span class="st"> </span><span class="dv">4</span> <span class="co"># pick some k</span></a>
<a class="sourceLine" id="cb226-4" data-line-number="4"><span class="kw">head</span>(y) <span class="co"># check data</span></a></code></pre></div>
<pre><code>## [1] 12.042334  6.564140  7.087301  5.503419  5.181420  4.315569</code></pre>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb228-1" data-line-number="1"><span class="co"># using the EDF</span></a>
<a class="sourceLine" id="cb228-2" data-line-number="2"><span class="kw">mean</span>(y <span class="op">&lt;</span><span class="st"> </span>k)</a></code></pre></div>
<pre><code>## [1] 0.2</code></pre>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb230-1" data-line-number="1"><span class="co"># using an estimated normal distribution (not quite using the MLE)</span></a>
<a class="sourceLine" id="cb230-2" data-line-number="2"><span class="kw">pnorm</span>(<span class="dt">q =</span> k, <span class="dt">mean =</span> <span class="kw">mean</span>(y), <span class="dt">sd =</span> <span class="kw">sd</span>(y))</a></code></pre></div>
<pre><code>## [1] 0.2088465</code></pre>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb232-1" data-line-number="1"><span class="co"># using the true (but assumed unknown) CDF</span></a>
<a class="sourceLine" id="cb232-2" data-line-number="2"><span class="kw">pnorm</span>(<span class="dt">q =</span> k, <span class="dt">mean =</span> <span class="dv">6</span>, <span class="dt">sd =</span> <span class="fl">2.6</span>) </a></code></pre></div>
<pre><code>## [1] 0.2208782</code></pre>
<p>Note that technically <code>sd(x)</code> does not return the MLE of <span class="math inline">\(\sigma\)</span> since it uses the unbiased estimator with a denominator of <span class="math inline">\(n - 1\)</span> instead of <span class="math inline">\(n\)</span>, but we’re being lazy for the sake of some cleaner code.</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb234-1" data-line-number="1"><span class="kw">plot</span>(<span class="kw">ecdf</span>(y), </a>
<a class="sourceLine" id="cb234-2" data-line-number="2">     <span class="dt">col.01line =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">verticals =</span> <span class="ot">TRUE</span>, <span class="dt">do.points =</span> <span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb234-3" data-line-number="3">     <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">15</span>), <span class="dt">lwd =</span> <span class="dv">2</span>, <span class="dt">lty =</span> <span class="dv">1</span>, <span class="dt">ylab =</span> <span class="st">&quot;F(y)&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;y&quot;</span>,</a>
<a class="sourceLine" id="cb234-4" data-line-number="4">     <span class="dt">main =</span> <span class="st">&quot;Comparing the EDF to The Truth and MLE&quot;</span>)</a>
<a class="sourceLine" id="cb234-5" data-line-number="5"><span class="kw">curve</span>(<span class="kw">pnorm</span>(x, <span class="dt">mean =</span> <span class="dv">6</span>, <span class="dt">sd =</span> <span class="fl">2.6</span>), </a>
<a class="sourceLine" id="cb234-6" data-line-number="6">      <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">15</span>), <span class="dt">col =</span> <span class="st">&quot;dodgerblue&quot;</span>, <span class="dt">lty =</span> <span class="dv">2</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb234-7" data-line-number="7"><span class="kw">curve</span>(<span class="kw">pnorm</span>(x, <span class="dt">mean =</span> <span class="kw">mean</span>(y), <span class="dt">sd =</span> <span class="kw">sd</span>(y)), </a>
<a class="sourceLine" id="cb234-8" data-line-number="8">      <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">15</span>), <span class="dt">col =</span> <span class="st">&quot;darkorange&quot;</span>, <span class="dt">lty =</span> <span class="dv">3</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb234-9" data-line-number="9"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;EDF&quot;</span>, <span class="st">&quot;Truth&quot;</span>, <span class="st">&quot;MLE&quot;</span>),</a>
<a class="sourceLine" id="cb234-10" data-line-number="10">  <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;dodgerblue&quot;</span>, <span class="st">&quot;darkorange&quot;</span>), <span class="dt">lty =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb234-11" data-line-number="11"><span class="kw">grid</span>()</a></code></pre></div>
<p><img src="statistics_files/figure-html/unnamed-chunk-3-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>We have purposefully used a “small” sample size here so that the EDF is visibly a step function. Modify the code above to increase the sample size. You should notice that the three functions converge as the sample size increases.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/daviddalpiaz/bsl/edit/master/statistics.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
