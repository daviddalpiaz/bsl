<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Linear Regression | Basics of Statistical Learning</title>
  <meta name="description" content="Chapter 2 Linear Regression | Basics of Statistical Learning" />
  <meta name="generator" content="bookdown 0.19.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Linear Regression | Basics of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://statisticallearning.org/" />
  
  
  <meta name="github-repo" content="daviddalpiaz/bsl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Linear Regression | Basics of Statistical Learning" />
  
  
  

<meta name="author" content="David Dalpiaz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ml-overview.html"/>
<link rel="next" href="nonparametric-regression.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">

<!-- The loading of KaTeX is deferred to speed up page rendering -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>

<!-- To automatically render math in text elements, include the auto-render extension: -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Basics of Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i><b>0.1</b> Who?</a><ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#readers"><i class="fa fa-check"></i><b>0.1.1</b> Readers</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#author"><i class="fa fa-check"></i><b>0.1.2</b> Author</a></li>
<li class="chapter" data-level="0.1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>0.1.3</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#what"><i class="fa fa-check"></i><b>0.2</b> What?</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#why"><i class="fa fa-check"></i><b>0.3</b> Why?</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#where"><i class="fa fa-check"></i><b>0.4</b> Where?</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#when"><i class="fa fa-check"></i><b>0.5</b> When?</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#how"><i class="fa fa-check"></i><b>0.6</b> How?</a><ul>
<li class="chapter" data-level="0.6.1" data-path="index.html"><a href="index.html#build-tools"><i class="fa fa-check"></i><b>0.6.1</b> Build Tools</a></li>
<li class="chapter" data-level="0.6.2" data-path="index.html"><a href="index.html#active-development"><i class="fa fa-check"></i><b>0.6.2</b> Active Development</a></li>
<li class="chapter" data-level="0.6.3" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>0.6.3</b> License</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ml-overview.html"><a href="ml-overview.html"><i class="fa fa-check"></i><b>1</b> Machine Learning Overview</a><ul>
<li class="chapter" data-level="1.1" data-path="ml-overview.html"><a href="ml-overview.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-tasks"><i class="fa fa-check"></i><b>1.2</b> Machine Learning Tasks</a><ul>
<li class="chapter" data-level="1.2.1" data-path="ml-overview.html"><a href="ml-overview.html#supervised-learning"><i class="fa fa-check"></i><b>1.2.1</b> Supervised Learning</a></li>
<li class="chapter" data-level="1.2.2" data-path="ml-overview.html"><a href="ml-overview.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.2.2</b> Unsupervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ml-overview.html"><a href="ml-overview.html#open-questions"><i class="fa fa-check"></i><b>1.3</b> Open Questions</a></li>
<li class="chapter" data-level="1.4" data-path="ml-overview.html"><a href="ml-overview.html#source"><i class="fa fa-check"></i><b>1.4</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#reading"><i class="fa fa-check"></i><b>2.1</b> Reading</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#explanation-versus-prediction"><i class="fa fa-check"></i><b>2.2</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#setup"><i class="fa fa-check"></i><b>2.3</b> Setup</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#mathematical-setup"><i class="fa fa-check"></i><b>2.4</b> Mathematical Setup</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-models"><i class="fa fa-check"></i><b>2.5</b> Linear Regression Models</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#using-lm"><i class="fa fa-check"></i><b>2.6</b> Using <code>lm()</code></a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression.html"><a href="linear-regression.html#the-predict-function"><i class="fa fa-check"></i><b>2.7</b> The <code>predict()</code> Function</a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression.html"><a href="linear-regression.html#data-splitting"><i class="fa fa-check"></i><b>2.8</b> Data Splitting</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression.html"><a href="linear-regression.html#regression-metrics"><i class="fa fa-check"></i><b>2.9</b> Regression Metrics</a><ul>
<li class="chapter" data-level="2.9.1" data-path="linear-regression.html"><a href="linear-regression.html#graphical-evaluation"><i class="fa fa-check"></i><b>2.9.1</b> Graphical Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="linear-regression.html"><a href="linear-regression.html#example-simple-simulated-data"><i class="fa fa-check"></i><b>2.10</b> Example: “Simple” Simulated Data</a></li>
<li class="chapter" data-level="2.11" data-path="linear-regression.html"><a href="linear-regression.html#example-diamonds-data"><i class="fa fa-check"></i><b>2.11</b> Example: Diamonds Data</a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression.html"><a href="linear-regression.html#example-credit-card-data"><i class="fa fa-check"></i><b>2.12</b> Example: Credit Card Data</a></li>
<li class="chapter" data-level="2.13" data-path="linear-regression.html"><a href="linear-regression.html#source-1"><i class="fa fa-check"></i><b>2.13</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html"><i class="fa fa-check"></i><b>3</b> Nonparametric Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#reading-1"><i class="fa fa-check"></i><b>3.1</b> Reading</a></li>
<li class="chapter" data-level="3.2" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#mathematical-setup-1"><i class="fa fa-check"></i><b>3.2</b> Mathematical Setup</a></li>
<li class="chapter" data-level="3.3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="3.4" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#decision-trees"><i class="fa fa-check"></i><b>3.4</b> Decision Trees</a></li>
<li class="chapter" data-level="3.5" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#example-credit-card-data-1"><i class="fa fa-check"></i><b>3.5</b> Example: Credit Card Data</a></li>
<li class="chapter" data-level="3.6" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#source-2"><i class="fa fa-check"></i><b>3.6</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bvt.html"><a href="bvt.html"><i class="fa fa-check"></i><b>4</b> Bias–Variance Tradeoff</a><ul>
<li class="chapter" data-level="4.1" data-path="bvt.html"><a href="bvt.html#reducible-and-irreducible-error"><i class="fa fa-check"></i><b>4.1</b> Reducible and Irreducible Error</a></li>
<li class="chapter" data-level="4.2" data-path="bvt.html"><a href="bvt.html#bias-variance-decomposition"><i class="fa fa-check"></i><b>4.2</b> Bias-Variance Decomposition</a></li>
<li class="chapter" data-level="4.3" data-path="bvt.html"><a href="bvt.html#simulation"><i class="fa fa-check"></i><b>4.3</b> Simulation</a></li>
<li class="chapter" data-level="4.4" data-path="bvt.html"><a href="bvt.html#estimating-expected-prediction-error"><i class="fa fa-check"></i><b>4.4</b> Estimating Expected Prediction Error</a></li>
<li class="chapter" data-level="4.5" data-path="bvt.html"><a href="bvt.html#source-3"><i class="fa fa-check"></i><b>4.5</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-overview.html"><a href="regression-overview.html"><i class="fa fa-check"></i><b>5</b> Regression Overview</a><ul>
<li class="chapter" data-level="5.1" data-path="regression-overview.html"><a href="regression-overview.html#goal"><i class="fa fa-check"></i><b>5.1</b> Goal</a></li>
<li class="chapter" data-level="5.2" data-path="regression-overview.html"><a href="regression-overview.html#strategy"><i class="fa fa-check"></i><b>5.2</b> Strategy</a></li>
<li class="chapter" data-level="5.3" data-path="regression-overview.html"><a href="regression-overview.html#models"><i class="fa fa-check"></i><b>5.3</b> Models</a></li>
<li class="chapter" data-level="5.4" data-path="regression-overview.html"><a href="regression-overview.html#model-flexibility"><i class="fa fa-check"></i><b>5.4</b> Model Flexibility</a></li>
<li class="chapter" data-level="5.5" data-path="regression-overview.html"><a href="regression-overview.html#overfitting"><i class="fa fa-check"></i><b>5.5</b> Overfitting</a></li>
<li class="chapter" data-level="5.6" data-path="regression-overview.html"><a href="regression-overview.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.6</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="5.7" data-path="regression-overview.html"><a href="regression-overview.html#source-4"><i class="fa fa-check"></i><b>5.7</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>6</b> Classification</a><ul>
<li class="chapter" data-level="6.1" data-path="classification.html"><a href="classification.html#reading-2"><i class="fa fa-check"></i><b>6.1</b> Reading</a></li>
<li class="chapter" data-level="6.2" data-path="classification.html"><a href="classification.html#classification-metrics"><i class="fa fa-check"></i><b>6.2</b> Classification Metrics</a></li>
<li class="chapter" data-level="6.3" data-path="classification.html"><a href="classification.html#source-5"><i class="fa fa-check"></i><b>6.3</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html"><i class="fa fa-check"></i><b>7</b> Nonparametric Classification</a><ul>
<li class="chapter" data-level="7.1" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html#reading-3"><i class="fa fa-check"></i><b>7.1</b> Reading</a></li>
<li class="chapter" data-level="7.2" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html#source-6"><i class="fa fa-check"></i><b>7.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Logistic Regression</a><ul>
<li class="chapter" data-level="8.1" data-path="logistic-regression.html"><a href="logistic-regression.html#reading-4"><i class="fa fa-check"></i><b>8.1</b> Reading</a></li>
<li class="chapter" data-level="8.2" data-path="logistic-regression.html"><a href="logistic-regression.html#source-7"><i class="fa fa-check"></i><b>8.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="binary-classification.html"><a href="binary-classification.html"><i class="fa fa-check"></i><b>9</b> Binary Classification</a><ul>
<li class="chapter" data-level="9.1" data-path="binary-classification.html"><a href="binary-classification.html#reading-5"><i class="fa fa-check"></i><b>9.1</b> Reading</a></li>
<li class="chapter" data-level="9.2" data-path="binary-classification.html"><a href="binary-classification.html#source-8"><i class="fa fa-check"></i><b>9.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="generative.html"><a href="generative.html"><i class="fa fa-check"></i><b>10</b> Generative Models</a><ul>
<li class="chapter" data-level="10.1" data-path="generative.html"><a href="generative.html#reading-6"><i class="fa fa-check"></i><b>10.1</b> Reading</a></li>
<li class="chapter" data-level="10.2" data-path="generative.html"><a href="generative.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>10.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="10.3" data-path="generative.html"><a href="generative.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>10.3</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="10.4" data-path="generative.html"><a href="generative.html#naive-bayes"><i class="fa fa-check"></i><b>10.4</b> Naive Bayes</a></li>
<li class="chapter" data-level="10.5" data-path="generative.html"><a href="generative.html#discrete-inputs"><i class="fa fa-check"></i><b>10.5</b> Discrete Inputs</a></li>
<li class="chapter" data-level="10.6" data-path="generative.html"><a href="generative.html#source-9"><i class="fa fa-check"></i><b>10.6</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="supervised-i.html"><a href="supervised-i.html"><i class="fa fa-check"></i><b>11</b> Supervised Learning Overview I</a><ul>
<li class="chapter" data-level="11.1" data-path="supervised-i.html"><a href="supervised-i.html#reading-7"><i class="fa fa-check"></i><b>11.1</b> Reading</a></li>
<li class="chapter" data-level="11.2" data-path="supervised-i.html"><a href="supervised-i.html#source-10"><i class="fa fa-check"></i><b>11.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="simulation.html"><a href="simulation.html"><i class="fa fa-check"></i><b>12</b> Simulation</a><ul>
<li class="chapter" data-level="12.1" data-path="simulation.html"><a href="simulation.html#reading-8"><i class="fa fa-check"></i><b>12.1</b> Reading</a></li>
<li class="chapter" data-level="12.2" data-path="simulation.html"><a href="simulation.html#source-11"><i class="fa fa-check"></i><b>12.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="bootstrap.html"><a href="bootstrap.html"><i class="fa fa-check"></i><b>13</b> Bootstrap Resampling</a><ul>
<li class="chapter" data-level="13.1" data-path="bootstrap.html"><a href="bootstrap.html#reading-9"><i class="fa fa-check"></i><b>13.1</b> Reading</a></li>
<li class="chapter" data-level="13.2" data-path="bootstrap.html"><a href="bootstrap.html#source-12"><i class="fa fa-check"></i><b>13.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>14</b> Cross-Validation</a><ul>
<li class="chapter" data-level="14.1" data-path="cross-validation.html"><a href="cross-validation.html#reading-10"><i class="fa fa-check"></i><b>14.1</b> Reading</a></li>
<li class="chapter" data-level="14.2" data-path="cross-validation.html"><a href="cross-validation.html#source-13"><i class="fa fa-check"></i><b>14.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="supervised-ii.html"><a href="supervised-ii.html"><i class="fa fa-check"></i><b>15</b> Supervised Learning Overview II</a><ul>
<li class="chapter" data-level="15.1" data-path="supervised-ii.html"><a href="supervised-ii.html#classification-2"><i class="fa fa-check"></i><b>15.1</b> Classification</a><ul>
<li class="chapter" data-level="15.1.1" data-path="supervised-ii.html"><a href="supervised-ii.html#tuning"><i class="fa fa-check"></i><b>15.1.1</b> Tuning</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="supervised-ii.html"><a href="supervised-ii.html#regression-1"><i class="fa fa-check"></i><b>15.2</b> Regression</a><ul>
<li class="chapter" data-level="15.2.1" data-path="supervised-ii.html"><a href="supervised-ii.html#methods"><i class="fa fa-check"></i><b>15.2.1</b> Methods</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="supervised-ii.html"><a href="supervised-ii.html#external-links"><i class="fa fa-check"></i><b>15.3</b> External Links</a></li>
<li class="chapter" data-level="15.4" data-path="supervised-ii.html"><a href="supervised-ii.html#rmarkdown"><i class="fa fa-check"></i><b>15.4</b> <code>rmarkdown</code></a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>16</b> Regularization</a><ul>
<li class="chapter" data-level="16.1" data-path="regularization.html"><a href="regularization.html#reading-11"><i class="fa fa-check"></i><b>16.1</b> Reading</a></li>
<li class="chapter" data-level="16.2" data-path="regularization.html"><a href="regularization.html#source-14"><i class="fa fa-check"></i><b>16.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="dimension-reduction.html"><a href="dimension-reduction.html"><i class="fa fa-check"></i><b>17</b> Dimension Reduction</a><ul>
<li class="chapter" data-level="17.1" data-path="dimension-reduction.html"><a href="dimension-reduction.html#reading-12"><i class="fa fa-check"></i><b>17.1</b> Reading</a></li>
<li class="chapter" data-level="17.2" data-path="dimension-reduction.html"><a href="dimension-reduction.html#source-15"><i class="fa fa-check"></i><b>17.2</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>18</b> Ensemble Methods</a><ul>
<li class="chapter" data-level="18.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#reading-13"><i class="fa fa-check"></i><b>18.1</b> Reading</a></li>
<li class="chapter" data-level="18.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#source-16"><i class="fa fa-check"></i><b>18.2</b> Source</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="additional-reading.html"><a href="additional-reading.html"><i class="fa fa-check"></i><b>A</b> Additional Reading</a><ul>
<li class="chapter" data-level="A.1" data-path="additional-reading.html"><a href="additional-reading.html#books"><i class="fa fa-check"></i><b>A.1</b> Books</a></li>
<li class="chapter" data-level="A.2" data-path="additional-reading.html"><a href="additional-reading.html#papers"><i class="fa fa-check"></i><b>A.2</b> Papers</a></li>
<li class="chapter" data-level="A.3" data-path="additional-reading.html"><a href="additional-reading.html#blog-posts"><i class="fa fa-check"></i><b>A.3</b> Blog Posts</a></li>
<li class="chapter" data-level="A.4" data-path="additional-reading.html"><a href="additional-reading.html#miscellaneous"><i class="fa fa-check"></i><b>A.4</b> Miscellaneous</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="ten-rules.html"><a href="ten-rules.html"><i class="fa fa-check"></i><b>B</b> Ten Simple Rules for Success in STAT 432</a><ul>
<li class="chapter" data-level="B.1" data-path="ten-rules.html"><a href="ten-rules.html#rule-1-there-are-no-rules"><i class="fa fa-check"></i><b>B.1</b> Rule 1: There Are No Rules</a></li>
<li class="chapter" data-level="B.2" data-path="ten-rules.html"><a href="ten-rules.html#rule-2-read-the-syllabus"><i class="fa fa-check"></i><b>B.2</b> Rule 2: Read the Syllabus</a></li>
<li class="chapter" data-level="B.3" data-path="ten-rules.html"><a href="ten-rules.html#rule-3-previous-learning-is-not-gospel"><i class="fa fa-check"></i><b>B.3</b> Rule 3: Previous Learning is Not Gospel</a></li>
<li class="chapter" data-level="B.4" data-path="ten-rules.html"><a href="ten-rules.html#rule-4-all-statements-are-true"><i class="fa fa-check"></i><b>B.4</b> Rule 4: All Statements Are True</a></li>
<li class="chapter" data-level="B.5" data-path="ten-rules.html"><a href="ten-rules.html#rule-5-dont-miss-the-forest-for-the-trees"><i class="fa fa-check"></i><b>B.5</b> Rule 5: Don’t Miss The Forest For The Trees</a></li>
<li class="chapter" data-level="B.6" data-path="ten-rules.html"><a href="ten-rules.html#rule-6-you-will-struggle"><i class="fa fa-check"></i><b>B.6</b> Rule 6: You Will Struggle</a></li>
<li class="chapter" data-level="B.7" data-path="ten-rules.html"><a href="ten-rules.html#rule-7-keep-it-simple"><i class="fa fa-check"></i><b>B.7</b> Rule 7: Keep It Simple</a></li>
<li class="chapter" data-level="B.8" data-path="ten-rules.html"><a href="ten-rules.html#rule-8-rtfm"><i class="fa fa-check"></i><b>B.8</b> Rule 8: RTFM</a></li>
<li class="chapter" data-level="B.9" data-path="ten-rules.html"><a href="ten-rules.html#rule-9-there-are-no-stupid-questions"><i class="fa fa-check"></i><b>B.9</b> Rule 9: There Are No Stupid Questions</a></li>
<li class="chapter" data-level="B.10" data-path="ten-rules.html"><a href="ten-rules.html#rule-10-learn-by-doing"><i class="fa fa-check"></i><b>B.10</b> Rule 10: Learn By Doing</a></li>
<li class="chapter" data-level="B.11" data-path="ten-rules.html"><a href="ten-rules.html#conclusion"><i class="fa fa-check"></i><b>B.11</b> Conclusion</a></li>
<li class="chapter" data-level="B.12" data-path="ten-rules.html"><a href="ten-rules.html#source-17"><i class="fa fa-check"></i><b>B.12</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="computing.html"><a href="computing.html"><i class="fa fa-check"></i><b>C</b> Computing</a><ul>
<li class="chapter" data-level="C.1" data-path="computing.html"><a href="computing.html#reading-14"><i class="fa fa-check"></i><b>C.1</b> Reading</a></li>
<li class="chapter" data-level="C.2" data-path="computing.html"><a href="computing.html#additional-resources"><i class="fa fa-check"></i><b>C.2</b> Additional Resources</a><ul>
<li class="chapter" data-level="C.2.1" data-path="computing.html"><a href="computing.html#r"><i class="fa fa-check"></i><b>C.2.1</b> R</a></li>
<li class="chapter" data-level="C.2.2" data-path="computing.html"><a href="computing.html#rstudio"><i class="fa fa-check"></i><b>C.2.2</b> RStudio</a></li>
<li class="chapter" data-level="C.2.3" data-path="computing.html"><a href="computing.html#r-markdown"><i class="fa fa-check"></i><b>C.2.3</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="computing.html"><a href="computing.html#stat-432-idioms"><i class="fa fa-check"></i><b>C.3</b> STAT 432 Idioms</a><ul>
<li class="chapter" data-level="C.3.1" data-path="computing.html"><a href="computing.html#dont-restore-old-workspaces"><i class="fa fa-check"></i><b>C.3.1</b> Don’t Restore Old Workspaces</a></li>
<li class="chapter" data-level="C.3.2" data-path="computing.html"><a href="computing.html#r-versions"><i class="fa fa-check"></i><b>C.3.2</b> R Versions</a></li>
<li class="chapter" data-level="C.3.3" data-path="computing.html"><a href="computing.html#code-style"><i class="fa fa-check"></i><b>C.3.3</b> Code Style</a></li>
<li class="chapter" data-level="C.3.4" data-path="computing.html"><a href="computing.html#reference-style"><i class="fa fa-check"></i><b>C.3.4</b> Reference Style</a></li>
<li class="chapter" data-level="C.3.5" data-path="computing.html"><a href="computing.html#stat-432-r-style-overrides"><i class="fa fa-check"></i><b>C.3.5</b> STAT 432 R Style Overrides</a></li>
<li class="chapter" data-level="C.3.6" data-path="computing.html"><a href="computing.html#stat-432-r-markdown-style"><i class="fa fa-check"></i><b>C.3.6</b> STAT 432 R Markdown Style</a></li>
<li class="chapter" data-level="C.3.7" data-path="computing.html"><a href="computing.html#style-heuristics"><i class="fa fa-check"></i><b>C.3.7</b> Style Heuristics</a></li>
<li class="chapter" data-level="C.3.8" data-path="computing.html"><a href="computing.html#objects-and-functions"><i class="fa fa-check"></i><b>C.3.8</b> Objects and Functions</a></li>
<li class="chapter" data-level="C.3.9" data-path="computing.html"><a href="computing.html#print-versus-return"><i class="fa fa-check"></i><b>C.3.9</b> Print versus Return</a></li>
<li class="chapter" data-level="C.3.10" data-path="computing.html"><a href="computing.html#help"><i class="fa fa-check"></i><b>C.3.10</b> Help</a></li>
<li class="chapter" data-level="C.3.11" data-path="computing.html"><a href="computing.html#keyboard-shortcuts"><i class="fa fa-check"></i><b>C.3.11</b> Keyboard Shortcuts</a></li>
<li class="chapter" data-level="C.3.12" data-path="computing.html"><a href="computing.html#common-issues"><i class="fa fa-check"></i><b>C.3.12</b> Common Issues</a></li>
</ul></li>
<li class="chapter" data-level="C.4" data-path="computing.html"><a href="computing.html#source-18"><i class="fa fa-check"></i><b>C.4</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>D</b> Probability</a><ul>
<li class="chapter" data-level="D.1" data-path="probability.html"><a href="probability.html#reading-15"><i class="fa fa-check"></i><b>D.1</b> Reading</a></li>
<li class="chapter" data-level="D.2" data-path="probability.html"><a href="probability.html#probability-models"><i class="fa fa-check"></i><b>D.2</b> Probability Models</a></li>
<li class="chapter" data-level="D.3" data-path="probability.html"><a href="probability.html#probability-axioms"><i class="fa fa-check"></i><b>D.3</b> Probability Axioms</a></li>
<li class="chapter" data-level="D.4" data-path="probability.html"><a href="probability.html#probability-rules"><i class="fa fa-check"></i><b>D.4</b> Probability Rules</a></li>
<li class="chapter" data-level="D.5" data-path="probability.html"><a href="probability.html#random-variables"><i class="fa fa-check"></i><b>D.5</b> Random Variables</a><ul>
<li class="chapter" data-level="D.5.1" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>D.5.1</b> Distributions</a></li>
<li class="chapter" data-level="D.5.2" data-path="probability.html"><a href="probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>D.5.2</b> Discrete Random Variables</a></li>
<li class="chapter" data-level="D.5.3" data-path="probability.html"><a href="probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>D.5.3</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="D.5.4" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>D.5.4</b> Distributions in R</a></li>
<li class="chapter" data-level="D.5.5" data-path="probability.html"><a href="probability.html#several-random-variables"><i class="fa fa-check"></i><b>D.5.5</b> Several Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="D.6" data-path="probability.html"><a href="probability.html#expectations"><i class="fa fa-check"></i><b>D.6</b> Expectations</a></li>
<li class="chapter" data-level="D.7" data-path="probability.html"><a href="probability.html#likelihood"><i class="fa fa-check"></i><b>D.7</b> Likelihood</a></li>
<li class="chapter" data-level="D.8" data-path="probability.html"><a href="probability.html#references"><i class="fa fa-check"></i><b>D.8</b> References</a><ul>
<li class="chapter" data-level="D.8.1" data-path="probability.html"><a href="probability.html#videos"><i class="fa fa-check"></i><b>D.8.1</b> Videos</a></li>
</ul></li>
<li class="chapter" data-level="D.9" data-path="probability.html"><a href="probability.html#source-19"><i class="fa fa-check"></i><b>D.9</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>E</b> Statistics</a><ul>
<li class="chapter" data-level="E.1" data-path="statistics.html"><a href="statistics.html#reading-16"><i class="fa fa-check"></i><b>E.1</b> Reading</a></li>
<li class="chapter" data-level="E.2" data-path="statistics.html"><a href="statistics.html#statistics-1"><i class="fa fa-check"></i><b>E.2</b> Statistics</a></li>
<li class="chapter" data-level="E.3" data-path="statistics.html"><a href="statistics.html#estimators"><i class="fa fa-check"></i><b>E.3</b> Estimators</a><ul>
<li class="chapter" data-level="E.3.1" data-path="statistics.html"><a href="statistics.html#properties"><i class="fa fa-check"></i><b>E.3.1</b> Properties</a></li>
<li class="chapter" data-level="E.3.2" data-path="statistics.html"><a href="statistics.html#example-mse-of-an-estimator"><i class="fa fa-check"></i><b>E.3.2</b> Example: MSE of an Estimator</a></li>
<li class="chapter" data-level="E.3.3" data-path="statistics.html"><a href="statistics.html#estimation-methods"><i class="fa fa-check"></i><b>E.3.3</b> Estimation Methods</a></li>
<li class="chapter" data-level="E.3.4" data-path="statistics.html"><a href="statistics.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>E.3.4</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="E.3.5" data-path="statistics.html"><a href="statistics.html#method-of-moments"><i class="fa fa-check"></i><b>E.3.5</b> Method of Moments</a></li>
<li class="chapter" data-level="E.3.6" data-path="statistics.html"><a href="statistics.html#empirical-distribution-function"><i class="fa fa-check"></i><b>E.3.6</b> Empirical Distribution Function</a></li>
</ul></li>
<li class="chapter" data-level="E.4" data-path="statistics.html"><a href="statistics.html#source-20"><i class="fa fa-check"></i><b>E.4</b> Source</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://daviddalpiaz.org" target="blank">&copy; 2020 David Dalpiaz</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basics of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Linear Regression</h1>
<hr />
<p>This chapter will discuss <strong>linear regression</strong> models, but for a very specific purpose: using linear regression models to make <strong>predictions</strong>.</p>
<p>Specifically, we will discuss:</p>
<ul>
<li>The regression function and estimating conditional means.</li>
<li>Using the <code>lm()</code> and <code>predict()</code> functions in R.</li>
<li>Data splits to evaluate model performance for machine learning tasks.</li>
</ul>
<hr />
<div id="reading" class="section level2">
<h2><span class="header-section-number">2.1</span> Reading</h2>
<ul>
<li><strong>Required:</strong> <a href="https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf">ISL Chapter 3</a>
<ul>
<li>Skip section 3.6 which is dedicated to R.</li>
<li>Consider this reading a review of previous regression knowledge. The information provided in this chapter of BSL will be the relevant material for STAT 432, but it is still worthwhile to read ISL. However note that this chapter of ISL overemphasizes inference, diagnostics, and at times hints too closely to causal claims for novice readers. (We’re not saying the authors are guilty of making the “correlation is not causation error,” instead, we’ve found that we need to be extremely clear about this issue with students in STAT 432.)</li>
</ul></li>
<li><strong>Reference:</strong> <a href="http://stat420.org">STAT 420 @ UIUC: Notes</a>
<ul>
<li>In particular <a href="https://daviddalpiaz.github.io/appliedstats/model-building.html">Chapter 10</a> which discusses model building will be relevant. The section on explanation versus prediction is extremely relevant, although note that it contains some differences in definitions, especially concerning test data. That said, the general ideas are important.</li>
</ul></li>
</ul>
<hr />
</div>
<div id="explanation-versus-prediction" class="section level2">
<h2><span class="header-section-number">2.2</span> Explanation versus Prediction</h2>
<p>Before we even begin to discuss regression, we make a bold announcement: <strong>STAT 432 is not a course about inference.</strong> It is very possible that there will be <strong>zero</strong> causal claims in this book. While it would certainly be nice (but extremely difficult) to uncover causal relationships, our focus will be on predictive relationships.</p>
<p>Suppose (although it is likely untrue) that there is a strong correlation between wearing a wrist watch, and car accidents. Depending on your frame of reference, you should view this information in very different ways.</p>
<ul>
<li>Suppose you are a car insurance company. This is great news! You can now more accurately predict the number of accidents of your policy holders if you know whether or not they wear a wrist watch. For the sake of understanding how much your company will need to pay out in a year, you don’t care what <em>causes</em> accidents, you just want to be able to <strong>predict</strong> (estimate) the number of accidents.</li>
<li>Suppose you are a car driver. As a driver, you want to stay safe. That is, you want to do things that decrease accidents. In this framing, you care about things that <strong>cause</strong> accidents, not things that <em>predict</em> accidents. In other words, this correlation information should <strong>not</strong> lead to you throwing away your wrist watch.</li>
</ul>
<p><em>Disclaimer:</em> Extremely high correlation should not simply be ignored. For example, there is a very high correlation between smoking and lung cancer. (Fun fact: <a href="https://en.wikipedia.org/wiki/Ronald_Fisher">RA Fisher</a>, the most famous statistician, did not believe that smoking caused cancer. It’s actually a part of a larger <a href="https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2014.00765.x">fasinating story</a>.) However, this strong correlation is not proof that smoking causes lung cancer. Instead, additional study is needed to rule out confounders, establish mechanistic relationships, and more.</p>
</div>
<div id="setup" class="section level2">
<h2><span class="header-section-number">2.3</span> Setup</h2>
<p>We now introduce the <strong>regression</strong> task. Regression is a subset of a broader machine learning tasks called <em>supervised learning</em>, which also include <em>classification</em>. (We will return later to discuss supervised learning in general after getting through some specifics of regression and classification.)</p>
<p>Stated simply, the regression tasks seeks to estimate (predict) a <strong>numeric</strong> quantity. For example:</p>
<ul>
<li>Estimating the <strong>salary</strong> of a baseball player.</li>
<li>Estimating the <strong>price</strong> of a home for sale.</li>
<li>Estimating the <strong>credit score</strong> of a bank customer,</li>
<li>Estimating the number of <strong>downloads</strong> of a podcast.</li>
</ul>
<p>Each of these quantities is some numeric value. The goal of regression is to estimate (predict) these quantities when they are unknown through the use of additional, possibly correlated quantities, for example the offensive and defensive statistics of a baseball player, or the location and attributes of a home.</p>
</div>
<div id="mathematical-setup" class="section level2">
<h2><span class="header-section-number">2.4</span> Mathematical Setup</h2>
<p>To get a better grasp of what regression is, we move to defining the task mathematically. Consider a random variable <span class="math inline">\(Y\)</span> which represents a <strong>response</strong> (or outcome or target) variable, and <span class="math inline">\(p\)</span> <strong>feature</strong> variables <span class="math inline">\(\boldsymbol{X} = (X_1, X_2, \ldots, X_p)\)</span>. Features are also called covariates or predictors. (We find the “predictors” nomenclature to be problematic when discussing prediction tasks.)</p>
<p>In the most common regression setup, we assume that the response variable <span class="math inline">\(Y\)</span> is some function of the features, plus some random noise.</p>
<p><span class="math display">\[
Y = f(\boldsymbol{X}) + \epsilon
\]</span></p>
<ul>
<li>We call <span class="math inline">\(f(\boldsymbol{X})\)</span> the <strong>signal</strong>. This <span class="math inline">\(f\)</span> is the function that we would like to <em>learn</em>.</li>
<li>We call <span class="math inline">\(\epsilon\)</span> the <strong>noise</strong>. We do not want to learn this which we risk if we overfit. (More on this later.)</li>
</ul>
<p>So our goal will be to find some <span class="math inline">\(f\)</span> such that <span class="math inline">\(f(\boldsymbol{X})\)</span> is close to <span class="math inline">\(Y\)</span>. But how do we define close? There are many ways but we will start with, and most often consider, squared error loss. Specifically, we define a loss function,</p>
<p><span class="math display">\[
L(Y, f(\boldsymbol{X})) \triangleq \left(Y - f(\boldsymbol{X})\right) ^ 2
\]</span></p>
<p>Now we can clarify the goal of regression, which is to minimize the above loss, <em>on average</em>. We call this the <strong>risk</strong> of estimating <span class="math inline">\(Y\)</span> using <span class="math inline">\(f(\boldsymbol{X})\)</span>.</p>
<p><span class="math display">\[
R(Y, f(\boldsymbol{X})) \triangleq \mathbb{E}[L(Y, f(\boldsymbol{X}))] = \mathbb{E}_{\boldsymbol{X}, Y}[(Y - f(\boldsymbol{X})) ^ 2]
\]</span></p>
<p>Before attempting to minimize the risk, we first re-write the risk after conditioning on <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
<p><span class="math display">\[
\mathbb{E}_{\boldsymbol{X}, Y} \left[ (Y - f(\boldsymbol{X})) ^ 2 \right] = \mathbb{E}_{\boldsymbol{X}} \mathbb{E}_{Y \mid \boldsymbol{X}} \left[ ( Y - f(\boldsymbol{X}) ) ^ 2 \mid \boldsymbol{X} = \boldsymbol{x} \right]
\]</span></p>
<p>Minimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to <span class="math inline">\(Y \mid \boldsymbol{X}\)</span>, essentially minimizing the risk pointwise, for each <span class="math inline">\(\boldsymbol{x}\)</span>.</p>
<p>It turns out, that the risk is minimized by the <strong>conditional mean</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\boldsymbol{X}\)</span>,</p>
<p><span class="math display">\[
\mu(\boldsymbol{x}) \triangleq \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}]
\]</span></p>
<p>which we call the <strong>regression function</strong>. (This is not a learned function, this is the function we would like to learn in order to minimize the squared error loss on average. <span class="math inline">\(f\)</span> is any function, <span class="math inline">\(\mu\)</span> is the function that would minimize squared error loss on average if we knew if, but will instead need to learn it form the data.</p>
<p>Note that <span class="math inline">\(\boldsymbol{x}\)</span> represents (potential) realized values of the random variables <span class="math inline">\(\boldsymbol{X}\)</span>.</p>
<p><span class="math display">\[
\boldsymbol{x} = (x_1, x_2, \ldots, x_p)
\]</span></p>
<p>We can now state the goal of the regression task: we want to <strong>estimate</strong> the <strong>regression function</strong>. How do we do that?</p>
</div>
<div id="linear-regression-models" class="section level2">
<h2><span class="header-section-number">2.5</span> Linear Regression Models</h2>
<p>What do linear regression models <strong>do</strong>? They estimate the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\boldsymbol{X}\)</span>! (How convenient.)</p>
<p>Consider the following probability model</p>
<p><span class="math display">\[
Y = 1 - 2x - 3x ^ 2 + 5x ^ 3 + \epsilon
\]</span></p>
<p>where <span class="math inline">\(\epsilon \sim \text{N}(0, \sigma^2)\)</span>.</p>
<p>Alternatively we could write</p>
<p><span class="math display">\[
Y \mid X \sim \text{N}(1 - 2x - 3x ^ 2 + 5x ^ 3, \sigma^2)
\]</span></p>
<p>This perhaps makes it clearer that</p>
<p><span class="math display">\[
\mu(x) = \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}] = 1 - 2x - 3x ^ 2 + 5x ^ 3
\]</span></p>
<p>What do linear models do? More specifically than before, linear regression models estimate the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\boldsymbol{X}\)</span> by assuming this conditional mean is a <strong>linear combination of the feature variables</strong>.</p>
<p>Suppose for a moment that we did not know the above <strong>true</strong> probability model, or even the more specifically the regression function. Instead, all we had was some data, <span class="math inline">\((x_i, y_i)\)</span> for <span class="math inline">\(i = 1, 2, \ldots, n\)</span>.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-0.47
</td>
<td style="text-align:right;">
-0.06
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.26
</td>
<td style="text-align:right;">
1.72
</td>
</tr>
<tr>
<td style="text-align:right;">
0.15
</td>
<td style="text-align:right;">
1.39
</td>
</tr>
<tr>
<td style="text-align:right;">
0.82
</td>
<td style="text-align:right;">
0.68
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.60
</td>
<td style="text-align:right;">
-0.27
</td>
</tr>
<tr>
<td style="text-align:right;">
0.80
</td>
<td style="text-align:right;">
1.55
</td>
</tr>
<tr>
<td style="text-align:right;">
0.89
</td>
<td style="text-align:right;">
0.76
</td>
</tr>
<tr>
<td style="text-align:right;">
0.32
</td>
<td style="text-align:right;">
-0.40
</td>
</tr>
<tr>
<td style="text-align:right;">
0.26
</td>
<td style="text-align:right;">
-1.85
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.88
</td>
<td style="text-align:right;">
-1.85
</td>
</tr>
</tbody>
</table>
<p>How do we fit (or “train” in ML language) a linear model with this data? In order words, how to be learn the regression function from this data with a linear regression model?</p>
<p>First, we need to make assumptions about the <em>form</em> of the regression function, up to, but <strong>not</strong> including some unknown parameters. Consider three possible linear models, in particular, three possible regression functions.</p>
<p><strong>Degree 1 Polynomial</strong></p>
<p><span class="math display">\[
\mu(x) = \beta_0 + \beta_1 x
\]</span></p>
<p><strong>Degree 3 Polynomial</strong></p>
<p><span class="math display">\[
\mu(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3
\]</span></p>
<p><strong>Degree 9 Polynomial</strong></p>
<p><span class="math display">\[
\mu(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \ldots + \beta_9 x^9
\]</span></p>
<p>These are chosen mostly arbitrarily for illustrative purposes which we’ll see in a moment.</p>
<p>So how do we actually <strong>fit</strong> these models, that is train them, with the given data. We have a couple of options: Maximum Likelihood or <strong>Least Squares</strong>! In this case, they actually produce the same result, so we use least squares for simplicity of explanation.</p>
<p>To fit the degree 3 polynomial using least squares, we <em>minimize</em></p>
<p><span class="math display">\[
\sum_{i = 1}^{n}\left(y_i - (\beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3)\right) ^ 2
\]</span></p>
<p>Skipping the details of the minimization, we would acquire <span class="math inline">\(\hat{\beta}_0\)</span>, <span class="math inline">\(\hat{\beta}_1\)</span>, <span class="math inline">\(\hat{\beta}_2\)</span>, and <span class="math inline">\(\hat{\beta}_3\)</span> which are estimates of <span class="math inline">\({\beta}_0\)</span>, <span class="math inline">\({\beta}_1\)</span>, <span class="math inline">\({\beta}_2\)</span>, and <span class="math inline">\({\beta}_3\)</span>.</p>
<p>Taken together, we would have</p>
<p><span class="math display">\[
\hat{\mu}(x) = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2^2 + \hat{\beta}_3 x_3^3
\]</span></p>
<p>which is then an estimate of <span class="math inline">\(\mu(x)\)</span>.</p>
<p>While in this case, it will almost certainly not be the case that <span class="math inline">\(\hat{\beta}_0 = 1\)</span> or <span class="math inline">\(\hat{\beta}_1 = -2\)</span> or <span class="math inline">\(\hat{\beta}_2 = -3\)</span> or <span class="math inline">\(\hat{\beta}_3 = 5\)</span>, which are the true values of the <span class="math inline">\(\beta\)</span> coefficients, they are at least reasonable estimates.</p>
<p>As a bit of an aside, note that in this case, it is sort of ambiguous as to whether there is one feature, <span class="math inline">\(x\)</span>, which is seen in the data, or three features <span class="math inline">\(x\)</span>, <span class="math inline">\(x^2\)</span>, and <span class="math inline">\(x^3\)</span>, which are seen in the model. The truth is sort of in the middle. The data has a single feature, but through feature engineering, we have created two additional features for fitting the model. Note that when using R, <strong>you do not need to modify the data to do this</strong>, instead you should use R’s formula syntax to specify this feature engineering when fitting the model. More on this when we discuss the <code>lm()</code> function in R. (We introduce this somewhat confusing notion early so we can emphasize that linear models are about linear combinations of features, not necessarily linear relationships. Although, linear models are very good at learning linear relationships.)</p>
<p>Suppose instead we had assumed that</p>
<p><span class="math display">\[
\mu(x) = \beta_0 + \beta_1 x
\]</span></p>
<p>This model is obviously flawed as it doesn’t contain enough terms to capture the true regression function. (Later we will say this model is not “flexible” enough.)</p>
<p>Or, suppose we had assumed</p>
<p><span class="math display">\[
\mu(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \ldots + \beta_9 x^9
\]</span></p>
<p>This model is also flawed, but for a different reason. (Later we will say this model is too “flexible.”) After using least squares, we will obtain some <span class="math inline">\(\hat{\beta}_9\)</span> even though there is not a 9th degree term in the true regression function!</p>
<p>Let’s take a look at this visually.</p>
<div class="wide">
<p><img src="linear-regression_files/figure-html/unnamed-chunk-6-1.png" width="1152" style="display: block; margin: auto;" /></p>
</div>
<p>Here we see the three models fit to the data above. The dashed black curve is the true mean function, that is the true mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span>, and the solid colored curves are the estimated mean functions.</p>
<p>Now we ask the question: which of these models is best? Given these pictures, there are two criteria that we could consider.</p>
<ul>
<li>How close is the estimated regression (mean) function to the data? (Degree 9 is best! There is no error!)</li>
<li>How close is the estimated regression (mean) function to the true regression (mean) function? (Degree 3 is best.)</li>
</ul>
<p>From the presentation here, it’s probably clear that the latter is actually what matters. We can demonstrate this by generating some “new” data.</p>
<p><img src="linear-regression_files/figure-html/unnamed-chunk-8-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>These plots match the plots above, except newly simulated data is shown. (The regression functions were still estimated with the previous data.) Note that the degree 3 polynomial matches the data about the same as before. The degree 9 polynomial now correctly predicts none of the new data and makes some <strong>huge</strong> errors.</p>
<p>We will define these concepts more generally later, but for now we note that:</p>
<ul>
<li>The Degree 9 Polynomial is <strong>overfitting</strong>. It performs well on the data used to fit the model, but poorly on new data.</li>
<li>The Degree 1 Polynomial is <strong>underfitting</strong>. It performs poorly on the data used to fit the model and poorly on new data.</li>
</ul>
<p>There’s a bit of a problem though! In practice, we don’t know the true mean function, and we don’t have the magical ability to simulate new data! Yikes! After we discuss a bit about how to fit these models in R, we’ll return to this issue. (Spoiler: Don’t fit the model to all the available data. Pretend the data you didn’t use is “new” when you evaluate models.)</p>
</div>
<div id="using-lm" class="section level2">
<h2><span class="header-section-number">2.6</span> Using <code>lm()</code></h2>
<p>Before we continue, let’s consider a different data generating process. We first define this data generating process as an R function.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1">gen_mlr_data =<span class="st"> </span><span class="cf">function</span>(<span class="dt">sample_size =</span> <span class="dv">250</span>) {</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">  x1 =<span class="st"> </span><span class="kw">round</span>(<span class="kw">runif</span>(<span class="dt">n =</span> sample_size), <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">  x2 =<span class="st"> </span><span class="kw">round</span>(<span class="kw">runif</span>(<span class="dt">n =</span> sample_size), <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1-4" data-line-number="4">  x3 =<span class="st"> </span><span class="kw">round</span>(<span class="kw">runif</span>(<span class="dt">n =</span> sample_size), <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1-5" data-line-number="5">  x4 =<span class="st"> </span><span class="kw">factor</span>(<span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;C&quot;</span>), <span class="dt">size =</span> sample_size, <span class="dt">replace =</span> <span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb1-6" data-line-number="6">  x5 =<span class="st"> </span><span class="kw">round</span>(<span class="kw">runif</span>(<span class="dt">n =</span> sample_size), <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1-7" data-line-number="7">  x6 =<span class="st"> </span><span class="kw">round</span>(<span class="kw">runif</span>(<span class="dt">n =</span> sample_size), <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1-8" data-line-number="8">  y =<span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="kw">sin</span>(x2) <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x3 <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1-9" data-line-number="9"><span class="st">    </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>(x4 <span class="op">==</span><span class="st"> &quot;B&quot;</span>) <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(x4 <span class="op">==</span><span class="st"> &quot;C&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1-10" data-line-number="10"><span class="st">    </span><span class="kw">rnorm</span>(<span class="dt">n =</span> sample_size, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb1-11" data-line-number="11">  <span class="kw">tibble</span>(y, x1, x2, x3, x4, x5, x6)</a>
<a class="sourceLine" id="cb1-12" data-line-number="12">}</a></code></pre></div>
<p>We then run the function and store the data that is returned.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">sim_mlr_data =<span class="st"> </span><span class="kw">gen_mlr_data</span>()</a></code></pre></div>
<p>We then inspect the data.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">head</span>(sim_mlr_data)</a></code></pre></div>
<pre><code>## # A tibble: 6 x 7
##       y    x1    x2    x3 x4       x5    x6
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  2.85 0.91   0.33  0.14 A      0.53  0.24
## 2  6.22 0.94   0.19  0.18 B      0.7   0.51
## 3  6.71 0.290  0.27  0.52 B      0.05  0.51
## 4  7.84 0.83   0.53  0.81 B      0.92  0.76
## 5  2.75 0.64   0.02  0.12 A      0.03  0.27
## 6  4.60 0.52   0.8   0.89 A      0.78  0.69</code></pre>
<p>Note that we see only numeric (<code>dbl</code> or <code>int</code>) and factor (<code>fctr</code>) variables. For now, we will require that data contains only these types, and in particular, we will coerce any categorical variables to be factors. (More on this later.)</p>
<p>Mathematically, this data was generated from the probability model</p>
<p><span class="math display">\[
Y \mid \boldsymbol{X} \sim \text{N}(2 + 1\cdot x_1 + 1 \cdot \sin(x_2) + 3 \cdot x_3^3 + 3 \cdot x_{4B} -2 \cdot x_{4C}, \sigma^2 = 0.25)
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(x_{4B}\)</span> is a dummy variable which takes the value 1 when <span class="math inline">\(x_4 = \text{B}\)</span> and 0 otherwise</li>
<li><span class="math inline">\(x_{4C}\)</span> is a dummy variable which takes the value 1 when <span class="math inline">\(x_4 = \text{C}\)</span> and 0 otherwise</li>
</ul>
<p>In particular, the true mean function is</p>
<p><span class="math display">\[
\mu(\boldsymbol{x}) = 2 + 1\cdot x_1 + 1 \cdot \sin(x_2) + 3 \cdot x_3^3 + 3 \cdot x_{4B} -2 \cdot x_{4C}
\]</span></p>
<p>Now, finally, let’s fit some models it R to this data! To do so, we will use one of the most important functions in R, the <code>lm()</code> function.</p>
<p>Let’s specify the form of some assumed mean functions of models that we would like to fit.</p>
<p><strong>Model 1</strong> or <code>mod_1</code> in R</p>
<p><span class="math display">\[
\mu_1(\boldsymbol{x}) = \beta_0 + \beta_1 x_1
\]</span></p>
<p><strong>Model 2</strong> or <code>mod_2</code> in R</p>
<p><span class="math display">\[
\mu_2(\boldsymbol{x}) = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<p><strong>Model 3</strong> or <code>mod_3</code> in R</p>
<p><span class="math display">\[
\mu_3(\boldsymbol{x}) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_{4B} x_{4B} +\beta_{4C} x_{4C} + \beta_5 x_5 + \beta_6 x_6
\]</span></p>
<p><strong>Model 4</strong> or <code>mod_4</code> in R</p>
<p><span class="math display">\[
\mu_4(\boldsymbol{x}) = \beta_0 + \beta_1 x_1 + \beta_2 \sin(x_2) + \beta_3 x_3^3 + \beta_{4B} x_{4B} + \beta_{4C} x_{4C}
\]</span></p>
<p>Now, finally, R!</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1">mod_<span class="dv">1</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1, <span class="dt">data =</span> sim_mlr_data)</a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="kw">coef</span>(mod_<span class="dv">1</span>)</a></code></pre></div>
<pre><code>## (Intercept)          x1 
##   3.7834423   0.9530758</code></pre>
<p>Nothing too interesting here about fitting Model 1. We see that the <code>coef()</code> function returns estimate of the <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> parameters defined above.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1">mod_<span class="dv">2</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">data =</span> sim_mlr_data)</a>
<a class="sourceLine" id="cb7-2" data-line-number="2"><span class="kw">coef</span>(mod_<span class="dv">2</span>)</a></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   3.8747999   0.9400654  -0.1802538</code></pre>
<p>Again, Model 2 isn’t too interesting. We see that the <code>coef()</code> function returns estimate of the <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span> parameters defined above.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1">mod_<span class="dv">3</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> sim_mlr_data)</a>
<a class="sourceLine" id="cb9-2" data-line-number="2"><span class="kw">coef</span>(mod_<span class="dv">3</span>)</a></code></pre></div>
<pre><code>## (Intercept)          x1          x2          x3         x4B         x4C 
##  1.71015079  0.76017877  0.77637360  3.00479841  3.06812204 -1.93068734 
##          x5          x6 
## -0.12248770 -0.04797294</code></pre>
<p>Now, Model 3, we see a couple interesting things. First, the formula syntax <code>y ~ .</code> fits a model with <code>y</code> as the response, and all other variables in the <code>sim_mlr_data</code> data frame (tibble) as features.</p>
<p>Also note: <strong>we did not manually create the needed dummy variables!</strong> R did this for us!</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">levels</span>(sim_mlr_data<span class="op">$</span>x4)</a></code></pre></div>
<pre><code>## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot;</code></pre>
<p>Because <code>x4</code> is a factor variable, R uses the first level, <code>A</code>, as the reference level, and then creates dummy variables for the remaining levels. Cool!</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1">mod_<span class="dv">4</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">sin</span>(x2)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(x3 <span class="op">^</span><span class="st"> </span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>x4, <span class="dt">data =</span> sim_mlr_data)</a>
<a class="sourceLine" id="cb13-2" data-line-number="2"><span class="kw">coef</span>(mod_<span class="dv">4</span>)</a></code></pre></div>
<pre><code>## (Intercept)          x1  I(sin(x2))     I(x3^3)         x4B         x4C 
##   2.3435702   0.8176247   0.9159963   3.0446314   3.0369950  -1.9421931</code></pre>
<p>Our last model, <code>mod_4</code> is the most interesting. It makes use of the inhibit function, <code>I()</code>. This allows for on-the-fly feature engineering based on available features. We’re creating new features via R’s formula syntax as we fit the model.</p>
<p>To see why this is necessary, consider the following:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>(x1 <span class="op">+</span><span class="st"> </span>x2) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>, <span class="dt">data =</span> sim_mlr_data)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ (x1 + x2)^2, data = sim_mlr_data)
## 
## Coefficients:
## (Intercept)           x1           x2        x1:x2  
##      4.1800       0.3353      -0.8259       1.3130</code></pre>
<p>This created an interaction term! That means the <code>^</code> operator has different uses depending on the context. In specifying a formula, it has a particular use, in this case specifying an interaction term, and all lower order terms. However, inside of <code>I()</code> it will be used for exponentiation. For details, use <code>?I</code> and <code>?formula</code>. These are complex R topics, but it will help to start to learn them. For some additional reading on R’s formula syntax, the following two blog posts by Max Kuhn are good reads:</p>
<ul>
<li><a href="https://rviews.rstudio.com/2017/02/01/the-r-formula-method-the-good-parts/">The R Formula Method: The <strong>Good</strong> Parts</a></li>
<li><a href="https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/">The R Formula Method: The <strong>Bad</strong> Parts</a></li>
</ul>
<p>For the first half of this book, we will always keep the data mostly untouched, and rely heavily on the use of R’s formula syntax.</p>
<p>If you are ever interested in what’s happening under the hood when you use the formula syntax, and you recall the linear algebra necessary to perform linear regression, the <code>model.matrix()</code> function will be useful.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">model.matrix</span>(mod_<span class="dv">4</span>))</a></code></pre></div>
<pre><code>##   (Intercept)   x1 I(sin(x2))  I(x3^3) x4B x4C
## 1           1 0.91 0.32404303 0.002744   0   0
## 2           1 0.94 0.18885889 0.005832   1   0
## 3           1 0.29 0.26673144 0.140608   1   0
## 4           1 0.83 0.50553334 0.531441   1   0
## 5           1 0.64 0.01999867 0.001728   0   0
## 6           1 0.52 0.71735609 0.704969   0   0</code></pre>
<p>Back to talking about <code>mod_4</code>. Recall that we had assumed that</p>
<p><span class="math display">\[
\mu_4(\boldsymbol{x}) = \beta_0 + \beta_1 x_1 + \beta_2 \sin(x_2) + \beta_3 x_3^3 + \beta_{4B} x_{4B} + \beta_{4C} x_{4C}
\]</span></p>
<p>Also recall that the true mean function is</p>
<p><span class="math display">\[
\mu(\boldsymbol{x}) = 2 + 1\cdot x_1 + 1 \cdot \sin(x_2) + 3 \cdot x_3^3 + 3 \cdot x_{4B} -2 \cdot x_{4C}
\]</span></p>
<p>Because we know this, we can investigate how well our model is performing. We know the true values of the parameters, in this case</p>
<ul>
<li><span class="math inline">\(\beta_0 = 2\)</span></li>
<li><span class="math inline">\(\beta_1 = 1\)</span></li>
<li><span class="math inline">\(\beta_2 = 1\)</span></li>
<li><span class="math inline">\(\beta_3 = 3\)</span></li>
<li><span class="math inline">\(\beta_{4B} = 3\)</span></li>
<li><span class="math inline">\(\beta_{4C} = -2\)</span></li>
<li><span class="math inline">\(\beta_5 = 0\)</span> (<span class="math inline">\(x_5\)</span> is not used in the true mean function.)</li>
<li><span class="math inline">\(\beta_6 = 0\)</span> (<span class="math inline">\(x_6\)</span> is not used in the true mean function.)</li>
</ul>
<p>We also have the estimated coefficients from <code>mod_4</code>.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="kw">coef</span>(mod_<span class="dv">4</span>)</a></code></pre></div>
<pre><code>## (Intercept)          x1  I(sin(x2))     I(x3^3)         x4B         x4C 
##   2.3435702   0.8176247   0.9159963   3.0446314   3.0369950  -1.9421931</code></pre>
<ul>
<li><span class="math inline">\(\hat{\beta}_0 = 2.34\)</span></li>
<li><span class="math inline">\(\hat{\beta}_1 = 0.82\)</span></li>
<li><span class="math inline">\(\hat{\beta}_2 = 0.92\)</span></li>
<li><span class="math inline">\(\hat{\beta}_3 = 3.04\)</span></li>
<li><span class="math inline">\(\hat{\beta}_{4B} = 3.04\)</span></li>
<li><span class="math inline">\(\hat{\beta}_{4C} = -1.94\)</span></li>
<li><span class="math inline">\(\hat{\beta}_5 = 0\)</span> (We <strong>assumed</strong> <span class="math inline">\(x_5\)</span> is not used in the true mean function.)</li>
<li><span class="math inline">\(\hat{\beta}_6 = 0\)</span> (We <strong>assumed</strong> <span class="math inline">\(x_6\)</span> is not used in the true mean function.)</li>
</ul>
<p>Our estimated regression (mean) function is then</p>
<p><span class="math display">\[
\hat{\mu}_4(\boldsymbol{x}) = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 \sin(x_2) + \hat{\beta}_3 x_3^3 + \hat{\beta}_{4B} x_{4B} + \hat{\beta}_{4C} x_{4C}
\]</span></p>
<p>Perfect? No. Pretty good? Maybe. However, in reality, this is not a check that we can perform! We still need an evaluation strategy that doesn’t depend on knowing the true model!</p>
<p>Note that the other models are “bad” in this case because they are either missing features (<code>mod_1</code> and <code>mod_2</code>) or the are both missing features and contain unnecessary features <code>(mod_3</code>).</p>
</div>
<div id="the-predict-function" class="section level2">
<h2><span class="header-section-number">2.7</span> The <code>predict()</code> Function</h2>
<p>We stated previously that fitting a linear regression model means that we are learning the regression (mean) <strong>function</strong>. Now that we fit and stored some models, how do we access these estimated regression (mean) functions? The <code>predict()</code> function!</p>
<p>The <code>predict()</code> function will be the workhorse of STAT 432. Let’s see how to use it with models fit using the <code>lm()</code> function.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb21-2" data-line-number="2">new_obs =<span class="st"> </span><span class="kw">gen_mlr_data</span>(<span class="dt">sample_size =</span> <span class="dv">1</span>)</a></code></pre></div>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1">new_obs</a></code></pre></div>
<pre><code>## # A tibble: 1 x 7
##       y    x1    x2    x3 x4       x5    x6
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 0.751  0.17  0.81  0.38 C       0.6   0.6</code></pre>
<p>Suppose we wanted to estimate the mean of <span class="math inline">\(Y\)</span> when</p>
<ul>
<li><span class="math inline">\(x_1 = 0.17\)</span></li>
<li><span class="math inline">\(x_2 = 0.81\)</span></li>
<li><span class="math inline">\(x_3 = 0.38\)</span></li>
<li><span class="math inline">\(x_4 = \text{C}\)</span></li>
<li><span class="math inline">\(x_5 = 0.38\)</span></li>
<li><span class="math inline">\(x_6 = 0.38\)</span></li>
</ul>
<p>In other words, we want to estimate</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid \boldsymbol{X} = \texttt{new_obs}] = 
\mathbb{E}[Y \mid  
X_1 = 0.17, 
X_2 = 0.81, 
X_3 = 0.38, 
X_4 = \text{C}, 
X_5 = 0.6, 
X_6 = 0.6]
\]</span></p>
<p>The <code>predict()</code> function to the rescue!</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1"><span class="kw">predict</span>(mod_<span class="dv">1</span>, new_obs)</a></code></pre></div>
<pre><code>##        1 
## 3.945465</code></pre>
<p>What’s being returned here?</p>
<p><span class="math display">\[
\hat{\mu}_1(\texttt{new_obs}) = \hat{\mathbb{E}}[Y \mid \boldsymbol{X} = \texttt{new_obs}] = 3.9454652
\]</span></p>
<p>The predict function, together with a trained model, <strong>is</strong> the estimated regression (mean) function! Supply a different trained model, then you get that estimated regression (mean) function.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">predict</span>(mod_<span class="dv">4</span>, new_obs)</a></code></pre></div>
<pre><code>##        1 
## 1.370883</code></pre>
<p>What’s being returned here?</p>
<p><span class="math display">\[
\hat{\mu}_4(\texttt{new_obs}) = \hat{\mathbb{E}}[Y \mid \boldsymbol{X} = \texttt{new_obs}] = 1.3708827
\]</span></p>
<p>We could compare these two estimates of the conditional mean of <span class="math inline">\(Y\)</span> to the true value of <code>y</code> observed in the observation. More on that in the next section.</p>
<p>If given an entire dataset, instead of a single observation, <code>predict()</code> returns the estimated conditional mean of each observation.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">9</span>)</a>
<a class="sourceLine" id="cb28-2" data-line-number="2">some_more_data =<span class="st"> </span><span class="kw">gen_mlr_data</span>(<span class="dt">sample_size =</span> <span class="dv">10</span>)</a></code></pre></div>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1"><span class="kw">predict</span>(mod_<span class="dv">4</span>, some_more_data)</a></code></pre></div>
<pre><code>##         1         2         3         4         5         6         7         8 
## 7.8896349 5.4061018 1.3788387 0.8560024 6.6246872 8.2203544 3.2140060 3.5738889 
##         9        10 
## 5.9928135 8.4908895</code></pre>
<p>Neat!</p>
<p>A warning: <strong>Do not <em>name</em> the second argument to the predict function.</strong> This will cause issues because sometimes the name of that argument is <code>newdata</code>, as it is here, but sometimes it is <code>data</code>. If you use the wrong name, bad things will happen. It is safer to simply never name this argument. (However, in general, arguments after the first should be named. The <code>predict()</code> function is the exception.)</p>
</div>
<div id="data-splitting" class="section level2">
<h2><span class="header-section-number">2.8</span> Data Splitting</h2>
<p>(Note: Many readers will have possibly seen some machine learning previously. <strong>For now, please pretend that you have never heard of or seen cross-validation</strong>. Cross-validation will clutter the initial introduction of many concepts. We will return to and formalize it later.)</p>
<p>OK. So now we can fit models, and make predictions (create estimates of the conditional mean of <span class="math inline">\(Y\)</span> given values of the features), how do we evaluate how well our models perform, <strong>without</strong> knowing the true model!</p>
<p>First, let’s state a somewhat specific goal. We would like to train models that <strong>generalize</strong> well, that is, perform well on “new” or “unseen” data that was <strong>not</strong> used to train the model.</p>
<p>To accomplish this goal, we’ll just “create” a dataset that isn’t used to train the model! To create it, we will just split it off. (We’ll actually do so twice.)</p>
<p>First, denote the <em>entire</em> available data as <span class="math inline">\(\mathcal{D}\)</span>.</p>
<p><span class="math display">\[
\mathcal{D} = \{ (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}, \ i = 1, 2, \ldots n \}
\]</span></p>
<p>We first split this data into a <strong>train</strong> and <strong>test</strong> set. We will discuss these two dataset ad nauseam, but let’s set two rules right now.</p>
<ul>
<li>You can do <strong>whatever</strong> you would like with the training data.
<ul>
<li>However, it is best used to train, evaluate, and select models.</li>
</ul></li>
<li><strong>Do not, ever, for any reason, fit a model using test data!</strong>
<ul>
<li>Additionally, you should not <strong>select</strong> models using test data.</li>
<li>In STAT 432, we will only use test data to provide a final estimate of the generalization error of a chosen model. (Much more on this along the way.)</li>
</ul></li>
</ul>
<p>Again, <strong>do not, ever, for any reason, fit a model using test data!</strong> I repeat: <strong>Do not, ever, for any reason, fit a model using test data!</strong> (You’ve been warned.)</p>
<p>To perform this split, we will <em>randomly</em> select some observations for the train (<code>trn</code>) set, the remainder will be used for the test (<code>tst</code>) set.</p>
<p><span class="math display">\[
\mathcal{D} = \mathcal{D}_{\texttt{trn}} \cup \mathcal{D}_{\texttt{tst}}
\]</span></p>
<p>As a general guiding heuristic, use 80% of the data for training, 20% for testing.</p>
<p>In addition to the train-test split, we will further split the train data into <strong>estimation</strong> and <strong>validation</strong> sets. These are somewhat confusing terms, developed for STAT 432, but hear us out.</p>
<p>To perform this split, we will <em>randomly</em> select some observations (from the train set) for the estimation (<code>est</code>) set, the remainder will be used for the validation (<code>val</code>) set.</p>
<p><span class="math display">\[
\mathcal{D}_{\texttt{trn}} = \mathcal{D}_{\texttt{est}} \cup \mathcal{D}_{\texttt{val}}
\]</span></p>
<p>Again, use 80% of the data for estimation, 20% for validation.</p>
<p>The need for this second split might not become super clear until later on, but the general idea is this:</p>
<ul>
<li>Fit a bunch of candidate models to the <strong>estimation</strong> data. (Think of this as the data to <em>estimate</em> the model parameters. That’s how we chose the name.)</li>
<li>Using these candidate models, evaluate how well they perform using the <strong>validation</strong> data.</li>
<li>After evaluating and picking a single model, re-fit this model to the entire <strong>training</strong> dataset.</li>
<li>Provide an estimate of how well this model performs using the <strong>test</strong> data.</li>
</ul>
<p>Now that we have data for estimation, and validation, we need some <strong>metrics</strong> for evaluating these models.</p>
</div>
<div id="regression-metrics" class="section level2">
<h2><span class="header-section-number">2.9</span> Regression Metrics</h2>
<p>If our goal is to “predict” then we want small errors. In general there are two types of errors we consider:</p>
<ul>
<li>Squared Errors: <span class="math inline">\((y_i - \hat{\mu}(\boldsymbol{x}_i)) ^2\)</span></li>
<li>Absolute Errors: <span class="math inline">\(|y_i - \hat{\mu}(\boldsymbol{x}_i)|\)</span></li>
</ul>
<p>In both cases, we will want to consider the average errors made. We define two metrics.</p>
<p><strong>Root Mean Square Error</strong> (RMSE)</p>
<p><span class="math display">\[
\text{rmse}\left(\hat{f}_{\texttt{set_f}}, \mathcal{D}_{\texttt{set_D}} \right) = \sqrt{\frac{1}{n_{\texttt{set_D}}}\displaystyle\sum_{i \in {\texttt{set_D}}}^{}\left(y_i - \hat{f}_{\texttt{set_f}}({x}_i)\right)^2}
\]</span></p>
<p><strong>Mean Absolute Error</strong> (MAE)</p>
<p><span class="math display">\[
\text{mae}\left(\hat{f}_{\texttt{set_f}}, \mathcal{D}_{\texttt{set_D}} \right) = \frac{1}{n_{\texttt{set_D}}}\displaystyle\sum_{i \in {\texttt{set_D}}}^{}\left|y_i - \hat{f}_{\texttt{set_f}}({x}_i)\right|
\]</span></p>
<ul>
<li><span class="math inline">\(\hat{f}_{\texttt{set_f}}\)</span> is a function <span class="math inline">\(f\)</span> estimated using a model fit to some dataset <span class="math inline">\(\texttt{set_f}\)</span>.</li>
<li>The <span class="math inline">\((x_i, y_i)\)</span> are data from dataset <span class="math inline">\(\mathcal{D}_{\texttt{set_D}}\)</span>.</li>
</ul>
<p>For both, smaller is better. (Less error on average.) In both, we note both the data that the model was fit to, as well as the data the model is evaluated on.</p>
<p>Depending on the data used for these different sets, we “define” different metrics. For example, for RMSE, we have:</p>
<p><strong>Train RMSE</strong>: Evaluate a model fit to estimation data, using estimation data.</p>
<p><span class="math display">\[
\text{RMSE}_{\texttt{trn}} = \text{rmse}\left(\hat{f}_{\texttt{est}}, \mathcal{D}_{\texttt{est}}\right) = \sqrt{\frac{1}{n_{\texttt{est}}}\displaystyle\sum_{i \in {\texttt{est}}}^{}\left(y_i - \hat{f}_{\texttt{est}}({x}_i)\right)^2}
\]</span></p>
<p><strong>Validation RMSE</strong>: Evaluate a model fit to estimation data, using validation data.</p>
<p><span class="math display">\[
\text{RMSE}_{\texttt{val}} = \text{rmse}\left(\hat{f}_{\texttt{est}}, \mathcal{D}_{\texttt{val}}\right) = \sqrt{\frac{1}{n_{\texttt{val}}}\displaystyle\sum_{i \in {\texttt{val}}}^{}\left(y_i - \hat{f}_{\texttt{est}}({x}_i)\right)^2}
\]</span></p>
<p><strong>Test RMSE</strong>: Evaluate a model fit to training data, using test data.</p>
<p><span class="math display">\[
\text{RMSE}_{\texttt{tst}} = \text{rmse}\left(\hat{f}_{\texttt{trn}}, \mathcal{D}_{\texttt{tst}}\right) = \sqrt{\frac{1}{n_{\texttt{tst}}}\displaystyle\sum_{i \in {\texttt{tst}}}^{}\left(y_i - \hat{f}_{\texttt{trn}}({x}_i)\right)^2}
\]</span></p>
<p>For the rest of this chapter, we will largely ignore train error. It’s a bit confusing, since it doesn’t use the full training data! However, think of training error this way: training error evaluates how well a model performs on the data used to fit the model. (This is the general concept behind “training error.” Others might simply call the “estimation” set the training set. We use “estimation” so that we can reserve “train” for the full training dataset, not just the subset use to initially fit the model.)</p>
<p>Let’s return to the <code>sim_mlr_data</code> data and apply these splits and metrics to this data.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb31-1" data-line-number="1"><span class="co"># test-train split</span></a>
<a class="sourceLine" id="cb31-2" data-line-number="2">mlr_trn_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(sim_mlr_data), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(sim_mlr_data))</a>
<a class="sourceLine" id="cb31-3" data-line-number="3">mlr_trn =<span class="st"> </span>sim_mlr_data[mlr_trn_idx, ]</a>
<a class="sourceLine" id="cb31-4" data-line-number="4">mlr_tst =<span class="st"> </span>sim_mlr_data[<span class="op">-</span>mlr_trn_idx, ]</a></code></pre></div>
<p>Here we randomly select 80% of the rows of the full data, and store these indices as <code>mlr_trn_idx</code>. We then create the <code>mlr_trn</code> and <code>mlr_tst</code> datasets by either selecting or anti-selecting these rows from the original dataset.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1"><span class="co"># estimation-validation split</span></a>
<a class="sourceLine" id="cb32-2" data-line-number="2">mlr_est_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(mlr_trn), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(mlr_trn))</a>
<a class="sourceLine" id="cb32-3" data-line-number="3">mlr_est =<span class="st"> </span>mlr_trn[mlr_est_idx, ]</a>
<a class="sourceLine" id="cb32-4" data-line-number="4">mlr_val =<span class="st"> </span>mlr_trn[<span class="op">-</span>mlr_est_idx, ]</a></code></pre></div>
<p>We then repeat the process from above within the train data.</p>
<p>Now, let’s compare <code>mod_3</code> and <code>mod_4</code>. To do so, we first fit both models to the estimation data.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" data-line-number="1">mod_<span class="dv">3</span>_est =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> mlr_est)</a>
<a class="sourceLine" id="cb33-2" data-line-number="2">mod_<span class="dv">4</span>_est =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">sin</span>(x2)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(x3 <span class="op">^</span><span class="st"> </span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>x4, <span class="dt">data =</span> mlr_est)</a></code></pre></div>
<p>We then calculate the validation error for both. Because we will do it so often, we go ahead and write a function to calculate RMSE, given vectors of the actual values (from the data used to evaluate) and the predictions from the model.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1">calc_rmse =<span class="st"> </span><span class="cf">function</span>(actual, predicted) {</a>
<a class="sourceLine" id="cb34-2" data-line-number="2">  <span class="kw">sqrt</span>(<span class="kw">mean</span>((actual <span class="op">-</span><span class="st"> </span>predicted) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb34-3" data-line-number="3">}</a></code></pre></div>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb35-1" data-line-number="1"><span class="co"># calculate validation RMSE, model 3</span></a>
<a class="sourceLine" id="cb35-2" data-line-number="2"><span class="kw">calc_rmse</span>(<span class="dt">actual =</span> mlr_val<span class="op">$</span>y,</a>
<a class="sourceLine" id="cb35-3" data-line-number="3">          <span class="dt">predicted =</span> <span class="kw">predict</span>(mod_<span class="dv">3</span>_est, mlr_val))</a></code></pre></div>
<pre><code>## [1] 0.5788282</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1"><span class="co"># calculate validation RMSE, model 4</span></a>
<a class="sourceLine" id="cb37-2" data-line-number="2"><span class="kw">calc_rmse</span>(<span class="dt">actual =</span> mlr_val<span class="op">$</span>y,</a>
<a class="sourceLine" id="cb37-3" data-line-number="3">          <span class="dt">predicted =</span> <span class="kw">predict</span>(mod_<span class="dv">4</span>_est, mlr_val))</a></code></pre></div>
<pre><code>## [1] 0.5452852</code></pre>
<p>Here we see that <code>mod_4_est</code> achieves a lower validation error, so we move forward with this model. We then refit to the full train data, then evaluate on test.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1">mod_<span class="dv">4</span>_trn =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(<span class="kw">sin</span>(x2)) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(x3 <span class="op">^</span><span class="st"> </span><span class="dv">3</span>) <span class="op">+</span><span class="st"> </span>x4, <span class="dt">data =</span> mlr_trn)</a></code></pre></div>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1"><span class="co"># calculate test RMSE, model 4</span></a>
<a class="sourceLine" id="cb40-2" data-line-number="2"><span class="kw">calc_rmse</span>(<span class="dt">actual =</span> mlr_tst<span class="op">$</span>y,</a>
<a class="sourceLine" id="cb40-3" data-line-number="3">          <span class="dt">predicted =</span> <span class="kw">predict</span>(mod_<span class="dv">4</span>_trn, mlr_tst))</a></code></pre></div>
<pre><code>## [1] 0.538057</code></pre>
<p>We ignore the validation metrics. (We already used them for selecting a model.) This test RMSE is our estimate of how well our selected model will perform on unseen data, on average (in a squared error sense).</p>
<p>Note that for selecting a model there is no difference between MSE and RMSE, but for the sake of understanding, RMSE has preferential units, the same units as the response variables. (Whereas MSE has units squared.) We will always report RMSE.</p>
<div id="graphical-evaluation" class="section level3">
<h3><span class="header-section-number">2.9.1</span> Graphical Evaluation</h3>
<p>In addition to numeric evaluations, we can evaluate a regression model graphically, in particular with a <strong>predicted versus actual</strong> plot.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1"><span class="kw">plot</span>(</a>
<a class="sourceLine" id="cb42-2" data-line-number="2">  <span class="dt">x =</span> mlr_tst<span class="op">$</span>y,</a>
<a class="sourceLine" id="cb42-3" data-line-number="3">  <span class="dt">y =</span> <span class="kw">predict</span>(mod_<span class="dv">4</span>_trn, mlr_tst),</a>
<a class="sourceLine" id="cb42-4" data-line-number="4">  <span class="dt">pch =</span> <span class="dv">20</span>, <span class="dt">col =</span> <span class="st">&quot;darkgrey&quot;</span>,</a>
<a class="sourceLine" id="cb42-5" data-line-number="5">  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">10</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb42-6" data-line-number="6">  <span class="dt">main =</span> <span class="st">&quot;Predicted vs Actual, Model 4, Test Data&quot;</span>,</a>
<a class="sourceLine" id="cb42-7" data-line-number="7">  <span class="dt">xlab =</span> <span class="st">&quot;Actual&quot;</span>,</a>
<a class="sourceLine" id="cb42-8" data-line-number="8">  <span class="dt">ylab =</span> <span class="st">&quot;Predicted&quot;</span></a>
<a class="sourceLine" id="cb42-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb42-10" data-line-number="10"><span class="kw">abline</span>(<span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">b =</span> <span class="dv">1</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb42-11" data-line-number="11"><span class="kw">grid</span>()</a></code></pre></div>
<p><img src="linear-regression_files/figure-html/unnamed-chunk-34-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The closer to the line the better. Also, the less of a pattern the better. In other words, this plot will help diagnose if our model is making similar sized errors for all predictions, or if there are systematic differences. It can also help identify large errors. Sometimes, errors can be on average small, but include some huge errors. In some settings, this may be extremely undiserable.</p>
<p>This might get you thinking about “checking the assumptions” of a linear model. Assessing things like: normality, constant variance, etc. Note that while these are nice things to have, we aren’t really concerned with these things. If we care how well our model <em>predicts</em>, then we will directly evaluate how well it predicts. Least squares is least squares. It minimizes errors. It doesn’t care about model assumptions.</p>
<hr />
</div>
</div>
<div id="example-simple-simulated-data" class="section level2">
<h2><span class="header-section-number">2.10</span> Example: “Simple” Simulated Data</h2>
<p>Let’s return to our initial dataset with a single feature <span class="math inline">\(x\)</span>. This time we’ll generate more data, and then split it.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1">cubic_mean =<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb43-2" data-line-number="2">  <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">5</span> <span class="op">*</span><span class="st"> </span>x <span class="op">^</span><span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb43-3" data-line-number="3">}</a></code></pre></div>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" data-line-number="1">gen_slr_data =<span class="st"> </span><span class="cf">function</span>(<span class="dt">sample_size =</span> <span class="dv">100</span>, mu) {</a>
<a class="sourceLine" id="cb44-2" data-line-number="2">  x =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> sample_size, <span class="dt">min =</span> <span class="dv">-1</span>, <span class="dt">max =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb44-3" data-line-number="3">  y =<span class="st"> </span><span class="kw">mu</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> sample_size)</a>
<a class="sourceLine" id="cb44-4" data-line-number="4">  <span class="kw">tibble</span>(x, y)</a>
<a class="sourceLine" id="cb44-5" data-line-number="5">}</a></code></pre></div>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb45-2" data-line-number="2">sim_slr_data =<span class="st"> </span><span class="kw">gen_slr_data</span>(<span class="dt">sample_size =</span> <span class="dv">100</span>, <span class="dt">mu =</span> cubic_mean)</a></code></pre></div>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1"><span class="co"># test-train split</span></a>
<a class="sourceLine" id="cb46-2" data-line-number="2">slr_trn_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(sim_slr_data), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(sim_slr_data))</a>
<a class="sourceLine" id="cb46-3" data-line-number="3">slr_trn =<span class="st"> </span>sim_slr_data[slr_trn_idx, ]</a>
<a class="sourceLine" id="cb46-4" data-line-number="4">slr_tst =<span class="st"> </span>sim_slr_data[<span class="op">-</span>slr_trn_idx, ]</a></code></pre></div>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" data-line-number="1"><span class="co"># estimation-validation split</span></a>
<a class="sourceLine" id="cb47-2" data-line-number="2">slr_est_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(slr_trn), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(slr_trn))</a>
<a class="sourceLine" id="cb47-3" data-line-number="3">slr_est =<span class="st"> </span>slr_trn[slr_est_idx, ]</a>
<a class="sourceLine" id="cb47-4" data-line-number="4">slr_val =<span class="st"> </span>slr_trn[<span class="op">-</span>slr_est_idx, ]</a></code></pre></div>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" data-line-number="1"><span class="co"># check data</span></a>
<a class="sourceLine" id="cb48-2" data-line-number="2"><span class="kw">head</span>(slr_trn, <span class="dt">n =</span> <span class="dv">10</span>)</a></code></pre></div>
<pre><code>## # A tibble: 10 x 2
##         x      y
##     &lt;dbl&gt;  &lt;dbl&gt;
##  1  0.573 -1.18 
##  2  0.807  0.576
##  3  0.272 -0.973
##  4 -0.813 -1.78 
##  5 -0.161  0.833
##  6  0.736  1.07 
##  7 -0.242  2.97 
##  8  0.520 -1.64 
##  9 -0.664  0.269
## 10 -0.777 -2.02</code></pre>
<p>This time let’s evaluate nine different models. Polynomial models from degree 1 to 9. We fit each model to the estimation data, and store the results in a list.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" data-line-number="1">poly_mod_est_list =<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb50-2" data-line-number="2">  <span class="dt">poly_mod_1_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">1</span>), <span class="dt">data =</span> slr_est),</a>
<a class="sourceLine" id="cb50-3" data-line-number="3">  <span class="dt">poly_mod_2_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">2</span>), <span class="dt">data =</span> slr_est),</a>
<a class="sourceLine" id="cb50-4" data-line-number="4">  <span class="dt">poly_mod_3_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">3</span>), <span class="dt">data =</span> slr_est),</a>
<a class="sourceLine" id="cb50-5" data-line-number="5">  <span class="dt">poly_mod_4_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">4</span>), <span class="dt">data =</span> slr_est),</a>
<a class="sourceLine" id="cb50-6" data-line-number="6">  <span class="dt">poly_mod_5_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">5</span>), <span class="dt">data =</span> slr_est),</a>
<a class="sourceLine" id="cb50-7" data-line-number="7">  <span class="dt">poly_mod_6_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">6</span>), <span class="dt">data =</span> slr_est),</a>
<a class="sourceLine" id="cb50-8" data-line-number="8">  <span class="dt">poly_mod_7_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">7</span>), <span class="dt">data =</span> slr_est),</a>
<a class="sourceLine" id="cb50-9" data-line-number="9">  <span class="dt">poly_mod_8_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">8</span>), <span class="dt">data =</span> slr_est),</a>
<a class="sourceLine" id="cb50-10" data-line-number="10">  <span class="dt">poly_mod_9_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">9</span>), <span class="dt">data =</span> slr_est)</a>
<a class="sourceLine" id="cb50-11" data-line-number="11">)</a></code></pre></div>
<p>So, for example, to access the third model, we would use</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1">poly_mod_est_list[[<span class="dv">3</span>]]</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ poly(x, degree = 3), data = slr_est)
## 
## Coefficients:
##          (Intercept)  poly(x, degree = 3)1  poly(x, degree = 3)2  
##              -0.2058                5.3030               -7.4306  
## poly(x, degree = 3)3  
##               6.7638</code></pre>
<p>But let’s back up. That code was terrible to write. Too much repeated code. Consider the following code</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" data-line-number="1">poly_mod_est_list =<span class="st"> </span><span class="kw">map</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">9</span>, <span class="op">~</span><span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> .x), <span class="dt">data =</span> slr_est))</a></code></pre></div>
<p>This accomplishes the same task, but is much cleaner!</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb54-1" data-line-number="1">poly_mod_est_list[[<span class="dv">3</span>]]</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ poly(x, degree = .x), data = slr_est)
## 
## Coefficients:
##           (Intercept)  poly(x, degree = .x)1  poly(x, degree = .x)2  
##               -0.2058                 5.3030                -7.4306  
## poly(x, degree = .x)3  
##                6.7638</code></pre>
<p>Here we are using the <code>map()</code> function from the <code>purrr</code> package. The <code>~</code> here is used to create a function in place. We’ll consider another way to make it a bit clearer, that is, without writing the function within <code>map()</code>.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" data-line-number="1">fit_poly_mod_to_est_data =<span class="st"> </span><span class="cf">function</span>(d) {</a>
<a class="sourceLine" id="cb56-2" data-line-number="2">  <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> d), <span class="dt">data =</span> slr_est)</a>
<a class="sourceLine" id="cb56-3" data-line-number="3">}</a></code></pre></div>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1">poly_mod_est_list =<span class="st"> </span><span class="kw">map</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">9</span>, fit_poly_mod_to_est_data)</a></code></pre></div>
<p>Again, the same thing.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" data-line-number="1">poly_mod_est_list[[<span class="dv">3</span>]]</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ poly(x, degree = d), data = slr_est)
## 
## Coefficients:
##          (Intercept)  poly(x, degree = d)1  poly(x, degree = d)2  
##              -0.2058                5.3030               -7.4306  
## poly(x, degree = d)3  
##               6.7638</code></pre>
<p>We’ll continue to use this <code>map()</code> function throughout. We’ll explain more and more as we go. Note that the <code>map()</code> function returns a list. The following makes predictions for each of the models, once using the estimation data, once using validation.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb60-1" data-line-number="1">poly_mod_est_pred =<span class="st"> </span><span class="kw">map</span>(poly_mod_est_list, predict, slr_est)</a>
<a class="sourceLine" id="cb60-2" data-line-number="2">poly_mod_val_pred =<span class="st"> </span><span class="kw">map</span>(poly_mod_est_list, predict, slr_val)</a></code></pre></div>
<p>If instead we wanted to return a numeric vector, we would use, <code>map_dbl()</code>. Let’s use this to calculate train and validation RMSE.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" data-line-number="1"><span class="co"># calculate train RMSE</span></a>
<a class="sourceLine" id="cb61-2" data-line-number="2">slr_est_rmse =<span class="st"> </span><span class="kw">map_dbl</span>(poly_mod_est_pred, calc_rmse, <span class="dt">actual =</span> slr_est<span class="op">$</span>y) </a>
<a class="sourceLine" id="cb61-3" data-line-number="3"><span class="co"># calculate validation RMSE</span></a>
<a class="sourceLine" id="cb61-4" data-line-number="4">slr_val_rmse =<span class="st"> </span><span class="kw">map_dbl</span>(poly_mod_val_pred, calc_rmse, <span class="dt">actual =</span> slr_val<span class="op">$</span>y) </a></code></pre></div>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1">slr_est_rmse</a></code></pre></div>
<pre><code>## [1] 1.5748180 1.2717458 0.9500069 0.9480786 0.9302359 0.9187948 0.9151668
## [8] 0.9120942 0.9117093</code></pre>
<p>Note that training error goes down as degree goes up. More on this next chapter.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1">slr_val_rmse</a></code></pre></div>
<pre><code>## [1] 1.6584930 1.2791685 0.9574010 0.9729928 1.0104449 1.0505615 1.0617693
## [8] 1.0953461 1.0968283</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" data-line-number="1"><span class="kw">which.min</span>(slr_val_rmse)</a></code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>The model with polynomial degree 3 has the lowest validation error, so we move forward with this model. We re-fit to the full train dataset, then evaluate on the test set one last time.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1">poly_mod_<span class="dv">3</span>_trn =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">3</span>), <span class="dt">data =</span> slr_trn)</a></code></pre></div>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" data-line-number="1"><span class="kw">calc_rmse</span>(<span class="dt">actual =</span> slr_tst<span class="op">$</span>y,</a>
<a class="sourceLine" id="cb69-2" data-line-number="2">          <span class="dt">predicted =</span> <span class="kw">predict</span>(poly_mod_<span class="dv">3</span>_trn, slr_tst))</a></code></pre></div>
<pre><code>## [1] 0.7198306</code></pre>
<p>Note: There are hints here that this process is a bit unstable. See if you can figure out why. Hint: See what happens when you change the seed to generate or split the data. We’ll return to this issue when we introduce cross-validation, but for now, we’ll pretend we didn’t notice.</p>
<p>We’ll round out this chapter with three “real” data examples.</p>
<hr />
</div>
<div id="example-diamonds-data" class="section level2">
<h2><span class="header-section-number">2.11</span> Example: Diamonds Data</h2>
<p>For this example, we use (a subset of) the <code>diamonds</code> data from the <code>ggplot2</code> package.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" data-line-number="1"><span class="co"># load (subset of) data</span></a>
<a class="sourceLine" id="cb71-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">42</span>)</a>
<a class="sourceLine" id="cb71-3" data-line-number="3">dmnd =<span class="st"> </span>ggplot2<span class="op">::</span>diamonds[<span class="kw">sample</span>(<span class="kw">nrow</span>(diamonds), <span class="dt">size =</span> <span class="dv">5000</span>), ]</a></code></pre></div>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1"><span class="co"># data prep</span></a>
<a class="sourceLine" id="cb72-2" data-line-number="2">dmnd =<span class="st"> </span>dmnd <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb72-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cut =</span> <span class="kw">factor</span>(cut, <span class="dt">ordered =</span> <span class="ot">FALSE</span>),</a>
<a class="sourceLine" id="cb72-4" data-line-number="4">         <span class="dt">color =</span> <span class="kw">factor</span>(color, <span class="dt">ordered =</span> <span class="ot">FALSE</span>),</a>
<a class="sourceLine" id="cb72-5" data-line-number="5">         <span class="dt">clarity =</span> <span class="kw">factor</span>(clarity, <span class="dt">ordered =</span> <span class="ot">FALSE</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb72-6" data-line-number="6"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>price, <span class="kw">everything</span>())</a></code></pre></div>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" data-line-number="1"><span class="co"># test-train split</span></a>
<a class="sourceLine" id="cb73-2" data-line-number="2">dmnd_trn_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(dmnd), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(dmnd))</a>
<a class="sourceLine" id="cb73-3" data-line-number="3">dmnd_trn =<span class="st"> </span>dmnd[dmnd_trn_idx, ]</a>
<a class="sourceLine" id="cb73-4" data-line-number="4">dmnd_tst =<span class="st"> </span>dmnd[<span class="op">-</span>dmnd_trn_idx, ]</a></code></pre></div>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1"><span class="co"># estimation-validation split</span></a>
<a class="sourceLine" id="cb74-2" data-line-number="2">dmnd_est_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(dmnd_trn), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(dmnd_trn))</a>
<a class="sourceLine" id="cb74-3" data-line-number="3">dmnd_est =<span class="st"> </span>dmnd_trn[dmnd_est_idx, ]</a>
<a class="sourceLine" id="cb74-4" data-line-number="4">dmnd_val =<span class="st"> </span>dmnd_trn[<span class="op">-</span>dmnd_est_idx, ]</a></code></pre></div>
<p>The code above loads the data, then performs a test-train split, then additionally an estimation-validation split. We then look at the <strong>train</strong> data. That is we do not even look at the test data.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" data-line-number="1"><span class="co"># check data</span></a>
<a class="sourceLine" id="cb75-2" data-line-number="2"><span class="kw">head</span>(dmnd_trn, <span class="dt">n =</span> <span class="dv">10</span>)</a></code></pre></div>
<pre><code>## # A tibble: 10 x 10
##    carat cut       color clarity depth table     x     y     z price
##    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;
##  1 0.5   Premium   H     SI1      59    59    5.22  5.18  3.07  1156
##  2 1.01  Ideal     G     SI2      63.2  57    6.33  6.28  3.99  4038
##  3 0.62  Very Good D     SI1      61.3  58    5.47  5.49  3.36  1949
##  4 0.41  Ideal     D     VS2      62.4  54    4.78  4.74  2.97  1076
##  5 0.31  Ideal     G     IF       61.6  54    4.36  4.4   2.7    853
##  6 1.08  Ideal     I     SI1      62.6  53.9  6.51  6.56  4.09  5049
##  7 0.52  Very Good G     VS2      62.4  60    5.14  5.18  3.22  1423
##  8 1.01  Premium   F     SI2      60.9  60    6.45  6.42  3.91  3297
##  9 0.570 Ideal     H     VS1      61.7  54    5.33  5.36  3.3   1554
## 10 0.34  Ideal     H     VS2      62.5  54    4.54  4.49  2.82   689</code></pre>
<p>Our goal here will be to build a model to predict the <code>price</code> of a diamond given it’s characteristics. Let’s create a few EDA plots.</p>
<p><img src="linear-regression_files/figure-html/unnamed-chunk-60-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p><img src="linear-regression_files/figure-html/unnamed-chunk-61-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>Note that these plots do <strong>not</strong> contain the test data. If they did, we would be using the test data to influence model building and selection, a big no-no.</p>
<p>Let’s consider four possible models, each of which we fit to the estimation data.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1">dmnd_mod_<span class="dv">1</span>_est =<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span>carat, <span class="dt">data =</span> dmnd_est)</a>
<a class="sourceLine" id="cb77-2" data-line-number="2">dmnd_mod_<span class="dv">2</span>_est =<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span>carat <span class="op">+</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y <span class="op">+</span><span class="st"> </span>z, <span class="dt">data =</span> dmnd_est)</a>
<a class="sourceLine" id="cb77-3" data-line-number="3">dmnd_mod_<span class="dv">3</span>_est =<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(carat, <span class="dt">degree =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y <span class="op">+</span><span class="st"> </span>z, <span class="dt">data =</span> dmnd_est)</a>
<a class="sourceLine" id="cb77-4" data-line-number="4">dmnd_mod_<span class="dv">4</span>_est =<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(carat, <span class="dt">degree =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>carat, <span class="dt">data =</span> dmnd_est)</a></code></pre></div>
<p>Now, let’s calculate the validation RMSE of each.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1">dmnd_mod_list =<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb78-2" data-line-number="2">  dmnd_mod_<span class="dv">1</span>_est,</a>
<a class="sourceLine" id="cb78-3" data-line-number="3">  dmnd_mod_<span class="dv">2</span>_est,</a>
<a class="sourceLine" id="cb78-4" data-line-number="4">  dmnd_mod_<span class="dv">3</span>_est,</a>
<a class="sourceLine" id="cb78-5" data-line-number="5">  dmnd_mod_<span class="dv">4</span>_est</a>
<a class="sourceLine" id="cb78-6" data-line-number="6">)</a></code></pre></div>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" data-line-number="1">dmnd_mod_val_pred =<span class="st"> </span><span class="kw">map</span>(dmnd_mod_list, predict, dmnd_val)</a>
<a class="sourceLine" id="cb79-2" data-line-number="2"><span class="kw">map_dbl</span>(dmnd_mod_val_pred, calc_rmse, <span class="dt">actual =</span> dmnd_val<span class="op">$</span>price) </a></code></pre></div>
<pre><code>## [1] 1583.558 1517.080 1634.396 1350.659</code></pre>
<p>It looks like model <code>dmnd_mod_4_est</code> achieves the lowest validation error. We re-fit this model, then report the test RMSE.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" data-line-number="1">dmnd_mod_<span class="dv">4</span>_trn =<span class="st"> </span><span class="kw">lm</span>(price <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(carat, <span class="dt">degree =</span> <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span>. <span class="op">-</span><span class="st"> </span>carat, <span class="dt">data =</span> dmnd_trn)</a>
<a class="sourceLine" id="cb81-2" data-line-number="2"><span class="kw">calc_rmse</span>(<span class="dt">actual =</span> dmnd_tst<span class="op">$</span>price,</a>
<a class="sourceLine" id="cb81-3" data-line-number="3">          <span class="dt">predicted =</span> <span class="kw">predict</span>(dmnd_mod_<span class="dv">4</span>_trn, dmnd_tst))</a></code></pre></div>
<pre><code>## [1] 1094.916</code></pre>
<p>So, on average, this model is “wrong” by about $1000 dollars. However, less-so when it is a low cost diamond, more so with high priced diamonds, as we can see in the plot below.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1"><span class="kw">plot</span>(</a>
<a class="sourceLine" id="cb83-2" data-line-number="2">  <span class="dt">x =</span> dmnd_tst<span class="op">$</span>price,</a>
<a class="sourceLine" id="cb83-3" data-line-number="3">  <span class="dt">y =</span> <span class="kw">predict</span>(dmnd_mod_<span class="dv">4</span>_trn, dmnd_tst),</a>
<a class="sourceLine" id="cb83-4" data-line-number="4">  <span class="dt">pch =</span> <span class="dv">20</span>, <span class="dt">col =</span> <span class="st">&quot;darkgrey&quot;</span>,</a>
<a class="sourceLine" id="cb83-5" data-line-number="5">  <span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">25000</span>), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">25000</span>),</a>
<a class="sourceLine" id="cb83-6" data-line-number="6">  <span class="dt">main =</span> <span class="st">&quot;Diamonds: Predicted vs Actual, Model 4, Test Data&quot;</span>,</a>
<a class="sourceLine" id="cb83-7" data-line-number="7">  <span class="dt">xlab =</span> <span class="st">&quot;Actual&quot;</span>,</a>
<a class="sourceLine" id="cb83-8" data-line-number="8">  <span class="dt">ylab =</span> <span class="st">&quot;Predicted&quot;</span></a>
<a class="sourceLine" id="cb83-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb83-10" data-line-number="10"><span class="kw">abline</span>(<span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">b =</span> <span class="dv">1</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb83-11" data-line-number="11"><span class="kw">grid</span>()</a></code></pre></div>
<p><img src="linear-regression_files/figure-html/unnamed-chunk-66-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Some things to consider:</p>
<ul>
<li>Could you use the predicted versus actual plot to assist in selecting a model with the validation data?</li>
<li>Note that the model we have chosen is not necessarily the “best” model. It is simply the model with the lowest validation RMSE. This is currently a very simplistic analysis.</li>
<li>Can you improve this model? Would a log transform of price help?</li>
</ul>
<hr />
</div>
<div id="example-credit-card-data" class="section level2">
<h2><span class="header-section-number">2.12</span> Example: Credit Card Data</h2>
<p>Suppose you work for a small local bank, perhaps a credit union, that has a credit card product offering. For years, you relied on credit agencies to provide a rating of your customer’s credit, however, this costs your bank money. One day, you realize that it might be possible to reverse engineer your customers’ (and thus potential customers) credit rating based on the credit ratings that you have already purchased, as well as the demographic and credit card information that you already have, such as age, education level, credit limit, etc. (We make no comment on the legality or ethics of this idea. Consider these before using at your own risk.)</p>
<p>So long as you can estimate customers’ credit ratings with a reasonable error, you could stop buying the ratings from an outside agency. Effectively, you will have created your own rating.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1"><span class="co"># load data, coerce to tibble</span></a>
<a class="sourceLine" id="cb84-2" data-line-number="2">crdt =<span class="st"> </span><span class="kw">as_tibble</span>(ISLR<span class="op">::</span>Credit)</a></code></pre></div>
<p>To perform this analysis, we will use the <code>Credit</code> data form the <code>ISLR</code> package. Note: <strong>this is not real data.</strong> It has been simulated.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1"><span class="co"># data prep</span></a>
<a class="sourceLine" id="cb85-2" data-line-number="2">crdt =<span class="st"> </span>crdt <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb85-3" data-line-number="3"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>ID) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb85-4" data-line-number="4"><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>Rating, <span class="kw">everything</span>())</a></code></pre></div>
<p>We remove the <code>ID</code> variable as it should have no predictive power. We also move the <code>Rating</code> variable to the last column with a clever <code>dplyr</code> trick. This is in no way necessary, but is useful in creating some plots.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1"><span class="co"># test-train split</span></a>
<a class="sourceLine" id="cb86-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb86-3" data-line-number="3">crdt_trn_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(crdt), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(crdt))</a>
<a class="sourceLine" id="cb86-4" data-line-number="4">crdt_trn =<span class="st"> </span>crdt[crdt_trn_idx, ]</a>
<a class="sourceLine" id="cb86-5" data-line-number="5">crdt_tst =<span class="st"> </span>crdt[<span class="op">-</span>crdt_trn_idx, ]</a></code></pre></div>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" data-line-number="1"><span class="co"># estimation-validation split</span></a>
<a class="sourceLine" id="cb87-2" data-line-number="2">crdt_est_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(crdt_trn), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(crdt_trn))</a>
<a class="sourceLine" id="cb87-3" data-line-number="3">crdt_est =<span class="st"> </span>crdt_trn[crdt_est_idx, ]</a>
<a class="sourceLine" id="cb87-4" data-line-number="4">crdt_val =<span class="st"> </span>crdt_trn[<span class="op">-</span>crdt_est_idx, ]</a></code></pre></div>
<p>After train-test and estimation-validation splitting the data, we look at the train data.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1"><span class="co"># check data</span></a>
<a class="sourceLine" id="cb88-2" data-line-number="2"><span class="kw">head</span>(crdt_trn, <span class="dt">n =</span> <span class="dv">10</span>)</a></code></pre></div>
<pre><code>## # A tibble: 10 x 11
##    Income Limit Cards   Age Education Gender Student Married Ethnicity Balance
##     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;       &lt;int&gt;
##  1  183.  13913     4    98        17 &quot; Mal… No      Yes     Caucasian    1999
##  2   35.7  2880     2    35        15 &quot; Mal… No      No      African …       0
##  3  123.   8376     2    89        17 &quot; Mal… Yes     No      African …    1259
##  4   20.8  2672     1    70        18 &quot;Fema… No      No      African …       0
##  5   39.1  5565     4    48        18 &quot;Fema… No      Yes     Caucasian     772
##  6   36.5  3806     2    52        13 &quot; Mal… No      No      African …     188
##  7   45.1  3762     3    80         8 &quot; Mal… No      Yes     Caucasian      70
##  8   43.5  2906     4    69        11 &quot; Mal… No      No      Caucasian       0
##  9   23.1  3476     2    50        15 &quot;Fema… No      No      Caucasian     209
## 10   53.2  4943     2    46        16 &quot;Fema… No      Yes     Asian         382
## # … with 1 more variable: Rating &lt;int&gt;</code></pre>
<p>To get a better “look” at the data, consider running the following:</p>
<ul>
<li><code>skimr::skim(crdt_trn)</code></li>
<li><code>str(crdt_trn)</code></li>
<li><code>View(crdt_trn)</code></li>
</ul>
<p>We also create a pairs plot.</p>
<p><img src="linear-regression_files/figure-html/unnamed-chunk-72-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>We immediately notice three variables that have a strong correlation with <code>Rating</code>: <code>Income</code>, <code>Limit</code>, and <code>Balance</code>. Based on this, we evaluate five candidate models.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1">crdt_mod_list =<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb90-2" data-line-number="2">  <span class="dt">crdt_mod_0_est =</span> <span class="kw">lm</span>(Rating <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> crdt_est),</a>
<a class="sourceLine" id="cb90-3" data-line-number="3">  <span class="dt">crdt_mod_1_est =</span> <span class="kw">lm</span>(Rating <span class="op">~</span><span class="st"> </span>Limit, <span class="dt">data =</span> crdt_est),</a>
<a class="sourceLine" id="cb90-4" data-line-number="4">  <span class="dt">crdt_mod_2_est =</span> <span class="kw">lm</span>(Rating <span class="op">~</span><span class="st"> </span>Limit <span class="op">+</span><span class="st"> </span>Income <span class="op">+</span><span class="st"> </span>Balance, <span class="dt">data =</span> crdt_est),</a>
<a class="sourceLine" id="cb90-5" data-line-number="5">  <span class="dt">crdt_mod_3_est =</span> <span class="kw">lm</span>(Rating <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> crdt_est),</a>
<a class="sourceLine" id="cb90-6" data-line-number="6">  <span class="dt">crdt_mod_4_est =</span> <span class="kw">step</span>(<span class="kw">lm</span>(Rating <span class="op">~</span><span class="st"> </span>. <span class="op">^</span><span class="st"> </span><span class="dv">2</span>, <span class="dt">data =</span> crdt_est), <span class="dt">trace =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb90-7" data-line-number="7">)</a></code></pre></div>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1">crdt_mod_val_pred =<span class="st"> </span><span class="kw">map</span>(crdt_mod_list, predict, crdt_val)</a>
<a class="sourceLine" id="cb91-2" data-line-number="2"><span class="kw">map_dbl</span>(crdt_mod_val_pred, calc_rmse, <span class="dt">actual =</span> crdt_val<span class="op">$</span>Rating) </a></code></pre></div>
<pre><code>## crdt_mod_0_est crdt_mod_1_est crdt_mod_2_est crdt_mod_3_est crdt_mod_4_est 
##     140.080591      12.244099      12.333767       9.890607      11.575484</code></pre>
<p>From these results, it appears that the additive model, including all terms performs best. We move forward with this model.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1">final_credit_model =<span class="st"> </span><span class="kw">lm</span>(Rating <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> crdt_trn)</a>
<a class="sourceLine" id="cb93-2" data-line-number="2"><span class="kw">sqrt</span>(<span class="kw">mean</span>((<span class="kw">predict</span>(final_credit_model, crdt_tst) <span class="op">-</span><span class="st"> </span>crdt_tst<span class="op">$</span>Rating) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] 10.47727</code></pre>
<p>It seems that on average, this model errors by about 10 credit points.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" data-line-number="1"><span class="kw">range</span>(crdt_trn<span class="op">$</span>Rating)</a></code></pre></div>
<pre><code>## [1]  93 982</code></pre>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" data-line-number="1"><span class="kw">sd</span>(crdt_trn<span class="op">$</span>Rating)</a></code></pre></div>
<pre><code>## [1] 157.5897</code></pre>
<p>Given the range of possible ratings, this seem pretty good! What do you think?</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1"><span class="kw">plot</span>(</a>
<a class="sourceLine" id="cb99-2" data-line-number="2">  <span class="dt">x =</span> crdt_tst<span class="op">$</span>Rating,</a>
<a class="sourceLine" id="cb99-3" data-line-number="3">  <span class="dt">y =</span> <span class="kw">predict</span>(final_credit_model, crdt_tst),</a>
<a class="sourceLine" id="cb99-4" data-line-number="4">  <span class="dt">pch =</span> <span class="dv">20</span>, <span class="dt">col =</span> <span class="st">&quot;darkgrey&quot;</span>,</a>
<a class="sourceLine" id="cb99-5" data-line-number="5">  <span class="co"># xlim = c(0, 25000), ylim = c(0, 25000),</span></a>
<a class="sourceLine" id="cb99-6" data-line-number="6">  <span class="dt">main =</span> <span class="st">&quot;Credit: Predicted vs Actual, Test Data&quot;</span>,</a>
<a class="sourceLine" id="cb99-7" data-line-number="7">  <span class="dt">xlab =</span> <span class="st">&quot;Actual&quot;</span>,</a>
<a class="sourceLine" id="cb99-8" data-line-number="8">  <span class="dt">ylab =</span> <span class="st">&quot;Predicted&quot;</span></a>
<a class="sourceLine" id="cb99-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb99-10" data-line-number="10"><span class="kw">abline</span>(<span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">b =</span> <span class="dv">1</span>, <span class="dt">lwd =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb99-11" data-line-number="11"><span class="kw">grid</span>()</a></code></pre></div>
<p><img src="linear-regression_files/figure-html/unnamed-chunk-77-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The predicted versus actual plot almost looks too good to be true! Wow! (Oh, wait. This was simulated data…)</p>
<p>In summary, if this data were real, we might have an interesting result!</p>
<p>Do note, that both this example and the previous should not be considered <strong>data analyses</strong>, but instead, examples that reinforce how to use the validation and test sets. As part of a true analysis, we will need to be much more careful about some of our decision. More on this later! Up next: nonparametric regression methods.</p>
<hr />
</div>
<div id="source-1" class="section level2">
<h2><span class="header-section-number">2.13</span> Source</h2>
<ul>
<li><code>R</code> Markdown: <a href="linear-regression.Rmd"><code>linear-regression.Rmd</code></a></li>
</ul>
<hr />

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ml-overview.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nonparametric-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/daviddalpiaz/bsl/edit/master/linear-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
