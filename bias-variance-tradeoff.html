<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 The Bias–Variance Tradeoff | Basics of Statistical Learning</title>
  <meta name="description" content="Chapter 4 The Bias–Variance Tradeoff | Basics of Statistical Learning" />
  <meta name="generator" content="bookdown 0.21.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 The Bias–Variance Tradeoff | Basics of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://statisticallearning.org/" />
  
  
  <meta name="github-repo" content="daviddalpiaz/bsl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 The Bias–Variance Tradeoff | Basics of Statistical Learning" />
  
  
  

<meta name="author" content="David Dalpiaz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="nonparametric-regression.html"/>
<link rel="next" href="regression-overview.html"/>
<script src="libs/header-attrs-2.5.1/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Basics of Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i><b>0.1</b> Who?</a>
<ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#readers"><i class="fa fa-check"></i><b>0.1.1</b> Readers</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#author"><i class="fa fa-check"></i><b>0.1.2</b> Author</a></li>
<li class="chapter" data-level="0.1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>0.1.3</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#what"><i class="fa fa-check"></i><b>0.2</b> What?</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#why"><i class="fa fa-check"></i><b>0.3</b> Why?</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#where"><i class="fa fa-check"></i><b>0.4</b> Where?</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#when"><i class="fa fa-check"></i><b>0.5</b> When?</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#how"><i class="fa fa-check"></i><b>0.6</b> How?</a>
<ul>
<li class="chapter" data-level="0.6.1" data-path="index.html"><a href="index.html#build-tools"><i class="fa fa-check"></i><b>0.6.1</b> Build Tools</a></li>
<li class="chapter" data-level="0.6.2" data-path="index.html"><a href="index.html#active-development"><i class="fa fa-check"></i><b>0.6.2</b> Active Development</a></li>
<li class="chapter" data-level="0.6.3" data-path="index.html"><a href="index.html#packages"><i class="fa fa-check"></i><b>0.6.3</b> Packages</a></li>
<li class="chapter" data-level="0.6.4" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>0.6.4</b> License</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ml-overview.html"><a href="ml-overview.html"><i class="fa fa-check"></i><b>1</b> Machine Learning Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ml-overview.html"><a href="ml-overview.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-tasks"><i class="fa fa-check"></i><b>1.2</b> Machine Learning Tasks</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ml-overview.html"><a href="ml-overview.html#supervised-learning"><i class="fa fa-check"></i><b>1.2.1</b> Supervised Learning</a></li>
<li class="chapter" data-level="1.2.2" data-path="ml-overview.html"><a href="ml-overview.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.2.2</b> Unsupervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ml-overview.html"><a href="ml-overview.html#open-questions"><i class="fa fa-check"></i><b>1.3</b> Open Questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#r-setup-and-source"><i class="fa fa-check"></i><b>2.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#explanation-versus-prediction"><i class="fa fa-check"></i><b>2.2</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#task-setup"><i class="fa fa-check"></i><b>2.3</b> Task Setup</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#mathematical-setup"><i class="fa fa-check"></i><b>2.4</b> Mathematical Setup</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-models"><i class="fa fa-check"></i><b>2.5</b> Linear Regression Models</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#using-lm"><i class="fa fa-check"></i><b>2.6</b> Using <code>lm()</code></a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression.html"><a href="linear-regression.html#the-predict-function"><i class="fa fa-check"></i><b>2.7</b> The <code>predict()</code> Function</a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression.html"><a href="linear-regression.html#data-splitting"><i class="fa fa-check"></i><b>2.8</b> Data Splitting</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression.html"><a href="linear-regression.html#regression-metrics"><i class="fa fa-check"></i><b>2.9</b> Regression Metrics</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="linear-regression.html"><a href="linear-regression.html#graphical-evaluation"><i class="fa fa-check"></i><b>2.9.1</b> Graphical Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="linear-regression.html"><a href="linear-regression.html#example-simple-simulated-data"><i class="fa fa-check"></i><b>2.10</b> Example: “Simple” Simulated Data</a></li>
<li class="chapter" data-level="2.11" data-path="linear-regression.html"><a href="linear-regression.html#example-diamonds-data"><i class="fa fa-check"></i><b>2.11</b> Example: Diamonds Data</a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression.html"><a href="linear-regression.html#example-credit-card-data"><i class="fa fa-check"></i><b>2.12</b> Example: Credit Card Data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html"><i class="fa fa-check"></i><b>3</b> Nonparametric Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#r-setup-and-source-1"><i class="fa fa-check"></i><b>3.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="3.2" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#mathematical-setup-1"><i class="fa fa-check"></i><b>3.2</b> Mathematical Setup</a></li>
<li class="chapter" data-level="3.3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="3.4" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#decision-trees"><i class="fa fa-check"></i><b>3.4</b> Decision Trees</a></li>
<li class="chapter" data-level="3.5" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#example-credit-card-data-1"><i class="fa fa-check"></i><b>3.5</b> Example: Credit Card Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>4</b> The Bias–Variance Tradeoff</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#r-setup-and-source-2"><i class="fa fa-check"></i><b>4.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="4.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#the-regression-setup"><i class="fa fa-check"></i><b>4.2</b> The Regression Setup</a></li>
<li class="chapter" data-level="4.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#reducible-and-irreducible-error"><i class="fa fa-check"></i><b>4.3</b> Reducible and Irreducible Error</a></li>
<li class="chapter" data-level="4.4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#bias-variance-decomposition"><i class="fa fa-check"></i><b>4.4</b> Bias-Variance Decomposition</a></li>
<li class="chapter" data-level="4.5" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#using-simulation-to-estimate-bias-and-variance"><i class="fa fa-check"></i><b>4.5</b> Using Simulation to Estimate Bias and Variance</a></li>
<li class="chapter" data-level="4.6" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#estimating-expected-prediction-error"><i class="fa fa-check"></i><b>4.6</b> Estimating Expected Prediction Error</a></li>
<li class="chapter" data-level="4.7" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#model-flexibility"><i class="fa fa-check"></i><b>4.7</b> Model Flexibility</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#linear-models"><i class="fa fa-check"></i><b>4.7.1</b> Linear Models</a></li>
<li class="chapter" data-level="4.7.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#k-nearest-neighbors-1"><i class="fa fa-check"></i><b>4.7.2</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.7.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#decision-trees-1"><i class="fa fa-check"></i><b>4.7.3</b> Decision Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-overview.html"><a href="regression-overview.html"><i class="fa fa-check"></i><b>5</b> Regression Overview</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regression-overview.html"><a href="regression-overview.html#the-goal"><i class="fa fa-check"></i><b>5.1</b> The Goal</a></li>
<li class="chapter" data-level="5.2" data-path="regression-overview.html"><a href="regression-overview.html#general-strategy"><i class="fa fa-check"></i><b>5.2</b> General Strategy</a></li>
<li class="chapter" data-level="5.3" data-path="regression-overview.html"><a href="regression-overview.html#aglorithms"><i class="fa fa-check"></i><b>5.3</b> Aglorithms</a></li>
<li class="chapter" data-level="5.4" data-path="regression-overview.html"><a href="regression-overview.html#model-flexibility-1"><i class="fa fa-check"></i><b>5.4</b> Model Flexibility</a></li>
<li class="chapter" data-level="5.5" data-path="regression-overview.html"><a href="regression-overview.html#overfitting"><i class="fa fa-check"></i><b>5.5</b> Overfitting</a></li>
<li class="chapter" data-level="5.6" data-path="regression-overview.html"><a href="regression-overview.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.6</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="5.7" data-path="regression-overview.html"><a href="regression-overview.html#no-free-lunch"><i class="fa fa-check"></i><b>5.7</b> No Free Lunch</a></li>
<li class="chapter" data-level="5.8" data-path="regression-overview.html"><a href="regression-overview.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>5.8</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>6</b> Classification</a></li>
<li class="chapter" data-level="7" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html"><i class="fa fa-check"></i><b>7</b> Nonparametric Classification</a></li>
<li class="chapter" data-level="8" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Logistic Regression</a></li>
<li class="chapter" data-level="9" data-path="binary-classification.html"><a href="binary-classification.html"><i class="fa fa-check"></i><b>9</b> Binary Classification</a>
<ul>
<li class="chapter" data-level="9.1" data-path="binary-classification.html"><a href="binary-classification.html#r-setup-and-source-3"><i class="fa fa-check"></i><b>9.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="9.2" data-path="binary-classification.html"><a href="binary-classification.html#breast-cancer-data"><i class="fa fa-check"></i><b>9.2</b> Breast Cancer Data</a></li>
<li class="chapter" data-level="9.3" data-path="binary-classification.html"><a href="binary-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>9.3</b> Confusion Matrix</a></li>
<li class="chapter" data-level="9.4" data-path="binary-classification.html"><a href="binary-classification.html#binary-classification-metrics"><i class="fa fa-check"></i><b>9.4</b> Binary Classification Metrics</a></li>
<li class="chapter" data-level="9.5" data-path="binary-classification.html"><a href="binary-classification.html#probability-cutoff"><i class="fa fa-check"></i><b>9.5</b> Probability Cutoff</a></li>
<li class="chapter" data-level="9.6" data-path="binary-classification.html"><a href="binary-classification.html#r-packages-and-function"><i class="fa fa-check"></i><b>9.6</b> R Packages and Function</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="generative.html"><a href="generative.html"><i class="fa fa-check"></i><b>10</b> Generative Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="generative.html"><a href="generative.html#r-setup-and-source-4"><i class="fa fa-check"></i><b>10.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="10.2" data-path="generative.html"><a href="generative.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>10.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="10.3" data-path="generative.html"><a href="generative.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>10.3</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="10.4" data-path="generative.html"><a href="generative.html#naive-bayes"><i class="fa fa-check"></i><b>10.4</b> Naive Bayes</a></li>
<li class="chapter" data-level="10.5" data-path="generative.html"><a href="generative.html#categorical-features"><i class="fa fa-check"></i><b>10.5</b> Categorical Features</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>11</b> Cross-Validation</a></li>
<li class="chapter" data-level="12" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>12</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.1" data-path="regularization.html"><a href="regularization.html#ridge-regression"><i class="fa fa-check"></i><b>12.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="12.2" data-path="regularization.html"><a href="regularization.html#lasso"><i class="fa fa-check"></i><b>12.2</b> Lasso</a></li>
<li class="chapter" data-level="12.3" data-path="regularization.html"><a href="regularization.html#broom"><i class="fa fa-check"></i><b>12.3</b> <code>broom</code></a></li>
<li class="chapter" data-level="12.4" data-path="regularization.html"><a href="regularization.html#simulated-data-p-n"><i class="fa fa-check"></i><b>12.4</b> Simulated Data, <span class="math inline">\(p &gt; n\)</span></a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="additional-reading.html"><a href="additional-reading.html"><i class="fa fa-check"></i><b>A</b> Additional Reading</a>
<ul>
<li class="chapter" data-level="A.1" data-path="additional-reading.html"><a href="additional-reading.html#books"><i class="fa fa-check"></i><b>A.1</b> Books</a></li>
<li class="chapter" data-level="A.2" data-path="additional-reading.html"><a href="additional-reading.html#papers"><i class="fa fa-check"></i><b>A.2</b> Papers</a></li>
<li class="chapter" data-level="A.3" data-path="additional-reading.html"><a href="additional-reading.html#blog-posts"><i class="fa fa-check"></i><b>A.3</b> Blog Posts</a></li>
<li class="chapter" data-level="A.4" data-path="additional-reading.html"><a href="additional-reading.html#miscellaneous"><i class="fa fa-check"></i><b>A.4</b> Miscellaneous</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="computing.html"><a href="computing.html"><i class="fa fa-check"></i><b>B</b> Computing</a>
<ul>
<li class="chapter" data-level="B.1" data-path="computing.html"><a href="computing.html#reading"><i class="fa fa-check"></i><b>B.1</b> Reading</a></li>
<li class="chapter" data-level="B.2" data-path="computing.html"><a href="computing.html#additional-resources"><i class="fa fa-check"></i><b>B.2</b> Additional Resources</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="computing.html"><a href="computing.html#r"><i class="fa fa-check"></i><b>B.2.1</b> R</a></li>
<li class="chapter" data-level="B.2.2" data-path="computing.html"><a href="computing.html#rstudio"><i class="fa fa-check"></i><b>B.2.2</b> RStudio</a></li>
<li class="chapter" data-level="B.2.3" data-path="computing.html"><a href="computing.html#r-markdown"><i class="fa fa-check"></i><b>B.2.3</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="computing.html"><a href="computing.html#stat-432-idioms"><i class="fa fa-check"></i><b>B.3</b> STAT 432 Idioms</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="computing.html"><a href="computing.html#dont-restore-old-workspaces"><i class="fa fa-check"></i><b>B.3.1</b> Don’t Restore Old Workspaces</a></li>
<li class="chapter" data-level="B.3.2" data-path="computing.html"><a href="computing.html#r-versions"><i class="fa fa-check"></i><b>B.3.2</b> R Versions</a></li>
<li class="chapter" data-level="B.3.3" data-path="computing.html"><a href="computing.html#packages-1"><i class="fa fa-check"></i><b>B.3.3</b> Packages</a></li>
<li class="chapter" data-level="B.3.4" data-path="computing.html"><a href="computing.html#code-style"><i class="fa fa-check"></i><b>B.3.4</b> Code Style</a></li>
<li class="chapter" data-level="B.3.5" data-path="computing.html"><a href="computing.html#reference-style"><i class="fa fa-check"></i><b>B.3.5</b> Reference Style</a></li>
<li class="chapter" data-level="B.3.6" data-path="computing.html"><a href="computing.html#stat-432-r-style-overrides"><i class="fa fa-check"></i><b>B.3.6</b> STAT 432 R Style Overrides</a></li>
<li class="chapter" data-level="B.3.7" data-path="computing.html"><a href="computing.html#stat-432-r-markdown-style"><i class="fa fa-check"></i><b>B.3.7</b> STAT 432 R Markdown Style</a></li>
<li class="chapter" data-level="B.3.8" data-path="computing.html"><a href="computing.html#style-heuristics"><i class="fa fa-check"></i><b>B.3.8</b> Style Heuristics</a></li>
<li class="chapter" data-level="B.3.9" data-path="computing.html"><a href="computing.html#objects-and-functions"><i class="fa fa-check"></i><b>B.3.9</b> Objects and Functions</a></li>
<li class="chapter" data-level="B.3.10" data-path="computing.html"><a href="computing.html#print-versus-return"><i class="fa fa-check"></i><b>B.3.10</b> Print versus Return</a></li>
<li class="chapter" data-level="B.3.11" data-path="computing.html"><a href="computing.html#help"><i class="fa fa-check"></i><b>B.3.11</b> Help</a></li>
<li class="chapter" data-level="B.3.12" data-path="computing.html"><a href="computing.html#keyboard-shortcuts"><i class="fa fa-check"></i><b>B.3.12</b> Keyboard Shortcuts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>C</b> Probability</a>
<ul>
<li class="chapter" data-level="C.1" data-path="probability.html"><a href="probability.html#reading-1"><i class="fa fa-check"></i><b>C.1</b> Reading</a></li>
<li class="chapter" data-level="C.2" data-path="probability.html"><a href="probability.html#probability-models"><i class="fa fa-check"></i><b>C.2</b> Probability Models</a></li>
<li class="chapter" data-level="C.3" data-path="probability.html"><a href="probability.html#probability-axioms"><i class="fa fa-check"></i><b>C.3</b> Probability Axioms</a></li>
<li class="chapter" data-level="C.4" data-path="probability.html"><a href="probability.html#probability-rules"><i class="fa fa-check"></i><b>C.4</b> Probability Rules</a></li>
<li class="chapter" data-level="C.5" data-path="probability.html"><a href="probability.html#random-variables"><i class="fa fa-check"></i><b>C.5</b> Random Variables</a>
<ul>
<li class="chapter" data-level="C.5.1" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>C.5.1</b> Distributions</a></li>
<li class="chapter" data-level="C.5.2" data-path="probability.html"><a href="probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>C.5.2</b> Discrete Random Variables</a></li>
<li class="chapter" data-level="C.5.3" data-path="probability.html"><a href="probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>C.5.3</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="C.5.4" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>C.5.4</b> Distributions in R</a></li>
<li class="chapter" data-level="C.5.5" data-path="probability.html"><a href="probability.html#several-random-variables"><i class="fa fa-check"></i><b>C.5.5</b> Several Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="probability.html"><a href="probability.html#expectations"><i class="fa fa-check"></i><b>C.6</b> Expectations</a></li>
<li class="chapter" data-level="C.7" data-path="probability.html"><a href="probability.html#likelihood"><i class="fa fa-check"></i><b>C.7</b> Likelihood</a></li>
<li class="chapter" data-level="C.8" data-path="probability.html"><a href="probability.html#additional-references"><i class="fa fa-check"></i><b>C.8</b> Additional References</a>
<ul>
<li class="chapter" data-level="C.8.1" data-path="probability.html"><a href="probability.html#videos"><i class="fa fa-check"></i><b>C.8.1</b> Videos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>D</b> Statistics</a>
<ul>
<li class="chapter" data-level="D.1" data-path="statistics.html"><a href="statistics.html#reading-2"><i class="fa fa-check"></i><b>D.1</b> Reading</a></li>
<li class="chapter" data-level="D.2" data-path="statistics.html"><a href="statistics.html#statistics-1"><i class="fa fa-check"></i><b>D.2</b> Statistics</a></li>
<li class="chapter" data-level="D.3" data-path="statistics.html"><a href="statistics.html#estimators"><i class="fa fa-check"></i><b>D.3</b> Estimators</a>
<ul>
<li class="chapter" data-level="D.3.1" data-path="statistics.html"><a href="statistics.html#properties"><i class="fa fa-check"></i><b>D.3.1</b> Properties</a></li>
<li class="chapter" data-level="D.3.2" data-path="statistics.html"><a href="statistics.html#example-mse-of-an-estimator"><i class="fa fa-check"></i><b>D.3.2</b> Example: MSE of an Estimator</a></li>
<li class="chapter" data-level="D.3.3" data-path="statistics.html"><a href="statistics.html#estimation-methods"><i class="fa fa-check"></i><b>D.3.3</b> Estimation Methods</a></li>
<li class="chapter" data-level="D.3.4" data-path="statistics.html"><a href="statistics.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>D.3.4</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="D.3.5" data-path="statistics.html"><a href="statistics.html#method-of-moments"><i class="fa fa-check"></i><b>D.3.5</b> Method of Moments</a></li>
<li class="chapter" data-level="D.3.6" data-path="statistics.html"><a href="statistics.html#empirical-distribution-function"><i class="fa fa-check"></i><b>D.3.6</b> Empirical Distribution Function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://daviddalpiaz.org" target="blank">&copy; 2020 David Dalpiaz</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basics of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-biasvariance-tradeoff" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> The Bias–Variance Tradeoff</h1>
<p>This chapter will begin to dig into some theoretical details of estimating regression functions, in particular how the <strong>bias-variance tradeoff</strong> helps explain the relationship between <strong>model flexibility</strong> and the errors a model makes.</p>
<p>Specifically, we will discuss:</p>
<ul>
<li>The definitions and relationship between <strong>bias</strong>, <strong>variance</strong>, and <strong>mean squared error.</strong></li>
<li>The relationship between <strong>model flexibility</strong> and training error.</li>
<li>The relationship between <strong>model flexibility</strong> and validation error.</li>
</ul>
<p>Don’t fret if this presentation seems overwhelming. Next chapter we will review some general concepts about regression before moving on to classification.</p>
<div id="r-setup-and-source-2" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> R Setup and Source</h2>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="bias-variance-tradeoff.html#cb139-1"></a><span class="kw">library</span>(tibble)     <span class="co"># data frame printing</span></span>
<span id="cb139-2"><a href="bias-variance-tradeoff.html#cb139-2"></a><span class="kw">library</span>(dplyr)      <span class="co"># data manipulation</span></span>
<span id="cb139-3"><a href="bias-variance-tradeoff.html#cb139-3"></a></span>
<span id="cb139-4"><a href="bias-variance-tradeoff.html#cb139-4"></a><span class="kw">library</span>(caret)      <span class="co"># fitting knn</span></span>
<span id="cb139-5"><a href="bias-variance-tradeoff.html#cb139-5"></a><span class="kw">library</span>(rpart)      <span class="co"># fitting trees</span></span>
<span id="cb139-6"><a href="bias-variance-tradeoff.html#cb139-6"></a><span class="kw">library</span>(rpart.plot) <span class="co"># plotting trees</span></span></code></pre></div>
<p>Recall that the <a href="index.html">Welcome</a> chapter contains directions for installing all necessary packages for following along with the text. The R Markdown source is provided as some code, mostly for creating plots, has been suppressed from the rendered document that you are currently reading.</p>
<ul>
<li><strong>R Markdown Source:</strong> <a href="bias-variance-tradeoff.Rmd"><code>bias-variance-tradeoff.Rmd</code></a></li>
</ul>
</div>
<div id="the-regression-setup" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> The Regression Setup</h2>
<p>Consider the general regression setup where we are given a random pair <span class="math inline">\((X, Y) \in \mathbb{R}^p \times \mathbb{R}\)</span>. We would like to “predict” <span class="math inline">\(Y\)</span> with some function of <span class="math inline">\(X\)</span>, say, <span class="math inline">\(f(X)\)</span>.</p>
<p>To clarify what we mean by “predict,” we specify that we would like <span class="math inline">\(f(X)\)</span> to be “close” to <span class="math inline">\(Y\)</span>. To further clarify what we mean by “close,” we define the <strong>squared error loss</strong> of estimating <span class="math inline">\(Y\)</span> using <span class="math inline">\(f(X)\)</span>.</p>
<p><span class="math display">\[
L(Y, f(X)) \triangleq (Y - f(X)) ^ 2
\]</span></p>
<p>Now we can clarify the goal of regression, which is to minimize the above loss, on average. We call this the <strong>risk</strong> of estimating <span class="math inline">\(Y\)</span> using <span class="math inline">\(f(X)\)</span>.</p>
<p><span class="math display">\[
R(Y, f(X)) \triangleq \mathbb{E}[L(Y, f(X))] = \mathbb{E}_{X, Y}[(Y - f(X)) ^ 2]
\]</span></p>
<p>Before attempting to minimize the risk, we first re-write the risk after conditioning on <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[
\mathbb{E}_{X, Y} \left[ (Y - f(X)) ^ 2 \right] = \mathbb{E}_{X} \mathbb{E}_{Y \mid X} \left[ ( Y - f(X) ) ^ 2 \mid X = x \right]
\]</span></p>
<p>Minimizing the right-hand side is much easier, as it simply amounts to minimizing the inner expectation with respect to <span class="math inline">\(Y \mid X\)</span>, essentially minimizing the risk pointwise, for each <span class="math inline">\(x\)</span>.</p>
<p>It turns out, that the risk is minimized by setting <span class="math inline">\(f(x)\)</span> to be equal the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>,</p>
<p><span class="math display">\[
f(x) = \mathbb{E}(Y \mid X = x)
\]</span></p>
<p>which we call the <strong>regression function</strong>.<a href="#fn63" class="footnote-ref" id="fnref63"><sup>63</sup></a></p>
<p>Note that the choice of squared error loss is somewhat arbitrary. Suppose instead we chose absolute error loss.</p>
<p><span class="math display">\[
L(Y, f(X)) \triangleq | Y - f(X) | 
\]</span></p>
<p>The risk would then be minimized setting <span class="math inline">\(f(x)\)</span> equal to the conditional median.</p>
<p><span class="math display">\[
f(x) = \text{median}(Y \mid X = x)
\]</span></p>
<p>Despite this possibility, our preference will still be for squared error loss. The reasons for this are numerous, including: historical, ease of optimization, and protecting against large deviations.</p>
<p>Now, given data <span class="math inline">\(\mathcal{D} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}\)</span>, our goal becomes finding some <span class="math inline">\(\hat{f}\)</span> that is a good estimate of the regression function <span class="math inline">\(f\)</span>. We’ll see that this amounts to minimizing what we call the reducible error.</p>
</div>
<div id="reducible-and-irreducible-error" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Reducible and Irreducible Error</h2>
<p>Suppose that we obtain some <span class="math inline">\(\hat{f}\)</span>, how well does it estimate <span class="math inline">\(f\)</span>? We define the <strong>expected prediction error</strong> of predicting <span class="math inline">\(Y\)</span> using <span class="math inline">\(\hat{f}(X)\)</span>. A good <span class="math inline">\(\hat{f}\)</span> will have a low expected prediction error.</p>
<p><span class="math display">\[
\text{EPE}\left(Y, \hat{f}(X)\right) \triangleq \mathbb{E}_{X, Y, \mathcal{D}} \left[  \left( Y - \hat{f}(X) \right)^2 \right]
\]</span></p>
<p>This expectation is over <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and also <span class="math inline">\(\mathcal{D}\)</span>. The estimate <span class="math inline">\(\hat{f}\)</span> is actually random depending on the data, <span class="math inline">\(\mathcal{D}\)</span>, used to estimate <span class="math inline">\(\hat{f}\)</span>. We could actually write <span class="math inline">\(\hat{f}(X, \mathcal{D})\)</span> to make this dependence explicit, but our notation will become cumbersome enough as it is.</p>
<p>Like before, we’ll condition on <span class="math inline">\(X\)</span>. This results in the expected prediction error of predicting <span class="math inline">\(Y\)</span> using <span class="math inline">\(\hat{f}(X)\)</span> when <span class="math inline">\(X = x\)</span>.</p>
<p><span class="math display">\[
\text{EPE}\left(Y, \hat{f}(x)\right) = 
\mathbb{E}_{Y \mid X, \mathcal{D}} \left[  \left(Y - \hat{f}(X) \right)^2 \mid X = x \right] = 
\underbrace{\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]}_\textrm{reducible error} + 
\underbrace{\mathbb{V}_{Y \mid X} \left[ Y \mid X = x \right]}_\textrm{irreducible error}
\]</span></p>
<p>A number of things to note here:</p>
<ul>
<li>The expected prediction error is for a random <span class="math inline">\(Y\)</span> given a fixed <span class="math inline">\(x\)</span> and a random <span class="math inline">\(\hat{f}\)</span>. As such, the expectation is over <span class="math inline">\(Y \mid X\)</span> and <span class="math inline">\(\mathcal{D}\)</span>. Our estimated function <span class="math inline">\(\hat{f}\)</span> is random depending on the data, <span class="math inline">\(\mathcal{D}\)</span>, which is used to perform the estimation.</li>
<li>The expected prediction error of predicting <span class="math inline">\(Y\)</span> using <span class="math inline">\(\hat{f}(X)\)</span> when <span class="math inline">\(X = x\)</span> has been decomposed into two errors:
<ul>
<li>The <strong>reducible error</strong>, which is the expected squared error loss of estimation <span class="math inline">\(f(x)\)</span> using <span class="math inline">\(\hat{f}(x)\)</span> at a fixed point <span class="math inline">\(x\)</span>. The only thing that is random here is <span class="math inline">\(\mathcal{D}\)</span>, the data used to obtain <span class="math inline">\(\hat{f}\)</span>. (Both <span class="math inline">\(f\)</span> and <span class="math inline">\(x\)</span> are fixed.) We’ll often call this reducible error the <strong>mean squared error</strong> of estimating <span class="math inline">\(f(x)\)</span> using <span class="math inline">\(\hat{f}\)</span> at a fixed point <span class="math inline">\(x\)</span>. <span class="math display">\[
\text{MSE}\left(f(x), \hat{f}(x)\right) \triangleq 
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right]\]</span></li>
<li>The <strong>irreducible error</strong>. This is simply the variance of <span class="math inline">\(Y\)</span> given that <span class="math inline">\(X = x\)</span>, essentially noise that we do not want to learn. This is also called the <strong>Bayes error</strong>.</li>
</ul></li>
</ul>
<p>As the name suggests, the reducible error is the error that we have some control over. But how do we control this error?</p>
</div>
<div id="bias-variance-decomposition" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Bias-Variance Decomposition</h2>
<p>After decomposing the expected prediction error into reducible and irreducible error, we can further decompose the reducible error.</p>
<p>Recall the definition of the <strong>bias</strong> of an estimator.</p>
<p><span class="math display">\[
\text{bias}(\hat{\theta}) \triangleq \mathbb{E}\left[\hat{\theta}\right] - \theta
\]</span></p>
<p>Also recall the definition of the <strong>variance</strong> of an estimator.</p>
<p><span class="math display">\[
\mathbb{V}(\hat{\theta}) = \text{var}(\hat{\theta}) \triangleq \mathbb{E}\left [ ( \hat{\theta} -\mathbb{E}\left[\hat{\theta}\right] )^2 \right]
\]</span></p>
<p>Using this, we further decompose the reducible error (mean squared error) into bias squared and variance.</p>
<p><span class="math display">\[
\text{MSE}\left(f(x), \hat{f}(x)\right) = 
\mathbb{E}_{\mathcal{D}} \left[  \left(f(x) - \hat{f}(x) \right)^2 \right] = 
\underbrace{\left(f(x) - \mathbb{E} \left[ \hat{f}(x) \right]  \right)^2}_{\text{bias}^2 \left(\hat{f}(x) \right)} +
\underbrace{\mathbb{E} \left[ \left( \hat{f}(x) - \mathbb{E} \left[ \hat{f}(x) \right] \right)^2 \right]}_{\text{var} \left(\hat{f}(x) \right)}
\]</span></p>
<p>This is actually a common fact in estimation theory, but we have stated it here specifically for estimation of some regression function <span class="math inline">\(f\)</span> using <span class="math inline">\(\hat{f}\)</span> at some point <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[
\text{MSE}\left(f(x), \hat{f}(x)\right) = \text{bias}^2 \left(\hat{f}(x) \right) + \text{var} \left(\hat{f}(x) \right)
\]</span></p>
<p>In a perfect world, we would be able to find some <span class="math inline">\(\hat{f}\)</span> which is <strong>unbiased</strong>, that is <span class="math inline">\(\text{bias}\left(\hat{f}(x) \right) = 0\)</span>, which also has low variance. In practice, this isn’t always possible.</p>
<p>It turns out, there is a <strong>bias-variance tradeoff</strong>. That is, often, the more bias in our estimation, the lesser the variance. Similarly, less variance is often accompanied by more bias. Flexible models tend to be unbiased, but highly variable. Simple models are often extremely biased, but have low variance.</p>
<p>In the context of regression, models are biased when:</p>
<ul>
<li>Parametric: The form of the model <a href="https://en.wikipedia.org/wiki/Omitted-variable_bias">does not incorporate all the necessary variables</a>, or the form of the relationship is too simple. For example, a parametric model assumes a linear relationship, but the true relationship is quadratic.</li>
<li>Non-parametric: The model provides too much smoothing.</li>
</ul>
<p>In the context of regression, models are variable when:</p>
<ul>
<li>Parametric: The form of the model incorporates too many variables, or the form of the relationship is too flexible. For example, a parametric model assumes a cubic relationship, but the true relationship is linear.</li>
<li>Non-parametric: The model does not provide enough smoothing. It is very, “wiggly.”</li>
</ul>
<p>So for us, to select a model that appropriately balances the tradeoff between bias and variance, and thus minimizes the reducible error, we need to select a model of the appropriate flexibility for the data.</p>
<p>Recall that when fitting models, we’ve seen that train RMSE decreases as model flexibility is increasing. (Technically it is non-increasing.) For validation RMSE, we expect to see a U-shaped curve. Importantly, validation RMSE decreases, until a certain flexibility, then begins to increase.</p>
<p>Now we can understand why this is happening. The expected test RMSE is essentially the expected prediction error, which we now known decomposes into (squared) bias, variance, and the irreducible Bayes error. The following plots show three examples of this.</p>
<p><img src="bias-variance-tradeoff_files/figure-html/unnamed-chunk-2-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>The three plots show three examples of the bias-variance tradeoff. In the left panel, the variance influences the expected prediction error more than the bias. In the right panel, the opposite is true. The middle panel is somewhat neutral. In all cases, the difference between the Bayes error (the horizontal dashed grey line) and the expected prediction error (the solid black curve) is exactly the mean squared error, which is the sum of the squared bias (blue curve) and variance (orange curve). The vertical line indicates the flexibility that minimizes the prediction error.</p>
<p>To summarize, if we assume that irreducible error can be written as</p>
<p><span class="math display">\[
\mathbb{V}[Y \mid X = x] = \sigma ^ 2
\]</span></p>
<p>then we can write the full decomposition of the expected prediction error of predicting <span class="math inline">\(Y\)</span> using <span class="math inline">\(\hat{f}\)</span> when <span class="math inline">\(X = x\)</span> as</p>
<p><span class="math display">\[
\text{EPE}\left(Y, \hat{f}(x)\right) =  
\underbrace{\text{bias}^2\left(\hat{f}(x)\right) + \text{var}\left(\hat{f}(x)\right)}_\textrm{reducible error} + \sigma^2.
\]</span></p>
<p>As model flexibility increases, bias decreases, while variance increases. By understanding the tradeoff between bias and variance, we can manipulate model flexibility to find a model that will predict well on unseen observations.</p>
<p><img src="bias-variance-tradeoff_files/figure-html/error-vs-flex-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Tying this all together, the above image shows how we “expect” training and validation error to behavior in relation to model flexibility.<a href="#fn64" class="footnote-ref" id="fnref64"><sup>64</sup></a> In practice, we won’t always see such a nice “curve” in the validation error, but we expect to see the general trends.</p>
</div>
<div id="using-simulation-to-estimate-bias-and-variance" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Using Simulation to Estimate Bias and Variance</h2>
<p>We will illustrate these decompositions, most importantly the bias-variance tradeoff, through simulation. Suppose we would like to train a model to learn the true regression function function <span class="math inline">\(f(x) = x^2\)</span>.</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="bias-variance-tradeoff.html#cb140-1"></a>f =<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb140-2"><a href="bias-variance-tradeoff.html#cb140-2"></a>  x <span class="op">^</span><span class="st"> </span><span class="dv">2</span></span>
<span id="cb140-3"><a href="bias-variance-tradeoff.html#cb140-3"></a>}</span></code></pre></div>
<p>More specifically, we’d like to predict an observation, <span class="math inline">\(Y\)</span>, given that <span class="math inline">\(X = x\)</span> by using <span class="math inline">\(\hat{f}(x)\)</span> where</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid X = x] = f(x) = x^2
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbb{V}[Y \mid X = x] = \sigma ^ 2.
\]</span></p>
<p>Alternatively, we could write this as</p>
<p><span class="math display">\[
Y = f(X) + \epsilon
\]</span></p>
<p>where <span class="math inline">\(\mathbb{E}[\epsilon] = 0\)</span> and <span class="math inline">\(\mathbb{V}[\epsilon] = \sigma ^ 2\)</span>. In this formulation, we call <span class="math inline">\(f(X)\)</span> the <strong>signal</strong> and <span class="math inline">\(\epsilon\)</span> the <strong>noise</strong>.</p>
<p>To carry out a concrete simulation example, we need to fully specify the data generating process. We do so with the following <code>R</code> code.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="bias-variance-tradeoff.html#cb141-1"></a>gen_sim_data =<span class="st"> </span><span class="cf">function</span>(f, <span class="dt">sample_size =</span> <span class="dv">100</span>) {</span>
<span id="cb141-2"><a href="bias-variance-tradeoff.html#cb141-2"></a>  x =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> sample_size, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">1</span>)</span>
<span id="cb141-3"><a href="bias-variance-tradeoff.html#cb141-3"></a>  y =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> sample_size, <span class="dt">mean =</span> <span class="kw">f</span>(x), <span class="dt">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb141-4"><a href="bias-variance-tradeoff.html#cb141-4"></a>  <span class="kw">tibble</span>(x, y)</span>
<span id="cb141-5"><a href="bias-variance-tradeoff.html#cb141-5"></a>}</span></code></pre></div>
<p>Also note that if you prefer to think of this situation using the <span class="math inline">\(Y = f(X) + \epsilon\)</span> formulation, the following code represents the same data generating process.</p>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="bias-variance-tradeoff.html#cb142-1"></a>gen_sim_data =<span class="st"> </span><span class="cf">function</span>(f, <span class="dt">sample_size =</span> <span class="dv">100</span>) {</span>
<span id="cb142-2"><a href="bias-variance-tradeoff.html#cb142-2"></a>  x =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> sample_size, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">1</span>)</span>
<span id="cb142-3"><a href="bias-variance-tradeoff.html#cb142-3"></a>  eps =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> sample_size, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="fl">0.75</span>)</span>
<span id="cb142-4"><a href="bias-variance-tradeoff.html#cb142-4"></a>  y =<span class="st"> </span><span class="kw">f</span>(x) <span class="op">+</span><span class="st"> </span>eps</span>
<span id="cb142-5"><a href="bias-variance-tradeoff.html#cb142-5"></a>  <span class="kw">tibble</span>(x, y)</span>
<span id="cb142-6"><a href="bias-variance-tradeoff.html#cb142-6"></a>}</span></code></pre></div>
<p>To completely specify the data generating process, we have made more model assumptions than simply <span class="math inline">\(\mathbb{E}[Y \mid X = x] = x^2\)</span> and <span class="math inline">\(\mathbb{V}[Y \mid X = x] = \sigma ^ 2\)</span>. In particular,</p>
<ul>
<li>The <span class="math inline">\(x_i\)</span> in <span class="math inline">\(\mathcal{D}\)</span> are sampled from a uniform distribution over <span class="math inline">\([0, 1]\)</span>.</li>
<li>The <span class="math inline">\(x_i\)</span> and <span class="math inline">\(\epsilon\)</span> are independent.</li>
<li>The <span class="math inline">\(y_i\)</span> in <span class="math inline">\(\mathcal{D}\)</span> are sampled from the conditional normal distribution.</li>
</ul>
<p><span class="math display">\[
Y \mid X \sim N(f(x), \sigma^2)
\]</span></p>
<p>Using this setup, we will generate datasets, <span class="math inline">\(\mathcal{D}\)</span>, with a sample size <span class="math inline">\(n = 100\)</span> and fit four models.</p>
<p><span class="math display">\[
\begin{aligned}
\texttt{predict(fit0, x)} &amp;= \hat{f}_0(x) = \hat{\beta}_0\\
\texttt{predict(fit1, x)} &amp;= \hat{f}_1(x) = \hat{\beta}_0 + \hat{\beta}_1 x \\
\texttt{predict(fit2, x)} &amp;= \hat{f}_2(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 \\
\texttt{predict(fit9, x)} &amp;= \hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\end{aligned}
\]</span></p>
<p>To get a sense of the data and these four models, we generate one simulated dataset, and fit the four models.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="bias-variance-tradeoff.html#cb143-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb143-2"><a href="bias-variance-tradeoff.html#cb143-2"></a>sim_data =<span class="st"> </span><span class="kw">gen_sim_data</span>(f)</span></code></pre></div>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="bias-variance-tradeoff.html#cb144-1"></a>fit_<span class="dv">0</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,                   <span class="dt">data =</span> sim_data)</span>
<span id="cb144-2"><a href="bias-variance-tradeoff.html#cb144-2"></a>fit_<span class="dv">1</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">1</span>), <span class="dt">data =</span> sim_data)</span>
<span id="cb144-3"><a href="bias-variance-tradeoff.html#cb144-3"></a>fit_<span class="dv">2</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">2</span>), <span class="dt">data =</span> sim_data)</span>
<span id="cb144-4"><a href="bias-variance-tradeoff.html#cb144-4"></a>fit_<span class="dv">9</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">9</span>), <span class="dt">data =</span> sim_data)</span></code></pre></div>
<p>Note that technically we’re being lazy and using orthogonal polynomials, but the fitted values are the same, so this makes no difference for our purposes.</p>
<p>Plotting these four trained models, we see that the zero predictor model does very poorly. The first degree model is reasonable, but we can see that the second degree model fits much better. The ninth degree model seem rather wild.</p>
<p><img src="bias-variance-tradeoff_files/figure-html/unnamed-chunk-8-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>The following three plots were created using three additional simulated datasets. The zero predictor and ninth degree polynomial were fit to each.</p>
<p><img src="bias-variance-tradeoff_files/figure-html/unnamed-chunk-9-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>This plot should make clear the difference between the bias and variance of these two models. The zero predictor model is clearly wrong, that is, biased, but nearly the same for each of the datasets, since it has very low variance.</p>
<p>While the ninth degree model doesn’t appear to be correct for any of these three simulations, we’ll see that on average it is, and thus is performing unbiased estimation. These plots do however clearly illustrate that the ninth degree polynomial is extremely variable. Each dataset results in a very different fitted model. Correct on average isn’t the only goal we’re after, since in practice, we’ll only have a single dataset. This is why we’d also like our models to exhibit low variance.</p>
<p>We could have also fit <span class="math inline">\(k\)</span>-nearest neighbors models to these three datasets.</p>
<p><img src="bias-variance-tradeoff_files/figure-html/unnamed-chunk-10-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>Here we see that when <span class="math inline">\(k = 100\)</span> we have a biased model with very low variance.<a href="#fn65" class="footnote-ref" id="fnref65"><sup>65</sup></a> When <span class="math inline">\(k = 5\)</span>, we again have a highly variable model.</p>
<p>These two sets of plots reinforce our intuition about the bias-variance tradeoff. Flexible models (ninth degree polynomial and <span class="math inline">\(k\)</span> = 5) are highly variable, and often unbiased. Simple models (zero predictor linear model and <span class="math inline">\(k = 100\)</span>) are very biased, but have extremely low variance.</p>
<p>We will now complete a simulation study to understand the relationship between the bias, variance, and mean squared error for the estimates of <span class="math inline">\(f(x)\)</span> given by these four models at the point <span class="math inline">\(x = 0.90\)</span>. We use simulation to complete this task, as performing the analytical calculations would prove to be rather tedious and difficult.</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="bias-variance-tradeoff.html#cb145-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb145-2"><a href="bias-variance-tradeoff.html#cb145-2"></a>n_sims =<span class="st"> </span><span class="dv">250</span></span>
<span id="cb145-3"><a href="bias-variance-tradeoff.html#cb145-3"></a>n_models =<span class="st"> </span><span class="dv">4</span></span>
<span id="cb145-4"><a href="bias-variance-tradeoff.html#cb145-4"></a>x =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="fl">0.90</span>) <span class="co"># fixed point at which we make predictions</span></span>
<span id="cb145-5"><a href="bias-variance-tradeoff.html#cb145-5"></a>predictions =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> n_sims, <span class="dt">ncol =</span> n_models)</span></code></pre></div>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="bias-variance-tradeoff.html#cb146-1"></a><span class="cf">for</span> (sim <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n_sims) {</span>
<span id="cb146-2"><a href="bias-variance-tradeoff.html#cb146-2"></a></span>
<span id="cb146-3"><a href="bias-variance-tradeoff.html#cb146-3"></a>  <span class="co"># simulate new, random, training data</span></span>
<span id="cb146-4"><a href="bias-variance-tradeoff.html#cb146-4"></a>  <span class="co"># this is the only random portion of the bias, var, and mse calculations</span></span>
<span id="cb146-5"><a href="bias-variance-tradeoff.html#cb146-5"></a>  <span class="co"># this allows us to calculate the expectation over D</span></span>
<span id="cb146-6"><a href="bias-variance-tradeoff.html#cb146-6"></a>  sim_data =<span class="st"> </span><span class="kw">gen_sim_data</span>(f)</span>
<span id="cb146-7"><a href="bias-variance-tradeoff.html#cb146-7"></a></span>
<span id="cb146-8"><a href="bias-variance-tradeoff.html#cb146-8"></a>  <span class="co"># fit models</span></span>
<span id="cb146-9"><a href="bias-variance-tradeoff.html#cb146-9"></a>  fit_<span class="dv">0</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,                   <span class="dt">data =</span> sim_data)</span>
<span id="cb146-10"><a href="bias-variance-tradeoff.html#cb146-10"></a>  fit_<span class="dv">1</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">1</span>), <span class="dt">data =</span> sim_data)</span>
<span id="cb146-11"><a href="bias-variance-tradeoff.html#cb146-11"></a>  fit_<span class="dv">2</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">2</span>), <span class="dt">data =</span> sim_data)</span>
<span id="cb146-12"><a href="bias-variance-tradeoff.html#cb146-12"></a>  fit_<span class="dv">9</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">9</span>), <span class="dt">data =</span> sim_data)</span>
<span id="cb146-13"><a href="bias-variance-tradeoff.html#cb146-13"></a></span>
<span id="cb146-14"><a href="bias-variance-tradeoff.html#cb146-14"></a>  <span class="co"># get predictions</span></span>
<span id="cb146-15"><a href="bias-variance-tradeoff.html#cb146-15"></a>  predictions[sim, <span class="dv">1</span>] =<span class="st"> </span><span class="kw">predict</span>(fit_<span class="dv">0</span>, x)</span>
<span id="cb146-16"><a href="bias-variance-tradeoff.html#cb146-16"></a>  predictions[sim, <span class="dv">2</span>] =<span class="st"> </span><span class="kw">predict</span>(fit_<span class="dv">1</span>, x)</span>
<span id="cb146-17"><a href="bias-variance-tradeoff.html#cb146-17"></a>  predictions[sim, <span class="dv">3</span>] =<span class="st"> </span><span class="kw">predict</span>(fit_<span class="dv">2</span>, x)</span>
<span id="cb146-18"><a href="bias-variance-tradeoff.html#cb146-18"></a>  predictions[sim, <span class="dv">4</span>] =<span class="st"> </span><span class="kw">predict</span>(fit_<span class="dv">9</span>, x)</span>
<span id="cb146-19"><a href="bias-variance-tradeoff.html#cb146-19"></a>}</span></code></pre></div>
<p>Note that this is one of many ways we could have accomplished this task using <code>R</code>. For example we could have used a combination of <code>replicate()</code> and <code>*apply()</code> functions. Alternatively, we could have used a <a href="https://www.tidyverse.org/"><code>tidyverse</code></a> approach, which likely would have used some combination of <a href="http://dplyr.tidyverse.org/"><code>dplyr</code></a>, <a href="http://tidyr.tidyverse.org/"><code>tidyr</code></a>, and <a href="http://purrr.tidyverse.org/"><code>purrr</code></a>.</p>
<p>Our approach, which would be considered a <code>base</code> <code>R</code> approach, was chosen to make it as clear as possible what is being done. The <code>tidyverse</code> approach is rapidly gaining popularity in the <code>R</code> community, but might make it more difficult to see what is happening here, unless you are already familiar with that approach.</p>
<p>Also of note, while it may seem like the output stored in <code>predictions</code> would meet the definition of <a href="http://vita.had.co.nz/papers/tidy-data.html">tidy data</a> given by <a href="http://hadley.nz/">Hadley Wickham</a> since each row represents a simulation, it actually falls slightly short. For our data to be tidy, a row should store the simulation number, the model, and the resulting prediction. We’ve actually already aggregated one level above this. Our observational unit is a simulation (with four predictions), but for tidy data, it should be a single prediction.</p>
<p><img src="bias-variance-tradeoff_files/figure-html/unnamed-chunk-14-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>The above plot shows the predictions for each of the 250 simulations of each of the four models of different polynomial degrees. The truth, <span class="math inline">\(f(x = 0.90) = (0.9)^2 = 0.81\)</span>, is given by the solid black horizontal line.</p>
<p>Two things are immediately clear:</p>
<ul>
<li>As flexibility <em>increases</em>, <strong>bias decreases</strong>. The mean of a model’s predictions is closer to the truth.</li>
<li>As flexibility <em>increases</em>, <strong>variance increases</strong>. The variance about the mean of a model’s predictions increases.</li>
</ul>
<p>The goal of this simulation study is to show that the following holds true for each of the four models.</p>
<p><span class="math display">\[
\text{MSE}\left(f(0.90), \hat{f}_k(0.90)\right) = 
\underbrace{\left(\mathbb{E} \left[ \hat{f}_k(0.90) \right] - f(0.90) \right)^2}_{\text{bias}^2 \left(\hat{f}_k(0.90) \right)} +
\underbrace{\mathbb{E} \left[ \left( \hat{f}_k(0.90) - \mathbb{E} \left[ \hat{f}_k(0.90) \right] \right)^2 \right]}_{\text{var} \left(\hat{f}_k(0.90) \right)}
\]</span></p>
<p>We’ll use the empirical results of our simulations to estimate these quantities. (Yes, we’re using estimation to justify facts about estimation.) Note that we’ve actually used a rather small number of simulations. In practice we should use more, but for the sake of computation time, we’ve performed just enough simulations to obtain the desired results. (Since we’re estimating estimation, the bigger the sample size, the better.)</p>
<p>To estimate the mean squared error of our predictions, we’ll use</p>
<p><span class="math display">\[
\widehat{\text{MSE}}\left(f(0.90), \hat{f}_k(0.90)\right) = \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}} \left(f(0.90) - \hat{f}_k^{[i]}(0.90) \right)^2
\]</span></p>
<p>where <span class="math inline">\(\hat{f}_k^{[i]}(0.90)\)</span> is the estimate of <span class="math inline">\(f(0.90)\)</span> using the <span class="math inline">\(i\)</span>-th from the polynomial degree <span class="math inline">\(k\)</span> model.</p>
<p>We also write an accompanying <code>R</code> function.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="bias-variance-tradeoff.html#cb147-1"></a>get_mse =<span class="st"> </span><span class="cf">function</span>(truth, estimate) {</span>
<span id="cb147-2"><a href="bias-variance-tradeoff.html#cb147-2"></a>  <span class="kw">mean</span>((estimate <span class="op">-</span><span class="st"> </span>truth) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)</span>
<span id="cb147-3"><a href="bias-variance-tradeoff.html#cb147-3"></a>}</span></code></pre></div>
<p>Similarly, for the bias of our predictions we use,</p>
<p><span class="math display">\[
\widehat{\text{bias}} \left(\hat{f}(0.90) \right)  = \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}} \left(\hat{f}_k^{[i]}(0.90) \right) - f(0.90)
\]</span></p>
<p>And again, we write an accompanying <code>R</code> function.</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="bias-variance-tradeoff.html#cb148-1"></a>get_bias =<span class="st"> </span><span class="cf">function</span>(estimate, truth) {</span>
<span id="cb148-2"><a href="bias-variance-tradeoff.html#cb148-2"></a>  <span class="kw">mean</span>(estimate) <span class="op">-</span><span class="st"> </span>truth</span>
<span id="cb148-3"><a href="bias-variance-tradeoff.html#cb148-3"></a>}</span></code></pre></div>
<p>Lastly, for the variance of our predictions we have</p>
<p><span class="math display">\[
\widehat{\text{var}} \left(\hat{f}(0.90) \right) = \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}} \left(\hat{f}_k^{[i]}(0.90) - \frac{1}{n_{\texttt{sims}}}\sum_{i = 1}^{n_{\texttt{sims}}}\hat{f}_k^{[i]}(0.90) \right)^2 
\]</span></p>
<p>While there is already <code>R</code> function for variance, the following is more appropriate in this situation.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="bias-variance-tradeoff.html#cb149-1"></a>get_var =<span class="st"> </span><span class="cf">function</span>(estimate) {</span>
<span id="cb149-2"><a href="bias-variance-tradeoff.html#cb149-2"></a>  <span class="kw">mean</span>((estimate <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(estimate)) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)</span>
<span id="cb149-3"><a href="bias-variance-tradeoff.html#cb149-3"></a>}</span></code></pre></div>
<p>To quickly obtain these results for each of the four models, we utilize the <code>apply()</code> function.</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="bias-variance-tradeoff.html#cb150-1"></a>bias =<span class="st"> </span><span class="kw">apply</span>(predictions, <span class="dv">2</span>, get_bias, <span class="dt">truth =</span> <span class="kw">f</span>(<span class="dt">x =</span> <span class="fl">0.90</span>))</span>
<span id="cb150-2"><a href="bias-variance-tradeoff.html#cb150-2"></a>variance =<span class="st"> </span><span class="kw">apply</span>(predictions, <span class="dv">2</span>, get_var)</span>
<span id="cb150-3"><a href="bias-variance-tradeoff.html#cb150-3"></a>mse =<span class="st"> </span><span class="kw">apply</span>(predictions, <span class="dv">2</span>, get_mse, <span class="dt">truth =</span> <span class="kw">f</span>(<span class="dt">x =</span> <span class="fl">0.90</span>))</span></code></pre></div>
<p>We summarize these results in the following table.</p>
<table>
<thead>
<tr class="header">
<th align="center">Degree</th>
<th align="center">Mean Squared Error</th>
<th align="center">Bias Squared</th>
<th align="center">Variance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">0.22643</td>
<td align="center">0.22476</td>
<td align="center">0.00167</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">0.00829</td>
<td align="center">0.00508</td>
<td align="center">0.00322</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">0.00387</td>
<td align="center">0.00005</td>
<td align="center">0.00381</td>
</tr>
<tr class="even">
<td align="center">9</td>
<td align="center">0.01019</td>
<td align="center">0.00002</td>
<td align="center">0.01017</td>
</tr>
</tbody>
</table>
<p>A number of things to notice here:</p>
<ul>
<li>We use squared bias in this table. Since bias can be positive or negative, squared bias is more useful for observing the trend as flexibility increases.</li>
<li>The squared bias trend which we see here is <strong>decreasing</strong> as flexibility increases, which we expect to see in general.</li>
<li>The exact opposite is true of variance. As model flexibility increases, variance <strong>increases</strong>.</li>
<li>The mean squared error, which is a function of the bias and variance, decreases, then increases. This is a result of the bias-variance tradeoff. We can decrease bias, by increasing variance. Or, we can decrease variance by increasing bias. By striking the correct balance, we can find a good mean squared error!</li>
</ul>
<p>We can check for these trends with the <code>diff()</code> function in <code>R</code>.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="bias-variance-tradeoff.html#cb151-1"></a><span class="kw">all</span>(<span class="kw">diff</span>(bias <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="bias-variance-tradeoff.html#cb153-1"></a><span class="kw">all</span>(<span class="kw">diff</span>(variance) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="bias-variance-tradeoff.html#cb155-1"></a><span class="kw">diff</span>(mse) <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span> </span></code></pre></div>
<pre><code>##     1     2     9 
##  TRUE  TRUE FALSE</code></pre>
<p>The models with polynomial degrees 2 and 9 are both essentially unbiased. We see some bias here as a result of using simulation. If we increased the number of simulations, we would see both biases go down. Since they are both unbiased, the model with degree 2 outperforms the model with degree 9 due to its smaller variance.</p>
<p>Models with degree 0 and 1 are biased because they assume the wrong form of the regression function. While the degree 9 model does this as well, it does include all the necessary polynomial degrees.</p>
<p><span class="math display">\[
\hat{f}_9(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \ldots + \hat{\beta}_9 x^9
\]</span></p>
<p>Then, since least squares estimation is unbiased, importantly,</p>
<p><span class="math display">\[
\mathbb{E}\left[\hat{\beta}_d\right] = \beta_d = 0
\]</span></p>
<p>for <span class="math inline">\(d = 3, 4, \ldots 9\)</span>, we have</p>
<p><span class="math display">\[
\mathbb{E}\left[\hat{f}_9(x)\right] = \beta_0 + \beta_1 x + \beta_2 x^2
\]</span></p>
<p>Now we can finally verify the bias-variance decomposition.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="bias-variance-tradeoff.html#cb157-1"></a>bias <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>variance <span class="op">==</span><span class="st"> </span>mse</span></code></pre></div>
<pre><code>##     0     1     2     9 
## FALSE FALSE FALSE FALSE</code></pre>
<p>But wait, this says it isn’t true, except for the degree 9 model? It turns out, this is simply a computational issue. If we allow for some very small error tolerance, we see that the bias-variance decomposition is indeed true for predictions from these for models.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="bias-variance-tradeoff.html#cb159-1"></a><span class="kw">all.equal</span>(bias <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>variance, mse)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>See <code>?all.equal()</code> for details.</p>
<p>So far, we’ve focused our efforts on looking at the mean squared error of estimating <span class="math inline">\(f(0.90)\)</span> using <span class="math inline">\(\hat{f}(0.90)\)</span>. We could also look at the expected prediction error of using <span class="math inline">\(\hat{f}(X)\)</span> when <span class="math inline">\(X = 0.90\)</span> to estimate <span class="math inline">\(Y\)</span>.</p>
<p><span class="math display">\[
\text{EPE}\left(Y, \hat{f}_k(0.90)\right) = 
\mathbb{E}_{Y \mid X, \mathcal{D}} \left[  \left(Y - \hat{f}_k(X) \right)^2 \mid X = 0.90 \right]
\]</span></p>
<p>We can estimate this quantity for each of the four models using the simulation study we already performed.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="bias-variance-tradeoff.html#cb161-1"></a>get_epe =<span class="st"> </span><span class="cf">function</span>(realized, estimate) {</span>
<span id="cb161-2"><a href="bias-variance-tradeoff.html#cb161-2"></a>  <span class="kw">mean</span>((realized <span class="op">-</span><span class="st"> </span>estimate) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)</span>
<span id="cb161-3"><a href="bias-variance-tradeoff.html#cb161-3"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="bias-variance-tradeoff.html#cb162-1"></a>y =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="kw">nrow</span>(predictions), <span class="dt">mean =</span> <span class="kw">f</span>(<span class="dt">x =</span> <span class="fl">0.9</span>), <span class="dt">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb162-2"><a href="bias-variance-tradeoff.html#cb162-2"></a>epe =<span class="st"> </span><span class="kw">apply</span>(predictions, <span class="dv">2</span>, get_epe, <span class="dt">realized =</span> y)</span>
<span id="cb162-3"><a href="bias-variance-tradeoff.html#cb162-3"></a>epe</span></code></pre></div>
<pre><code>##         0         1         2         9 
## 0.3180470 0.1104055 0.1095955 0.1205570</code></pre>
<p>What about the unconditional expected prediction error. That is, for any <span class="math inline">\(X\)</span>, not just <span class="math inline">\(0.90\)</span>. Specifically, the expected prediction error of estimating <span class="math inline">\(Y\)</span> using <span class="math inline">\(\hat{f}(X)\)</span>. The following (new) simulation study provides an estimate of</p>
<p><span class="math display">\[
\text{EPE}\left(Y, \hat{f}_k(X)\right) = \mathbb{E}_{X, Y, \mathcal{D}} \left[  \left( Y - \hat{f}_k(X) \right)^2 \right]
\]</span></p>
<p>for the quadratic model, that is <span class="math inline">\(k = 2\)</span> as we have defined <span class="math inline">\(k\)</span>.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="bias-variance-tradeoff.html#cb164-1"></a><span class="kw">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb164-2"><a href="bias-variance-tradeoff.html#cb164-2"></a>n_sims =<span class="st"> </span><span class="dv">2500</span></span>
<span id="cb164-3"><a href="bias-variance-tradeoff.html#cb164-3"></a>X =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> n_sims, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">1</span>)</span>
<span id="cb164-4"><a href="bias-variance-tradeoff.html#cb164-4"></a>Y =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> n_sims, <span class="dt">mean =</span> <span class="kw">f</span>(X), <span class="dt">sd =</span> <span class="fl">0.3</span>)</span>
<span id="cb164-5"><a href="bias-variance-tradeoff.html#cb164-5"></a></span>
<span id="cb164-6"><a href="bias-variance-tradeoff.html#cb164-6"></a>f_hat_X =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(X))</span>
<span id="cb164-7"><a href="bias-variance-tradeoff.html#cb164-7"></a></span>
<span id="cb164-8"><a href="bias-variance-tradeoff.html#cb164-8"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_along</span>(X)) {</span>
<span id="cb164-9"><a href="bias-variance-tradeoff.html#cb164-9"></a>  sim_data =<span class="st"> </span><span class="kw">gen_sim_data</span>(f)</span>
<span id="cb164-10"><a href="bias-variance-tradeoff.html#cb164-10"></a>  fit_<span class="dv">2</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">2</span>), <span class="dt">data =</span> sim_data)</span>
<span id="cb164-11"><a href="bias-variance-tradeoff.html#cb164-11"></a>  f_hat_X[i] =<span class="st"> </span><span class="kw">predict</span>(fit_<span class="dv">2</span>, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> X[i]))</span>
<span id="cb164-12"><a href="bias-variance-tradeoff.html#cb164-12"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="bias-variance-tradeoff.html#cb165-1"></a><span class="co"># truth</span></span>
<span id="cb165-2"><a href="bias-variance-tradeoff.html#cb165-2"></a><span class="fl">0.3</span> <span class="op">^</span><span class="st"> </span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 0.09</code></pre>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="bias-variance-tradeoff.html#cb167-1"></a><span class="co"># via simulation</span></span>
<span id="cb167-2"><a href="bias-variance-tradeoff.html#cb167-2"></a><span class="kw">mean</span>((Y <span class="op">-</span><span class="st"> </span>f_hat_X) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.09566445</code></pre>
<p>Note that in practice, we should use many more simulations in this study.</p>
</div>
<div id="estimating-expected-prediction-error" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Estimating Expected Prediction Error</h2>
<p>While previously, we only decomposed the expected prediction error conditionally, a similar argument holds unconditionally.</p>
<p>Assuming</p>
<p><span class="math display">\[
\mathbb{V}[Y \mid X = x] = \sigma ^ 2.
\]</span></p>
<p>we have</p>
<p><span class="math display">\[
\text{EPE}\left(Y, \hat{f}(X)\right) = 
\mathbb{E}_{X, Y, \mathcal{D}} \left[  (Y - \hat{f}(X))^2 \right] = 
\underbrace{\mathbb{E}_{X} \left[\text{bias}^2\left(\hat{f}(X)\right)\right] + \mathbb{E}_{X} \left[\text{var}\left(\hat{f}(X)\right)\right]}_\textrm{reducible error} + \sigma^2
\]</span></p>
<p>Lastly, we note that if</p>
<p><span class="math display">\[
\mathcal{D} = \mathcal{D}_{\texttt{trn}} \cup \mathcal{D}_{\texttt{tst}} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}, \ i = 1, 2, \ldots n
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\mathcal{D}_{\texttt{trn}} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}, \ i \in \texttt{trn}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathcal{D}_{\texttt{tst}} = (x_i, y_i) \in \mathbb{R}^p \times \mathbb{R}, \ i \in \texttt{tst}
\]</span></p>
<p>Then, if we have a model fit to the training data <span class="math inline">\(\mathcal{D}_{\texttt{trn}}\)</span>, we can use the test mean squared error</p>
<p><span class="math display">\[
\sum_{i \in \texttt{tst}}\left(y_i - \hat{f}(x_i)\right) ^ 2
\]</span></p>
<p>as an estimate of</p>
<p><span class="math display">\[
\mathbb{E}_{X, Y, \mathcal{D}} \left[  (Y - \hat{f}(X))^2 \right]
\]</span></p>
<p>the expected prediction error.<a href="#fn66" class="footnote-ref" id="fnref66"><sup>66</sup></a></p>
<p>How good is this estimate? Well, if <span class="math inline">\(\mathcal{D}\)</span> is a random sample from <span class="math inline">\((X, Y)\)</span>, and the <span class="math inline">\(\texttt{tst}\)</span> data are randomly sampled observations randomly sampled from <span class="math inline">\(i = 1, 2, \ldots, n\)</span>, then it is a reasonable estimate. However, it is rather variable due to the randomness of selecting the observations for the test set, if the test set is small.</p>
</div>
<div id="model-flexibility" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Model Flexibility</h2>
<p>Let’s return to the dataset we saw in the linear regression chapter with a single feature <span class="math inline">\(x\)</span>.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="bias-variance-tradeoff.html#cb169-1"></a><span class="co"># define regression function</span></span>
<span id="cb169-2"><a href="bias-variance-tradeoff.html#cb169-2"></a>cubic_mean =<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb169-3"><a href="bias-variance-tradeoff.html#cb169-3"></a>  <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>x <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>x <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">5</span> <span class="op">*</span><span class="st"> </span>x <span class="op">^</span><span class="st"> </span><span class="dv">3</span></span>
<span id="cb169-4"><a href="bias-variance-tradeoff.html#cb169-4"></a>}</span></code></pre></div>
<p>Recall that this data was generated with a cubic mean function.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="bias-variance-tradeoff.html#cb170-1"></a><span class="co"># define full data generating process</span></span>
<span id="cb170-2"><a href="bias-variance-tradeoff.html#cb170-2"></a>gen_slr_data =<span class="st"> </span><span class="cf">function</span>(<span class="dt">sample_size =</span> <span class="dv">100</span>, mu) {</span>
<span id="cb170-3"><a href="bias-variance-tradeoff.html#cb170-3"></a>  x =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> sample_size, <span class="dt">min =</span> <span class="dv">-1</span>, <span class="dt">max =</span> <span class="dv">1</span>)</span>
<span id="cb170-4"><a href="bias-variance-tradeoff.html#cb170-4"></a>  y =<span class="st"> </span><span class="kw">mu</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> sample_size)</span>
<span id="cb170-5"><a href="bias-variance-tradeoff.html#cb170-5"></a>  <span class="kw">tibble</span>(x, y)</span>
<span id="cb170-6"><a href="bias-variance-tradeoff.html#cb170-6"></a>}</span></code></pre></div>
<p>After defining the data generating process, we generate and split the data.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="bias-variance-tradeoff.html#cb171-1"></a><span class="co"># simulate entire dataset</span></span>
<span id="cb171-2"><a href="bias-variance-tradeoff.html#cb171-2"></a><span class="kw">set.seed</span>(<span class="dv">3</span>)</span>
<span id="cb171-3"><a href="bias-variance-tradeoff.html#cb171-3"></a>sim_slr_data =<span class="st"> </span><span class="kw">gen_slr_data</span>(<span class="dt">sample_size =</span> <span class="dv">100</span>, <span class="dt">mu =</span> cubic_mean)</span>
<span id="cb171-4"><a href="bias-variance-tradeoff.html#cb171-4"></a></span>
<span id="cb171-5"><a href="bias-variance-tradeoff.html#cb171-5"></a><span class="co"># test-train split</span></span>
<span id="cb171-6"><a href="bias-variance-tradeoff.html#cb171-6"></a>slr_trn_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(sim_slr_data), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(sim_slr_data))</span>
<span id="cb171-7"><a href="bias-variance-tradeoff.html#cb171-7"></a>slr_trn =<span class="st"> </span>sim_slr_data[slr_trn_idx, ]</span>
<span id="cb171-8"><a href="bias-variance-tradeoff.html#cb171-8"></a>slr_tst =<span class="st"> </span>sim_slr_data[<span class="op">-</span>slr_trn_idx, ]</span>
<span id="cb171-9"><a href="bias-variance-tradeoff.html#cb171-9"></a></span>
<span id="cb171-10"><a href="bias-variance-tradeoff.html#cb171-10"></a><span class="co"># estimation-validation split</span></span>
<span id="cb171-11"><a href="bias-variance-tradeoff.html#cb171-11"></a>slr_est_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(slr_trn), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(slr_trn))</span>
<span id="cb171-12"><a href="bias-variance-tradeoff.html#cb171-12"></a>slr_est =<span class="st"> </span>slr_trn[slr_est_idx, ]</span>
<span id="cb171-13"><a href="bias-variance-tradeoff.html#cb171-13"></a>slr_val =<span class="st"> </span>slr_trn[<span class="op">-</span>slr_est_idx, ]</span>
<span id="cb171-14"><a href="bias-variance-tradeoff.html#cb171-14"></a></span>
<span id="cb171-15"><a href="bias-variance-tradeoff.html#cb171-15"></a><span class="co"># check data</span></span>
<span id="cb171-16"><a href="bias-variance-tradeoff.html#cb171-16"></a><span class="kw">head</span>(slr_trn, <span class="dt">n =</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## # A tibble: 10 x 2
##         x      y
##     &lt;dbl&gt;  &lt;dbl&gt;
##  1  0.573 -1.18 
##  2  0.807  0.576
##  3  0.272 -0.973
##  4 -0.813 -1.78 
##  5 -0.161  0.833
##  6  0.736  1.07 
##  7 -0.242  2.97 
##  8  0.520 -1.64 
##  9 -0.664  0.269
## 10 -0.777 -2.02</code></pre>
<p>For validating models, we will use RMSE.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="bias-variance-tradeoff.html#cb173-1"></a><span class="co"># helper function for calculating RMSE</span></span>
<span id="cb173-2"><a href="bias-variance-tradeoff.html#cb173-2"></a>calc_rmse =<span class="st"> </span><span class="cf">function</span>(actual, predicted) {</span>
<span id="cb173-3"><a href="bias-variance-tradeoff.html#cb173-3"></a>  <span class="kw">sqrt</span>(<span class="kw">mean</span>((actual <span class="op">-</span><span class="st"> </span>predicted) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</span>
<span id="cb173-4"><a href="bias-variance-tradeoff.html#cb173-4"></a>}</span></code></pre></div>
<p>Let’s check how linear, k-nearest neighbors, and decision tree models fit to this data make errors, while paying attention to their flexibility.</p>
<p><img src="bias-variance-tradeoff_files/figure-html/error-vs-flex-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>This picture is an idealized version of what we expect to see, but we’ll illustrate the sorts of validate “curves” that we might see in practice.</p>
<p>Note that in the following three sub-sections, a significant portion of the code is suppressed for visual clarity. See the source document for full details.</p>
<div id="linear-models" class="section level3" number="4.7.1">
<h3><span class="header-section-number">4.7.1</span> Linear Models</h3>
<p>First up, linear models. We will fit polynomial models with degree from one to nine, and then validate.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="bias-variance-tradeoff.html#cb174-1"></a><span class="co"># fit polynomial models</span></span>
<span id="cb174-2"><a href="bias-variance-tradeoff.html#cb174-2"></a>poly_mod_est_list =<span class="st"> </span><span class="kw">list</span>(</span>
<span id="cb174-3"><a href="bias-variance-tradeoff.html#cb174-3"></a>  <span class="dt">poly_mod_1_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">1</span>), <span class="dt">data =</span> slr_est),</span>
<span id="cb174-4"><a href="bias-variance-tradeoff.html#cb174-4"></a>  <span class="dt">poly_mod_2_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">2</span>), <span class="dt">data =</span> slr_est),</span>
<span id="cb174-5"><a href="bias-variance-tradeoff.html#cb174-5"></a>  <span class="dt">poly_mod_3_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">3</span>), <span class="dt">data =</span> slr_est),</span>
<span id="cb174-6"><a href="bias-variance-tradeoff.html#cb174-6"></a>  <span class="dt">poly_mod_4_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">4</span>), <span class="dt">data =</span> slr_est),</span>
<span id="cb174-7"><a href="bias-variance-tradeoff.html#cb174-7"></a>  <span class="dt">poly_mod_5_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">5</span>), <span class="dt">data =</span> slr_est),</span>
<span id="cb174-8"><a href="bias-variance-tradeoff.html#cb174-8"></a>  <span class="dt">poly_mod_6_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">6</span>), <span class="dt">data =</span> slr_est),</span>
<span id="cb174-9"><a href="bias-variance-tradeoff.html#cb174-9"></a>  <span class="dt">poly_mod_7_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">7</span>), <span class="dt">data =</span> slr_est),</span>
<span id="cb174-10"><a href="bias-variance-tradeoff.html#cb174-10"></a>  <span class="dt">poly_mod_8_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">8</span>), <span class="dt">data =</span> slr_est),</span>
<span id="cb174-11"><a href="bias-variance-tradeoff.html#cb174-11"></a>  <span class="dt">poly_mod_9_est =</span> <span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(x, <span class="dt">degree =</span> <span class="dv">9</span>), <span class="dt">data =</span> slr_est)</span>
<span id="cb174-12"><a href="bias-variance-tradeoff.html#cb174-12"></a>)</span></code></pre></div>
<p>The plot below visualizes the results.</p>
<p><img src="bias-variance-tradeoff_files/figure-html/unnamed-chunk-34-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>What do we see here? As the polynomial degree <em>increases</em>:</p>
<ul>
<li>The training error <em>decreases</em>.</li>
<li>The validation error <em>decreases</em>, then <em>increases</em>.</li>
</ul>
<p>This more of less matches the idealized version above, but the validation “curve” is much more jagged. This is something that we can expect in practice.</p>
<p>We have previously noted that training error isn’t particularly useful for validating models. That is still true. However, it can be useful for checking that everything is working as planned. In this case, since we known that training error decreases as model flexibility increases, we can verify our intuition that a higher degree polynomial is indeed more flexible.<a href="#fn67" class="footnote-ref" id="fnref67"><sup>67</sup></a></p>
</div>
<div id="k-nearest-neighbors-1" class="section level3" number="4.7.2">
<h3><span class="header-section-number">4.7.2</span> k-Nearest Neighbors</h3>
<p>Next up, k-nearest neighbors. We will consider values for <span class="math inline">\(k\)</span> that are odd and between <span class="math inline">\(1\)</span> and <span class="math inline">\(45\)</span> inclusive.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="bias-variance-tradeoff.html#cb175-1"></a><span class="co"># helper function for fitting knn models</span></span>
<span id="cb175-2"><a href="bias-variance-tradeoff.html#cb175-2"></a>fit_knn_mod =<span class="st"> </span><span class="cf">function</span>(neighbors) {</span>
<span id="cb175-3"><a href="bias-variance-tradeoff.html#cb175-3"></a>  <span class="kw">knnreg</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> slr_est, <span class="dt">k =</span> neighbors)</span>
<span id="cb175-4"><a href="bias-variance-tradeoff.html#cb175-4"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="bias-variance-tradeoff.html#cb176-1"></a><span class="co"># define values of tuning parameter k to evaluate</span></span>
<span id="cb176-2"><a href="bias-variance-tradeoff.html#cb176-2"></a>k_to_try =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">1</span>, <span class="dt">to =</span> <span class="dv">45</span>, <span class="dt">by =</span> <span class="dv">2</span>)</span>
<span id="cb176-3"><a href="bias-variance-tradeoff.html#cb176-3"></a></span>
<span id="cb176-4"><a href="bias-variance-tradeoff.html#cb176-4"></a><span class="co"># fit knn models</span></span>
<span id="cb176-5"><a href="bias-variance-tradeoff.html#cb176-5"></a>knn_mod_est_list =<span class="st"> </span><span class="kw">lapply</span>(k_to_try, fit_knn_mod)</span></code></pre></div>
<p>The plot below visualizes the results.</p>
<p><img src="bias-variance-tradeoff_files/figure-html/unnamed-chunk-38-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Here we see the “opposite” of the usual plot. Why? Because with k-nearest neighbors, a small value of <span class="math inline">\(k\)</span> generates a flexible model compared to larger values of <span class="math inline">\(k\)</span>. So visually, this plot is flipped. That is we see that as <span class="math inline">\(k\)</span> <em>increases</em>:</p>
<ul>
<li>The training error <em>increases</em>.</li>
<li>The validation error <em>decreases</em>, then <em>increases</em>.</li>
</ul>
<p>Important to note here: the pattern above only holds “in general,” that is, there can be minor deviations in the validation pattern along the way. This is due to the random nature of selection the data for the validate set.</p>
</div>
<div id="decision-trees-1" class="section level3" number="4.7.3">
<h3><span class="header-section-number">4.7.3</span> Decision Trees</h3>
<p>Lastly, we evaluate some decision tree models. We choose some arbitrary values of <code>cp</code> to evaluate, while holding <code>minsplit</code> constant at <code>5</code>. There are arbitrary choices that produce a plot that is useful for discussion.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="bias-variance-tradeoff.html#cb177-1"></a><span class="co"># helper function for fitting decision tree models</span></span>
<span id="cb177-2"><a href="bias-variance-tradeoff.html#cb177-2"></a>tree_knn_mod =<span class="st"> </span><span class="cf">function</span>(flex) {</span>
<span id="cb177-3"><a href="bias-variance-tradeoff.html#cb177-3"></a>  <span class="kw">rpart</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> slr_est, <span class="dt">cp =</span> flex, <span class="dt">minsplit =</span> <span class="dv">5</span>)</span>
<span id="cb177-4"><a href="bias-variance-tradeoff.html#cb177-4"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="bias-variance-tradeoff.html#cb178-1"></a><span class="co"># define values of tuning parameter cp to evaluate</span></span>
<span id="cb178-2"><a href="bias-variance-tradeoff.html#cb178-2"></a>cp_to_try =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>)</span>
<span id="cb178-3"><a href="bias-variance-tradeoff.html#cb178-3"></a></span>
<span id="cb178-4"><a href="bias-variance-tradeoff.html#cb178-4"></a><span class="co"># fit decision tree models</span></span>
<span id="cb178-5"><a href="bias-variance-tradeoff.html#cb178-5"></a>tree_mod_est_list =<span class="st"> </span><span class="kw">lapply</span>(cp_to_try, tree_knn_mod)</span></code></pre></div>
<p>The plot below visualizes the results.</p>
<p><img src="bias-variance-tradeoff_files/figure-html/unnamed-chunk-42-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Based on this plot, how is <code>cp</code> related to model flexibility?<a href="#fn68" class="footnote-ref" id="fnref68"><sup>68</sup></a></p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="63">
<li id="fn63"><p>Note that in this chapter, we will refer to <span class="math inline">\(f(x)\)</span> as the regression function instead of <span class="math inline">\(\mu(x)\)</span> for unimportant and arbitrary reasons.<a href="bias-variance-tradeoff.html#fnref63" class="footnote-back">↩︎</a></p></li>
<li id="fn64"><p>Someday, someone will tell you this is a lie. They aren’t wrong. In modern deep learning, there is a concept called <a href="https://openai.com/blog/deep-double-descent/">Deep Double Descent</a>. See also <span class="citation">Mikhail Belkin et al., “Reconciling Modern Machine Learning Practice and the Bias-Variance Trade-Off,” <em>arXiv Preprint arXiv:1812.11118</em>, 2018, <a href="https://arxiv.org/abs/1812.11118" role="doc-biblioref">https://arxiv.org/abs/1812.11118</a></span>.<a href="bias-variance-tradeoff.html#fnref64" class="footnote-back">↩︎</a></p></li>
<li id="fn65"><p>It’s actually the same as the 0 predictor linear model. Can you see why?<a href="bias-variance-tradeoff.html#fnref65" class="footnote-back">↩︎</a></p></li>
<li id="fn66"><p>In practice we prefer RMSE to MSE for comparing models and reporting because of the units.<a href="bias-variance-tradeoff.html#fnref66" class="footnote-back">↩︎</a></p></li>
<li id="fn67"><p>In practice, if you already know how your model’s flexibility works, by checking that the training error goes down as you increase flexibility, you can check that you have done your coding and model training correctly.<a href="bias-variance-tradeoff.html#fnref67" class="footnote-back">↩︎</a></p></li>
<li id="fn68"><p>As <code>cp</code> increases, model flexibility decreases.<a href="bias-variance-tradeoff.html#fnref68" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="nonparametric-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-overview.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/daviddalpiaz/bsl/edit/master/bias-variance-tradeoff.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
