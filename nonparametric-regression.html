<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Nonparametric Regression | Basics of Statistical Learning</title>
  <meta name="description" content="Chapter 3 Nonparametric Regression | Basics of Statistical Learning" />
  <meta name="generator" content="bookdown 0.21.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Nonparametric Regression | Basics of Statistical Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://statisticallearning.org/" />
  
  
  <meta name="github-repo" content="daviddalpiaz/bsl" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Nonparametric Regression | Basics of Statistical Learning" />
  
  
  

<meta name="author" content="David Dalpiaz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-regression.html"/>
<link rel="next" href="bias-variance-tradeoff.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Basics of Statistical Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#who"><i class="fa fa-check"></i><b>0.1</b> Who?</a>
<ul>
<li class="chapter" data-level="0.1.1" data-path="index.html"><a href="index.html#readers"><i class="fa fa-check"></i><b>0.1.1</b> Readers</a></li>
<li class="chapter" data-level="0.1.2" data-path="index.html"><a href="index.html#author"><i class="fa fa-check"></i><b>0.1.2</b> Author</a></li>
<li class="chapter" data-level="0.1.3" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i><b>0.1.3</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#what"><i class="fa fa-check"></i><b>0.2</b> What?</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#why"><i class="fa fa-check"></i><b>0.3</b> Why?</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#where"><i class="fa fa-check"></i><b>0.4</b> Where?</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#when"><i class="fa fa-check"></i><b>0.5</b> When?</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#how"><i class="fa fa-check"></i><b>0.6</b> How?</a>
<ul>
<li class="chapter" data-level="0.6.1" data-path="index.html"><a href="index.html#build-tools"><i class="fa fa-check"></i><b>0.6.1</b> Build Tools</a></li>
<li class="chapter" data-level="0.6.2" data-path="index.html"><a href="index.html#active-development"><i class="fa fa-check"></i><b>0.6.2</b> Active Development</a></li>
<li class="chapter" data-level="0.6.3" data-path="index.html"><a href="index.html#packages"><i class="fa fa-check"></i><b>0.6.3</b> Packages</a></li>
<li class="chapter" data-level="0.6.4" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>0.6.4</b> License</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="ml-overview.html"><a href="ml-overview.html"><i class="fa fa-check"></i><b>1</b> Machine Learning Overview</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ml-overview.html"><a href="ml-overview.html#what-is-machine-learning"><i class="fa fa-check"></i><b>1.1</b> What is Machine Learning?</a></li>
<li class="chapter" data-level="1.2" data-path="ml-overview.html"><a href="ml-overview.html#machine-learning-tasks"><i class="fa fa-check"></i><b>1.2</b> Machine Learning Tasks</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="ml-overview.html"><a href="ml-overview.html#supervised-learning"><i class="fa fa-check"></i><b>1.2.1</b> Supervised Learning</a></li>
<li class="chapter" data-level="1.2.2" data-path="ml-overview.html"><a href="ml-overview.html#unsupervised-learning"><i class="fa fa-check"></i><b>1.2.2</b> Unsupervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="ml-overview.html"><a href="ml-overview.html#open-questions"><i class="fa fa-check"></i><b>1.3</b> Open Questions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>2</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="linear-regression.html"><a href="linear-regression.html#r-setup-and-source"><i class="fa fa-check"></i><b>2.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="2.2" data-path="linear-regression.html"><a href="linear-regression.html#explanation-versus-prediction"><i class="fa fa-check"></i><b>2.2</b> Explanation versus Prediction</a></li>
<li class="chapter" data-level="2.3" data-path="linear-regression.html"><a href="linear-regression.html#task-setup"><i class="fa fa-check"></i><b>2.3</b> Task Setup</a></li>
<li class="chapter" data-level="2.4" data-path="linear-regression.html"><a href="linear-regression.html#mathematical-setup"><i class="fa fa-check"></i><b>2.4</b> Mathematical Setup</a></li>
<li class="chapter" data-level="2.5" data-path="linear-regression.html"><a href="linear-regression.html#linear-regression-models"><i class="fa fa-check"></i><b>2.5</b> Linear Regression Models</a></li>
<li class="chapter" data-level="2.6" data-path="linear-regression.html"><a href="linear-regression.html#using-lm"><i class="fa fa-check"></i><b>2.6</b> Using <code>lm()</code></a></li>
<li class="chapter" data-level="2.7" data-path="linear-regression.html"><a href="linear-regression.html#the-predict-function"><i class="fa fa-check"></i><b>2.7</b> The <code>predict()</code> Function</a></li>
<li class="chapter" data-level="2.8" data-path="linear-regression.html"><a href="linear-regression.html#data-splitting"><i class="fa fa-check"></i><b>2.8</b> Data Splitting</a></li>
<li class="chapter" data-level="2.9" data-path="linear-regression.html"><a href="linear-regression.html#regression-metrics"><i class="fa fa-check"></i><b>2.9</b> Regression Metrics</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="linear-regression.html"><a href="linear-regression.html#graphical-evaluation"><i class="fa fa-check"></i><b>2.9.1</b> Graphical Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="linear-regression.html"><a href="linear-regression.html#example-simple-simulated-data"><i class="fa fa-check"></i><b>2.10</b> Example: “Simple” Simulated Data</a></li>
<li class="chapter" data-level="2.11" data-path="linear-regression.html"><a href="linear-regression.html#example-diamonds-data"><i class="fa fa-check"></i><b>2.11</b> Example: Diamonds Data</a></li>
<li class="chapter" data-level="2.12" data-path="linear-regression.html"><a href="linear-regression.html#example-credit-card-data"><i class="fa fa-check"></i><b>2.12</b> Example: Credit Card Data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html"><i class="fa fa-check"></i><b>3</b> Nonparametric Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#r-setup-and-source-1"><i class="fa fa-check"></i><b>3.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="3.2" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#mathematical-setup-1"><i class="fa fa-check"></i><b>3.2</b> Mathematical Setup</a></li>
<li class="chapter" data-level="3.3" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>3.3</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="3.4" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#decision-trees"><i class="fa fa-check"></i><b>3.4</b> Decision Trees</a></li>
<li class="chapter" data-level="3.5" data-path="nonparametric-regression.html"><a href="nonparametric-regression.html#example-credit-card-data-1"><i class="fa fa-check"></i><b>3.5</b> Example: Credit Card Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html"><i class="fa fa-check"></i><b>4</b> The Bias–Variance Tradeoff</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#r-setup-and-source-2"><i class="fa fa-check"></i><b>4.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="4.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#the-regression-setup"><i class="fa fa-check"></i><b>4.2</b> The Regression Setup</a></li>
<li class="chapter" data-level="4.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#reducible-and-irreducible-error"><i class="fa fa-check"></i><b>4.3</b> Reducible and Irreducible Error</a></li>
<li class="chapter" data-level="4.4" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#bias-variance-decomposition"><i class="fa fa-check"></i><b>4.4</b> Bias-Variance Decomposition</a></li>
<li class="chapter" data-level="4.5" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#using-simulation-to-estimate-bias-and-variance"><i class="fa fa-check"></i><b>4.5</b> Using Simulation to Estimate Bias and Variance</a></li>
<li class="chapter" data-level="4.6" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#estimating-expected-prediction-error"><i class="fa fa-check"></i><b>4.6</b> Estimating Expected Prediction Error</a></li>
<li class="chapter" data-level="4.7" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#model-flexibility"><i class="fa fa-check"></i><b>4.7</b> Model Flexibility</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#linear-models"><i class="fa fa-check"></i><b>4.7.1</b> Linear Models</a></li>
<li class="chapter" data-level="4.7.2" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#k-nearest-neighbors-1"><i class="fa fa-check"></i><b>4.7.2</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.7.3" data-path="bias-variance-tradeoff.html"><a href="bias-variance-tradeoff.html#decision-trees-1"><i class="fa fa-check"></i><b>4.7.3</b> Decision Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-overview.html"><a href="regression-overview.html"><i class="fa fa-check"></i><b>5</b> Regression Overview</a>
<ul>
<li class="chapter" data-level="5.1" data-path="regression-overview.html"><a href="regression-overview.html#the-goal"><i class="fa fa-check"></i><b>5.1</b> The Goal</a></li>
<li class="chapter" data-level="5.2" data-path="regression-overview.html"><a href="regression-overview.html#general-strategy"><i class="fa fa-check"></i><b>5.2</b> General Strategy</a></li>
<li class="chapter" data-level="5.3" data-path="regression-overview.html"><a href="regression-overview.html#aglorithms"><i class="fa fa-check"></i><b>5.3</b> Aglorithms</a></li>
<li class="chapter" data-level="5.4" data-path="regression-overview.html"><a href="regression-overview.html#model-flexibility-1"><i class="fa fa-check"></i><b>5.4</b> Model Flexibility</a></li>
<li class="chapter" data-level="5.5" data-path="regression-overview.html"><a href="regression-overview.html#overfitting"><i class="fa fa-check"></i><b>5.5</b> Overfitting</a></li>
<li class="chapter" data-level="5.6" data-path="regression-overview.html"><a href="regression-overview.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.6</b> Bias-Variance Tradeoff</a></li>
<li class="chapter" data-level="5.7" data-path="regression-overview.html"><a href="regression-overview.html#no-free-lunch"><i class="fa fa-check"></i><b>5.7</b> No Free Lunch</a></li>
<li class="chapter" data-level="5.8" data-path="regression-overview.html"><a href="regression-overview.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>5.8</b> Curse of Dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>6</b> Classification</a>
<ul>
<li class="chapter" data-level="6.1" data-path="classification.html"><a href="classification.html#r-setup-and-source-3"><i class="fa fa-check"></i><b>6.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="6.2" data-path="classification.html"><a href="classification.html#data-setup"><i class="fa fa-check"></i><b>6.2</b> Data Setup</a></li>
<li class="chapter" data-level="6.3" data-path="classification.html"><a href="classification.html#mathematical-setup-2"><i class="fa fa-check"></i><b>6.3</b> Mathematical Setup</a></li>
<li class="chapter" data-level="6.4" data-path="classification.html"><a href="classification.html#example"><i class="fa fa-check"></i><b>6.4</b> Example</a></li>
<li class="chapter" data-level="6.5" data-path="classification.html"><a href="classification.html#bayes-classifier"><i class="fa fa-check"></i><b>6.5</b> Bayes Classifier</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="classification.html"><a href="classification.html#bayes-error-rate"><i class="fa fa-check"></i><b>6.5.1</b> Bayes Error Rate</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="classification.html"><a href="classification.html#building-a-classifier"><i class="fa fa-check"></i><b>6.6</b> Building a Classifier</a></li>
<li class="chapter" data-level="6.7" data-path="classification.html"><a href="classification.html#modeling"><i class="fa fa-check"></i><b>6.7</b> Modeling</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="classification.html"><a href="classification.html#linear-models-3"><i class="fa fa-check"></i><b>6.7.1</b> Linear Models</a></li>
<li class="chapter" data-level="6.7.2" data-path="classification.html"><a href="classification.html#k-nearest-neighbors-4"><i class="fa fa-check"></i><b>6.7.2</b> k-Nearest Neighbors</a></li>
<li class="chapter" data-level="6.7.3" data-path="classification.html"><a href="classification.html#decision-trees-3"><i class="fa fa-check"></i><b>6.7.3</b> Decision Trees</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="classification.html"><a href="classification.html#classification-metrics"><i class="fa fa-check"></i><b>6.8</b> Classification Metrics</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="classification.html"><a href="classification.html#misclassification"><i class="fa fa-check"></i><b>6.8.1</b> Misclassification</a></li>
<li class="chapter" data-level="6.8.2" data-path="classification.html"><a href="classification.html#accuracy"><i class="fa fa-check"></i><b>6.8.2</b> Accuracy</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html"><i class="fa fa-check"></i><b>7</b> Nonparametric Classification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html#example-knn-on-simulated-data"><i class="fa fa-check"></i><b>7.1</b> Example: KNN on Simulated Data</a></li>
<li class="chapter" data-level="7.2" data-path="nonparametric-classification.html"><a href="nonparametric-classification.html#example-decision-tree-on-penguin-data"><i class="fa fa-check"></i><b>7.2</b> Example: Decision Tree on Penguin Data</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>8</b> Logistic Regression</a></li>
<li class="chapter" data-level="9" data-path="binary-classification.html"><a href="binary-classification.html"><i class="fa fa-check"></i><b>9</b> Binary Classification</a>
<ul>
<li class="chapter" data-level="9.1" data-path="binary-classification.html"><a href="binary-classification.html#r-setup-and-source-4"><i class="fa fa-check"></i><b>9.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="9.2" data-path="binary-classification.html"><a href="binary-classification.html#breast-cancer-data"><i class="fa fa-check"></i><b>9.2</b> Breast Cancer Data</a></li>
<li class="chapter" data-level="9.3" data-path="binary-classification.html"><a href="binary-classification.html#confusion-matrix"><i class="fa fa-check"></i><b>9.3</b> Confusion Matrix</a></li>
<li class="chapter" data-level="9.4" data-path="binary-classification.html"><a href="binary-classification.html#binary-classification-metrics"><i class="fa fa-check"></i><b>9.4</b> Binary Classification Metrics</a></li>
<li class="chapter" data-level="9.5" data-path="binary-classification.html"><a href="binary-classification.html#probability-cutoff"><i class="fa fa-check"></i><b>9.5</b> Probability Cutoff</a></li>
<li class="chapter" data-level="9.6" data-path="binary-classification.html"><a href="binary-classification.html#r-packages-and-function"><i class="fa fa-check"></i><b>9.6</b> R Packages and Function</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="generative.html"><a href="generative.html"><i class="fa fa-check"></i><b>10</b> Generative Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="generative.html"><a href="generative.html#r-setup-and-source-5"><i class="fa fa-check"></i><b>10.1</b> R Setup and Source</a></li>
<li class="chapter" data-level="10.2" data-path="generative.html"><a href="generative.html#linear-discriminant-analysis"><i class="fa fa-check"></i><b>10.2</b> Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="10.3" data-path="generative.html"><a href="generative.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i><b>10.3</b> Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="10.4" data-path="generative.html"><a href="generative.html#naive-bayes"><i class="fa fa-check"></i><b>10.4</b> Naive Bayes</a></li>
<li class="chapter" data-level="10.5" data-path="generative.html"><a href="generative.html#categorical-features"><i class="fa fa-check"></i><b>10.5</b> Categorical Features</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="cross-validation.html"><a href="cross-validation.html"><i class="fa fa-check"></i><b>11</b> Cross-Validation</a></li>
<li class="chapter" data-level="12" data-path="regularization.html"><a href="regularization.html"><i class="fa fa-check"></i><b>12</b> Regularization</a>
<ul>
<li class="chapter" data-level="12.1" data-path="regularization.html"><a href="regularization.html#ridge-regression"><i class="fa fa-check"></i><b>12.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="12.2" data-path="regularization.html"><a href="regularization.html#lasso"><i class="fa fa-check"></i><b>12.2</b> Lasso</a></li>
<li class="chapter" data-level="12.3" data-path="regularization.html"><a href="regularization.html#broom"><i class="fa fa-check"></i><b>12.3</b> <code>broom</code></a></li>
<li class="chapter" data-level="12.4" data-path="regularization.html"><a href="regularization.html#simulated-data-p-n"><i class="fa fa-check"></i><b>12.4</b> Simulated Data, <span class="math inline">\(p &gt; n\)</span></a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="ensemble-methods.html"><a href="ensemble-methods.html"><i class="fa fa-check"></i><b>13</b> Ensemble Methods</a>
<ul>
<li class="chapter" data-level="13.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#bagging"><i class="fa fa-check"></i><b>13.1</b> Bagging</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#reading"><i class="fa fa-check"></i><b>13.1.1</b> Reading</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#random-forest"><i class="fa fa-check"></i><b>13.2</b> Random Forest</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#reading-1"><i class="fa fa-check"></i><b>13.2.1</b> Reading</a></li>
<li class="chapter" data-level="13.2.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#video"><i class="fa fa-check"></i><b>13.2.2</b> Video</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="ensemble-methods.html"><a href="ensemble-methods.html#boosting"><i class="fa fa-check"></i><b>13.3</b> Boosting</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="ensemble-methods.html"><a href="ensemble-methods.html#reading-2"><i class="fa fa-check"></i><b>13.3.1</b> Reading</a></li>
<li class="chapter" data-level="13.3.2" data-path="ensemble-methods.html"><a href="ensemble-methods.html#video-1"><i class="fa fa-check"></i><b>13.3.2</b> Video</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="supervised-overview.html"><a href="supervised-overview.html"><i class="fa fa-check"></i><b>14</b> Supervised Learning Overview</a>
<ul>
<li class="chapter" data-level="14.1" data-path="supervised-overview.html"><a href="supervised-overview.html#classification-2"><i class="fa fa-check"></i><b>14.1</b> Classification</a>
<ul>
<li class="chapter" data-level="14.1.1" data-path="supervised-overview.html"><a href="supervised-overview.html#tuning"><i class="fa fa-check"></i><b>14.1.1</b> Tuning</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="supervised-overview.html"><a href="supervised-overview.html#regression-1"><i class="fa fa-check"></i><b>14.2</b> Regression</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="supervised-overview.html"><a href="supervised-overview.html#methods"><i class="fa fa-check"></i><b>14.2.1</b> Methods</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="supervised-overview.html"><a href="supervised-overview.html#external-links"><i class="fa fa-check"></i><b>14.3</b> External Links</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="additional-reading.html"><a href="additional-reading.html"><i class="fa fa-check"></i><b>A</b> Additional Reading</a>
<ul>
<li class="chapter" data-level="A.1" data-path="additional-reading.html"><a href="additional-reading.html#books"><i class="fa fa-check"></i><b>A.1</b> Books</a></li>
<li class="chapter" data-level="A.2" data-path="additional-reading.html"><a href="additional-reading.html#papers"><i class="fa fa-check"></i><b>A.2</b> Papers</a></li>
<li class="chapter" data-level="A.3" data-path="additional-reading.html"><a href="additional-reading.html#blog-posts"><i class="fa fa-check"></i><b>A.3</b> Blog Posts</a></li>
<li class="chapter" data-level="A.4" data-path="additional-reading.html"><a href="additional-reading.html#miscellaneous"><i class="fa fa-check"></i><b>A.4</b> Miscellaneous</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="computing.html"><a href="computing.html"><i class="fa fa-check"></i><b>B</b> Computing</a>
<ul>
<li class="chapter" data-level="B.1" data-path="computing.html"><a href="computing.html#reading-3"><i class="fa fa-check"></i><b>B.1</b> Reading</a></li>
<li class="chapter" data-level="B.2" data-path="computing.html"><a href="computing.html#additional-resources"><i class="fa fa-check"></i><b>B.2</b> Additional Resources</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="computing.html"><a href="computing.html#r"><i class="fa fa-check"></i><b>B.2.1</b> R</a></li>
<li class="chapter" data-level="B.2.2" data-path="computing.html"><a href="computing.html#rstudio"><i class="fa fa-check"></i><b>B.2.2</b> RStudio</a></li>
<li class="chapter" data-level="B.2.3" data-path="computing.html"><a href="computing.html#r-markdown"><i class="fa fa-check"></i><b>B.2.3</b> R Markdown</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="computing.html"><a href="computing.html#stat-432-idioms"><i class="fa fa-check"></i><b>B.3</b> STAT 432 Idioms</a>
<ul>
<li class="chapter" data-level="B.3.1" data-path="computing.html"><a href="computing.html#dont-restore-old-workspaces"><i class="fa fa-check"></i><b>B.3.1</b> Don’t Restore Old Workspaces</a></li>
<li class="chapter" data-level="B.3.2" data-path="computing.html"><a href="computing.html#r-versions"><i class="fa fa-check"></i><b>B.3.2</b> R Versions</a></li>
<li class="chapter" data-level="B.3.3" data-path="computing.html"><a href="computing.html#packages-1"><i class="fa fa-check"></i><b>B.3.3</b> Packages</a></li>
<li class="chapter" data-level="B.3.4" data-path="computing.html"><a href="computing.html#code-style"><i class="fa fa-check"></i><b>B.3.4</b> Code Style</a></li>
<li class="chapter" data-level="B.3.5" data-path="computing.html"><a href="computing.html#reference-style"><i class="fa fa-check"></i><b>B.3.5</b> Reference Style</a></li>
<li class="chapter" data-level="B.3.6" data-path="computing.html"><a href="computing.html#stat-432-r-style-overrides"><i class="fa fa-check"></i><b>B.3.6</b> STAT 432 R Style Overrides</a></li>
<li class="chapter" data-level="B.3.7" data-path="computing.html"><a href="computing.html#stat-432-r-markdown-style"><i class="fa fa-check"></i><b>B.3.7</b> STAT 432 R Markdown Style</a></li>
<li class="chapter" data-level="B.3.8" data-path="computing.html"><a href="computing.html#style-heuristics"><i class="fa fa-check"></i><b>B.3.8</b> Style Heuristics</a></li>
<li class="chapter" data-level="B.3.9" data-path="computing.html"><a href="computing.html#objects-and-functions"><i class="fa fa-check"></i><b>B.3.9</b> Objects and Functions</a></li>
<li class="chapter" data-level="B.3.10" data-path="computing.html"><a href="computing.html#print-versus-return"><i class="fa fa-check"></i><b>B.3.10</b> Print versus Return</a></li>
<li class="chapter" data-level="B.3.11" data-path="computing.html"><a href="computing.html#help"><i class="fa fa-check"></i><b>B.3.11</b> Help</a></li>
<li class="chapter" data-level="B.3.12" data-path="computing.html"><a href="computing.html#keyboard-shortcuts"><i class="fa fa-check"></i><b>B.3.12</b> Keyboard Shortcuts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="C" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>C</b> Probability</a>
<ul>
<li class="chapter" data-level="C.1" data-path="probability.html"><a href="probability.html#reading-4"><i class="fa fa-check"></i><b>C.1</b> Reading</a></li>
<li class="chapter" data-level="C.2" data-path="probability.html"><a href="probability.html#probability-models"><i class="fa fa-check"></i><b>C.2</b> Probability Models</a></li>
<li class="chapter" data-level="C.3" data-path="probability.html"><a href="probability.html#probability-axioms"><i class="fa fa-check"></i><b>C.3</b> Probability Axioms</a></li>
<li class="chapter" data-level="C.4" data-path="probability.html"><a href="probability.html#probability-rules"><i class="fa fa-check"></i><b>C.4</b> Probability Rules</a></li>
<li class="chapter" data-level="C.5" data-path="probability.html"><a href="probability.html#random-variables"><i class="fa fa-check"></i><b>C.5</b> Random Variables</a>
<ul>
<li class="chapter" data-level="C.5.1" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>C.5.1</b> Distributions</a></li>
<li class="chapter" data-level="C.5.2" data-path="probability.html"><a href="probability.html#discrete-random-variables"><i class="fa fa-check"></i><b>C.5.2</b> Discrete Random Variables</a></li>
<li class="chapter" data-level="C.5.3" data-path="probability.html"><a href="probability.html#continuous-random-variables"><i class="fa fa-check"></i><b>C.5.3</b> Continuous Random Variables</a></li>
<li class="chapter" data-level="C.5.4" data-path="probability.html"><a href="probability.html#distributions-in-r"><i class="fa fa-check"></i><b>C.5.4</b> Distributions in R</a></li>
<li class="chapter" data-level="C.5.5" data-path="probability.html"><a href="probability.html#several-random-variables"><i class="fa fa-check"></i><b>C.5.5</b> Several Random Variables</a></li>
</ul></li>
<li class="chapter" data-level="C.6" data-path="probability.html"><a href="probability.html#expectations"><i class="fa fa-check"></i><b>C.6</b> Expectations</a></li>
<li class="chapter" data-level="C.7" data-path="probability.html"><a href="probability.html#likelihood"><i class="fa fa-check"></i><b>C.7</b> Likelihood</a></li>
<li class="chapter" data-level="C.8" data-path="probability.html"><a href="probability.html#additional-references"><i class="fa fa-check"></i><b>C.8</b> Additional References</a>
<ul>
<li class="chapter" data-level="C.8.1" data-path="probability.html"><a href="probability.html#videos"><i class="fa fa-check"></i><b>C.8.1</b> Videos</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>D</b> Statistics</a>
<ul>
<li class="chapter" data-level="D.1" data-path="statistics.html"><a href="statistics.html#reading-5"><i class="fa fa-check"></i><b>D.1</b> Reading</a></li>
<li class="chapter" data-level="D.2" data-path="statistics.html"><a href="statistics.html#statistics-1"><i class="fa fa-check"></i><b>D.2</b> Statistics</a></li>
<li class="chapter" data-level="D.3" data-path="statistics.html"><a href="statistics.html#estimators"><i class="fa fa-check"></i><b>D.3</b> Estimators</a>
<ul>
<li class="chapter" data-level="D.3.1" data-path="statistics.html"><a href="statistics.html#properties"><i class="fa fa-check"></i><b>D.3.1</b> Properties</a></li>
<li class="chapter" data-level="D.3.2" data-path="statistics.html"><a href="statistics.html#example-mse-of-an-estimator"><i class="fa fa-check"></i><b>D.3.2</b> Example: MSE of an Estimator</a></li>
<li class="chapter" data-level="D.3.3" data-path="statistics.html"><a href="statistics.html#estimation-methods"><i class="fa fa-check"></i><b>D.3.3</b> Estimation Methods</a></li>
<li class="chapter" data-level="D.3.4" data-path="statistics.html"><a href="statistics.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>D.3.4</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="D.3.5" data-path="statistics.html"><a href="statistics.html#method-of-moments"><i class="fa fa-check"></i><b>D.3.5</b> Method of Moments</a></li>
<li class="chapter" data-level="D.3.6" data-path="statistics.html"><a href="statistics.html#empirical-distribution-function"><i class="fa fa-check"></i><b>D.3.6</b> Empirical Distribution Function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://daviddalpiaz.org" target="blank">&copy; 2020 David Dalpiaz</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Basics of Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nonparametric-regression" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Nonparametric Regression</h1>
<p>In this chapter, we will continue to explore models for making <strong>predictions</strong>, but now we will introduce <strong>nonparametric models</strong> that will contrast the <strong>parametric models</strong> that we have used previously.</p>
<p>Specifically, we will discuss:</p>
<ul>
<li>How to use <strong>k-nearest neighbors</strong> for regression through the use of the <code>knnreg()</code> function from the <code>caret</code> package</li>
<li>How to use <strong>decision trees</strong> for regression through the use of the <code>rpart()</code> function from the <code>rpart</code> package.</li>
<li>How “making predictions” can be thought of as <strong>estimating the regression function</strong>, that is, the conditional mean of the response given values of the features.</li>
<li>The difference between <strong>parametric</strong> and <strong>nonparametric</strong> methods.</li>
<li>The difference between <strong>model parameters</strong> and <strong>tuning parameters</strong> methods.</li>
<li>How these nonparametric methods deal with <strong>categorical variables</strong> and <strong>interactions</strong>.</li>
</ul>
<p>We will also hint at, but delay for one more chapter a detailed discussion of:</p>
<ul>
<li>What is <strong>model flexibility</strong>?</li>
<li>What is <strong>overfitting</strong> and how do we avoid it?</li>
</ul>
<p>This chapter is currently <strong>under construction</strong>. While it is being developed, the following links to the STAT 432 course notes.</p>
<ul>
<li><a href="files/nonparametric.pdf"><strong>Notes:</strong> Nonparametric Regression</a></li>
</ul>
<div id="r-setup-and-source-1" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> R Setup and Source</h2>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="nonparametric-regression.html#cb100-1"></a><span class="kw">library</span>(tibble)     <span class="co"># data frame printing</span></span>
<span id="cb100-2"><a href="nonparametric-regression.html#cb100-2"></a><span class="kw">library</span>(dplyr)      <span class="co"># data manipulation</span></span>
<span id="cb100-3"><a href="nonparametric-regression.html#cb100-3"></a></span>
<span id="cb100-4"><a href="nonparametric-regression.html#cb100-4"></a><span class="kw">library</span>(caret)      <span class="co"># fitting knn</span></span>
<span id="cb100-5"><a href="nonparametric-regression.html#cb100-5"></a><span class="kw">library</span>(rpart)      <span class="co"># fitting trees</span></span>
<span id="cb100-6"><a href="nonparametric-regression.html#cb100-6"></a><span class="kw">library</span>(rpart.plot) <span class="co"># plotting trees</span></span>
<span id="cb100-7"><a href="nonparametric-regression.html#cb100-7"></a></span>
<span id="cb100-8"><a href="nonparametric-regression.html#cb100-8"></a><span class="kw">library</span>(knitr)      <span class="co"># creating tables</span></span>
<span id="cb100-9"><a href="nonparametric-regression.html#cb100-9"></a><span class="kw">library</span>(kableExtra) <span class="co"># styling tables</span></span></code></pre></div>
<p>Additionally, objects from <code>ISLR</code> are accessed. Recall that the <a href="index.html">Welcome</a> chapter contains directions for installing all necessary packages for following along with the text. The R Markdown source is provided as some code, mostly for creating plots, has been suppressed from the rendered document that you are currently reading.</p>
<ul>
<li><strong>R Markdown Source:</strong> <a href="nonparametric-regression.Rmd"><code>nonparametric-regression.Rmd</code></a></li>
</ul>
</div>
<div id="mathematical-setup-1" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Mathematical Setup</h2>
<p>Let’s return to the setup we defined in the previous chapter. Consider a random variable <span class="math inline">\(Y\)</span> which represents a <strong>response</strong> variable, and <span class="math inline">\(p\)</span> <strong>feature</strong> variables <span class="math inline">\(\boldsymbol{X} = (X_1, X_2, \ldots, X_p)\)</span>. We assume that the response variable <span class="math inline">\(Y\)</span> is some function of the features, plus some random noise.</p>
<p><span class="math display">\[
Y = f(\boldsymbol{X}) + \epsilon
\]</span></p>
<p>Our goal is to find some <span class="math inline">\(f\)</span> such that <span class="math inline">\(f(\boldsymbol{X})\)</span> is close to <span class="math inline">\(Y\)</span>. More specifically we want to minimize the risk under squared error loss.</p>
<p><span class="math display">\[
\mathbb{E}_{\boldsymbol{X}, Y} \left[ (Y - f(\boldsymbol{X})) ^ 2 \right] = \mathbb{E}_{\boldsymbol{X}} \mathbb{E}_{Y \mid \boldsymbol{X}} \left[ ( Y - f(\boldsymbol{X}) ) ^ 2 \mid \boldsymbol{X} = \boldsymbol{x} \right]
\]</span></p>
<p>We saw last chapter that this risk is minimized by the <strong>conditional mean</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\boldsymbol{X}\)</span>,</p>
<p><span class="math display">\[
\mu(\boldsymbol{x}) \triangleq \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}]
\]</span></p>
<p>which we called the <strong>regression function</strong>.</p>
<p>Our goal then is to <strong>estimate</strong> this <strong>regression function</strong>. Let’s return to the example from last chapter where we know the true probability model.</p>
<p><span class="math display">\[
Y = 1 - 2x - 3x ^ 2 + 5x ^ 3 + \epsilon
\]</span></p>
<p>where <span class="math inline">\(\epsilon \sim \text{N}(0, \sigma^2)\)</span>.</p>
<p>Recall that this implies that the regression function is</p>
<p><span class="math display">\[
\mu(x) = \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}] = 1 - 2x - 3x ^ 2 + 5x ^ 3
\]</span></p>
<p>Let’s also return to pretending that we do not actually know this information, but instead have some data, <span class="math inline">\((x_i, y_i)\)</span> for <span class="math inline">\(i = 1, 2, \ldots, n\)</span>.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
x
</th>
<th style="text-align:right;">
y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-0.4689827
</td>
<td style="text-align:right;">
0.7174461
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.2557522
</td>
<td style="text-align:right;">
1.2154439
</td>
</tr>
<tr>
<td style="text-align:right;">
0.1457067
</td>
<td style="text-align:right;">
1.6041985
</td>
</tr>
<tr>
<td style="text-align:right;">
0.8164156
</td>
<td style="text-align:right;">
0.9096322
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.5966361
</td>
<td style="text-align:right;">
0.6573128
</td>
</tr>
<tr>
<td style="text-align:right;">
0.7967794
</td>
<td style="text-align:right;">
0.9500528
</td>
</tr>
<tr>
<td style="text-align:right;">
0.8893505
</td>
<td style="text-align:right;">
1.1477361
</td>
</tr>
<tr>
<td style="text-align:right;">
0.3215956
</td>
<td style="text-align:right;">
0.2874057
</td>
</tr>
<tr>
<td style="text-align:right;">
0.2582281
</td>
<td style="text-align:right;">
-1.6197576
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.8764275
</td>
<td style="text-align:right;">
-2.2977242
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.5880509
</td>
<td style="text-align:right;">
0.0658105
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.6468865
</td>
<td style="text-align:right;">
-0.4708965
</td>
</tr>
<tr>
<td style="text-align:right;">
0.3740457
</td>
<td style="text-align:right;">
-1.3769103
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.2317926
</td>
<td style="text-align:right;">
0.7619832
</td>
</tr>
<tr>
<td style="text-align:right;">
0.5396828
</td>
<td style="text-align:right;">
0.2507367
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.0046015
</td>
<td style="text-align:right;">
2.3678186
</td>
</tr>
<tr>
<td style="text-align:right;">
0.4352370
</td>
<td style="text-align:right;">
-0.1293181
</td>
</tr>
<tr>
<td style="text-align:right;">
0.9838122
</td>
<td style="text-align:right;">
1.2774803
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.2399296
</td>
<td style="text-align:right;">
1.1842963
</td>
</tr>
<tr>
<td style="text-align:right;">
0.5548904
</td>
<td style="text-align:right;">
-1.5562874
</td>
</tr>
<tr>
<td style="text-align:right;">
0.8694105
</td>
<td style="text-align:right;">
-0.1356129
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.5757150
</td>
<td style="text-align:right;">
-0.1913002
</td>
</tr>
<tr>
<td style="text-align:right;">
0.3033475
</td>
<td style="text-align:right;">
0.1975021
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.7488898
</td>
<td style="text-align:right;">
-0.1847245
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.4655587
</td>
<td style="text-align:right;">
1.5395212
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.2277718
</td>
<td style="text-align:right;">
1.0762960
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.9732193
</td>
<td style="text-align:right;">
-4.7573427
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.2352241
</td>
<td style="text-align:right;">
1.9363453
</td>
</tr>
<tr>
<td style="text-align:right;">
0.7393817
</td>
<td style="text-align:right;">
0.4588894
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.3193020
</td>
<td style="text-align:right;">
0.4812168
</td>
</tr>
</tbody>
</table>
<p>We simulated a bit more data than last time to make the “pattern” clearer to recognize.</p>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Recall that when we used a linear model, we first need to make an <strong>assumption</strong> about the form of the regression function.</p>
<p>For example, we could assume that</p>
<p><span class="math display">\[
\mu(x) = \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}] = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3
\]</span></p>
<p>which is fit in R using the <code>lm()</code> function</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="nonparametric-regression.html#cb101-1"></a><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(x <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(x <span class="op">^</span><span class="st"> </span><span class="dv">3</span>), <span class="dt">data =</span> sim_slr_data)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x + I(x^2) + I(x^3), data = sim_slr_data)
## 
## Coefficients:
## (Intercept)            x       I(x^2)       I(x^3)  
##      0.8397      -2.7257      -2.3752       6.0906</code></pre>
<p>Notice that what is returned are (maximum likelihood or least squares) estimates of the unknown <span class="math inline">\(\beta\)</span> coefficients. That is, the “learning” that takes place with a linear models is “learning” the values of the coefficients.</p>
<p>For this reason, we call linear regression models <strong>parametric</strong> models. They have unknown <strong>model parameters</strong>, in this case the <span class="math inline">\(\beta\)</span> coefficients that must be learned from the data. The form of the regression function is assumed.</p>
<p>What if we don’t want to make an assumption about the form of the regression function? While in this case, you might look at the plot and arrive at a reasonable guess of assuming a third order polynomial, what if it isn’t so clear? What if you have 100 features? Making strong assumptions might not work well.</p>
<p>Enter <strong>nonparametric</strong> models. We will consider two examples: <strong>k-nearest neighbors</strong> and <strong>decision trees</strong>.</p>
</div>
<div id="k-nearest-neighbors" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> k-Nearest Neighbors</h2>
<p>We’ll start with <strong>k-nearest neighbors</strong> which is possibly a more intuitive procedure than linear models.<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a></p>
<p>If our goal is to estimate the mean function,</p>
<p><span class="math display">\[
\mu(x) = \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}]
\]</span></p>
<p>the most natural approach would be to use</p>
<p><span class="math display">\[
\text{average}(\{ y_i : x_i = x \}).
\]</span></p>
<p>That is, to estimate the conditional mean at <span class="math inline">\(x\)</span>, average the <span class="math inline">\(y_i\)</span> values for each data point where <span class="math inline">\(x_i = x\)</span>.</p>
<p>While this sounds nice, it has an obvious flaw. For most values of <span class="math inline">\(x\)</span> there will not be any <span class="math inline">\(x_i\)</span> in the data where <span class="math inline">\(x_i = x\)</span>!</p>
<p>So what’s the next best thing? Pick values of <span class="math inline">\(x_i\)</span> that are “close” to <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[
\text{average}( \{ y_i : x_i \text{ equal to (or very close to) x} \} ).
\]</span></p>
<p>This is the main idea behind many nonparametric approaches. The details often just amount to very specifically defining what “close” means.</p>
<p>In the case of k-nearest neighbors we use</p>
<p><span class="math display">\[
\hat{\mu}_k(x) = \frac{1}{k} \sum_{ \{i \ : \ x_i \in \mathcal{N}_k(x, \mathcal{D}) \} } y_i
\]</span></p>
<p>as our estimate of the regression function at <span class="math inline">\(x\)</span>. While this looks complicated, it is actually very simple. Here, we are using an average of the <span class="math inline">\(y_i\)</span> values of for the <span class="math inline">\(k\)</span> nearest neighbors to <span class="math inline">\(x\)</span>.</p>
<p>The <span class="math inline">\(k\)</span> “nearest” neighbors are the <span class="math inline">\(k\)</span> data points <span class="math inline">\((x_i, y_i)\)</span> that have <span class="math inline">\(x_i\)</span> values that are nearest to <span class="math inline">\(x\)</span>. We can define “nearest” using any distance we like, but unless otherwise noted, we are referring to euclidean distance.<a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a> We are using the notation <span class="math inline">\(\{i \ : \ x_i \in \mathcal{N}_k(x, \mathcal{D}) \}\)</span> to define the <span class="math inline">\(k\)</span> observations that have <span class="math inline">\(x_i\)</span> values that are nearest to the value <span class="math inline">\(x\)</span> in a dataset <span class="math inline">\(\mathcal{D}\)</span>, in other words, the <span class="math inline">\(k\)</span> nearest neighbors.</p>
<p>The plots below begin to illustrate this idea.</p>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-8-1.png" width="864" style="display: block; margin: auto;" /></p>
<ul>
<li>In the left plot, to estimate the mean of <span class="math inline">\(Y\)</span> at <span class="math inline">\(x = -0.5\)</span> we use the three nearest neighbors, which are highlighted with green. Our estimate is the average of the <span class="math inline">\(y_i\)</span> values of these three points indicated by the black x.</li>
<li>In the middle plot, to estimate the mean of <span class="math inline">\(Y\)</span> at <span class="math inline">\(x = 0\)</span> we use the five nearest neighbors, which are highlighted with green. Our estimate is the average of the <span class="math inline">\(y_i\)</span> values of these five points indicated by the black x.</li>
<li>In the right plot, to estimate the mean of <span class="math inline">\(Y\)</span> at <span class="math inline">\(x = 0.75\)</span> we use the nine nearest neighbors, which are highlighted with green. Our estimate is the average of the <span class="math inline">\(y_i\)</span> values of these nine points indicated by the black x.</li>
</ul>
<p>You might begin to notice a bit of an issue here. We have to do a new calculation each time we want to estimate the regression function at a different value of <span class="math inline">\(x\)</span>! For this reason, k-nearest neighbors is often said to be “fast to train” and “slow to predict.” Training, is instant. You just memorize the data! Prediction involves finding the distance between the <span class="math inline">\(x\)</span> considered and all <span class="math inline">\(x_i\)</span> in the data!<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a></p>
<p>So, how then, do we choose the value of the <strong>tuning</strong> parameter <span class="math inline">\(k\)</span>? We <em><strong>validate</strong></em>!</p>
<p>First, let’s take a look at what happens with this data if we consider three different values of <span class="math inline">\(k\)</span>.</p>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-10-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>For each plot, the black dashed curve is the true mean function.</p>
<ul>
<li>In the left plot we use <span class="math inline">\(k = 25\)</span>. The red “curve” is the estimate of the mean function for each <span class="math inline">\(x\)</span> shown in the plot.</li>
<li>In the left plot we use <span class="math inline">\(k = 5\)</span>. The blue “curve” is the estimate of the mean function for each <span class="math inline">\(x\)</span> shown in the plot.</li>
<li>In the left plot we use <span class="math inline">\(k = 1\)</span>. The green “curve” is the estimate of the mean function for each <span class="math inline">\(x\)</span> shown in the plot.</li>
</ul>
<p>Some things to notice here:</p>
<ul>
<li>The left plot with <span class="math inline">\(k = 25\)</span> is performing poorly. The estimated “curve” does not “move” enough. This is an example of an <strong>inflexible</strong> model.</li>
<li>The right plot with <span class="math inline">\(k = 1\)</span> might not perform too well. The estimated “curve” seems to “move” too much. (Notice, that it goes through each point. We’ve fit to the noise.) This is an example of a <strong>flexible</strong> model.</li>
</ul>
<p>While the middle plot with <span class="math inline">\(k = 5\)</span> is not “perfect” it seems to roughly capture the “motion” of the true regression function. We can begin to see that if we generated new data, this estimated regression function would perform better than the other two.</p>
<p>But remember, in practice, we won’t know the true regression function, so we will need to determine how our model performs using only the available data!</p>
<p>This <span class="math inline">\(k\)</span>, the number of neighbors, is an example of a <strong>tuning parameter</strong>. Instead of being learned from the data, like model parameters such as the <span class="math inline">\(\beta\)</span> coefficients in linear regression, a tuning parameter tells us <em>how</em> to learn from data. It is user-specified. To determine the value of <span class="math inline">\(k\)</span> that should be used, many models are fit to the estimation data, then evaluated on the validation. Using the information from the validation data, a value of <span class="math inline">\(k\)</span> is chosen. (More on this in a bit.)</p>
<ul>
<li><strong>Model parameters</strong> are “learned” using the same data that was used to fit the model.</li>
<li><strong>Tuning parameters</strong> are “chosen” using data not used to fit the model.</li>
</ul>
<p>This tuning parameter <span class="math inline">\(k\)</span> also defines the <strong>flexibility</strong> of the model. In KNN, a small value of <span class="math inline">\(k\)</span> is a flexible model, while a large value of <span class="math inline">\(k\)</span> is inflexible.<a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a></p>
<p>Before moving to an example of tuning a KNN model, we will first introduce decision trees.</p>
</div>
<div id="decision-trees" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Decision Trees</h2>
<p><strong>Decision trees</strong> are similar to k-nearest neighbors but instead of looking for neighbors, decision trees create neighborhoods. We won’t explore the full details of trees, but just start to understand the basic concepts, as well as learn to fit them in R.</p>
<p>Neighborhoods are created via recursive binary partitions. In simpler terms, pick a feature and a possible cutoff value. Data that have a value less than the cutoff for the selected feature are in one neighborhood (the left) and data that have a value greater than the cutoff are in another (the right). Within these two neighborhoods, repeat this procedure until a stopping rule is satisfied. To make a prediction, check which neighborhood a new piece of data would belong to and predict the average of the <span class="math inline">\(y_i\)</span> values of data in that neighborhood.</p>
<p>With the data above, which has a single feature <span class="math inline">\(x\)</span>, consider three possible cutoffs: -0.5, 0.0, and 0.75.</p>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-12-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>For each plot, the black vertical line defines the neighborhoods. The green horizontal lines are the average of the <span class="math inline">\(y_i\)</span> values for the points in the left neighborhood. The red horizontal lines are the average of the <span class="math inline">\(y_i\)</span> values for the points in the right neighborhood.</p>
<p>What makes a cutoff good? Large differences in the average <span class="math inline">\(y_i\)</span> between the two neighborhoods. More formally we want to find a cutoff value that minimizes</p>
<p><span class="math display">\[
\sum_{i \in N_L} \left( y_i - \hat{\mu}_{N_L} \right) ^ 2 + \sum_{i \in N_R} \left(y_i - \hat{\mu}_{N_R} \right) ^ 2
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(N_L\)</span> are the data in the left neighborhood, that is <span class="math inline">\(x &lt; c\)</span></li>
<li><span class="math inline">\(N_R\)</span> are the data in the right neighborhood, that is <span class="math inline">\(x &gt; c\)</span></li>
<li><span class="math inline">\(\hat{\mu}_{N_L}\)</span> is the mean of the <span class="math inline">\(y_i\)</span> for data in the left neighborhood</li>
<li><span class="math inline">\(\hat{\mu}_{N_R}\)</span> is the mean of the <span class="math inline">\(y_i\)</span> for data in the right neighborhood</li>
</ul>
<p>This quantity is the sum of two sum of squared errors, one for the left neighborhood, and one for the right neighborhood.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
Cutoff
</th>
<th style="text-align:right;">
Total SSE
</th>
<th style="text-align:right;">
Left SSE
</th>
<th style="text-align:right;">
Right SSE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
-0.50
</td>
<td style="text-align:right;">
45.02
</td>
<td style="text-align:right;">
21.28
</td>
<td style="text-align:right;">
23.74
</td>
</tr>
<tr>
<td style="text-align:right;">
0.00
</td>
<td style="text-align:right;">
58.94
</td>
<td style="text-align:right;">
44.68
</td>
<td style="text-align:right;">
14.26
</td>
</tr>
<tr>
<td style="text-align:right;">
0.75
</td>
<td style="text-align:right;">
56.71
</td>
<td style="text-align:right;">
55.46
</td>
<td style="text-align:right;">
1.25
</td>
</tr>
</tbody>
</table>
<p>The table above summarizes the results of the three potential splits. We see that (of the splits considered, which are not exhaustive<a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a>) the split based on a cutoff of <span class="math inline">\(x = -0.50\)</span> creates the best partitioning of the space.</p>
<p>Now let’s consider building a full tree.</p>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In the plot above, the true regression function is the dashed black curve, and the solid orange curve is the estimated regression function using a decision tree. We see that there are two splits, which we can visualize as a tree.</p>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The above “tree”<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a> shows the splits that were made. It informs us of the variable used, the cutoff value, and some summary of the resulting neighborhood. In “tree” terminology the resulting neighborhoods are “terminal nodes” of the tree. In contrast, “internal nodes” are neighborhoods that are created, but then further split.</p>
<p>The “root node” is the neighborhood contains all observations, before any splitting, and can be seen at the top of the image above. We see that this node represents 100% of the data. The other number, 0.21, is the mean of the response variable, in this case, <span class="math inline">\(y_i\)</span>.</p>
<p>Looking at a terminal node, for example the bottom left node, we see that 23% of the data is in this node. The average value of the <span class="math inline">\(y_i\)</span> in this node is -1, which can be seen in the plot above.</p>
<p>We also see that the first split is based on the <span class="math inline">\(x\)</span> variable, and a cutoff of <span class="math inline">\(x = -0.52\)</span>. Note that because there is only one variable here, all splits are based on <span class="math inline">\(x\)</span>, but in the future, we will have multiple features that can be split and neighborhoods will no longer be one-dimensional. However, this is hard to plot.</p>
<p>Let’s build a bigger, more flexible tree.</p>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-20-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>There are two tuning parameters at play here which we will call by their names in R which we will see soon:</p>
<ul>
<li><code>cp</code> or the “complexity parameter” as it is called.<a href="#fn57" class="footnote-ref" id="fnref57"><sup>57</sup></a> This parameter determines which splits are accepted. A split must improve the performance of the tree by more than <code>cp</code> in order to be used. When we get to R, we will see that the default value is 0.1.</li>
<li><code>minsplit</code>, the minimum number of observations in a node (neighborhood) in order to consider splitting within a neighborhood.</li>
</ul>
<p>There are actually many more possible tuning parameters for trees, possibly differing depending on who wrote the code you’re using. We will limit discussion to these two.<a href="#fn58" class="footnote-ref" id="fnref58"><sup>58</sup></a> Note that they effect each other, and they effect other parameters which we are not discussing. The main takeaway should be how they effect model flexibility.</p>
<p>First let’s look at what happens for a fixed <code>minsplit</code> by variable <code>cp</code>.</p>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-22-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>We see that as <code>cp</code> <em>decreases</em>, model flexibility <strong>increases</strong>. We see more splits, because the increase in performance needed to accept a split is smaller as <code>cp</code> is reduced.</p>
<p>Now the reverse, fix <code>cp</code> and vary <code>minsplit</code>.</p>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-24-1.png" width="1152" style="display: block; margin: auto;" /></p>
<p>We see that as <code>minsplit</code> <em>decreases</em>, model flexibility <strong>increases</strong>. By allowing splits of neighborhoods with fewer observations, we obtain more splits, which results in a more flexible model.</p>
</div>
<div id="example-credit-card-data-1" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Example: Credit Card Data</h2>
<p>Let’s return to the credit card data from the previous chapter. While last time we used the data to inform a bit of analysis, this time we will simply use the dataset to illustrate some concepts.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="nonparametric-regression.html#cb103-1"></a><span class="co"># load data, coerce to tibble</span></span>
<span id="cb103-2"><a href="nonparametric-regression.html#cb103-2"></a>crdt =<span class="st"> </span><span class="kw">as_tibble</span>(ISLR<span class="op">::</span>Credit)</span></code></pre></div>
<p>Again, we are using the <code>Credit</code> data form the <code>ISLR</code> package. Note: <strong>this is not real data.</strong> It has been simulated.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="nonparametric-regression.html#cb104-1"></a><span class="co"># data prep</span></span>
<span id="cb104-2"><a href="nonparametric-regression.html#cb104-2"></a>crdt =<span class="st"> </span>crdt <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb104-3"><a href="nonparametric-regression.html#cb104-3"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>ID) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb104-4"><a href="nonparametric-regression.html#cb104-4"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>Rating, <span class="kw">everything</span>())</span></code></pre></div>
<p>We remove the <code>ID</code> variable as it should have no predictive power. We also move the <code>Rating</code> variable to the last column with a clever <code>dplyr</code> trick. This is in no way necessary, but is useful in creating some plots.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="nonparametric-regression.html#cb105-1"></a><span class="co"># test-train split</span></span>
<span id="cb105-2"><a href="nonparametric-regression.html#cb105-2"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb105-3"><a href="nonparametric-regression.html#cb105-3"></a>crdt_trn_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(crdt), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(crdt))</span>
<span id="cb105-4"><a href="nonparametric-regression.html#cb105-4"></a>crdt_trn =<span class="st"> </span>crdt[crdt_trn_idx, ]</span>
<span id="cb105-5"><a href="nonparametric-regression.html#cb105-5"></a>crdt_tst =<span class="st"> </span>crdt[<span class="op">-</span>crdt_trn_idx, ]</span></code></pre></div>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="nonparametric-regression.html#cb106-1"></a><span class="co"># estimation-validation split</span></span>
<span id="cb106-2"><a href="nonparametric-regression.html#cb106-2"></a>crdt_est_idx =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(crdt_trn), <span class="dt">size =</span> <span class="fl">0.8</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(crdt_trn))</span>
<span id="cb106-3"><a href="nonparametric-regression.html#cb106-3"></a>crdt_est =<span class="st"> </span>crdt_trn[crdt_est_idx, ]</span>
<span id="cb106-4"><a href="nonparametric-regression.html#cb106-4"></a>crdt_val =<span class="st"> </span>crdt_trn[<span class="op">-</span>crdt_est_idx, ]</span></code></pre></div>
<p>After train-test and estimation-validation splitting the data, we look at the train data.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="nonparametric-regression.html#cb107-1"></a><span class="co"># check data</span></span>
<span id="cb107-2"><a href="nonparametric-regression.html#cb107-2"></a><span class="kw">head</span>(crdt_trn, <span class="dt">n =</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## # A tibble: 10 x 11
##    Income Limit Cards   Age Education Gender  Student Married Ethnicity  Balance
##     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;        &lt;int&gt;
##  1  183.  13913     4    98        17 &quot; Male&quot; No      Yes     Caucasian     1999
##  2   35.7  2880     2    35        15 &quot; Male&quot; No      No      African A…       0
##  3  123.   8376     2    89        17 &quot; Male&quot; Yes     No      African A…    1259
##  4   20.8  2672     1    70        18 &quot;Femal… No      No      African A…       0
##  5   39.1  5565     4    48        18 &quot;Femal… No      Yes     Caucasian      772
##  6   36.5  3806     2    52        13 &quot; Male&quot; No      No      African A…     188
##  7   45.1  3762     3    80         8 &quot; Male&quot; No      Yes     Caucasian       70
##  8   43.5  2906     4    69        11 &quot; Male&quot; No      No      Caucasian        0
##  9   23.1  3476     2    50        15 &quot;Femal… No      No      Caucasian      209
## 10   53.2  4943     2    46        16 &quot;Femal… No      Yes     Asian          382
## # … with 1 more variable: Rating &lt;int&gt;</code></pre>
<p>Recall that we would like to predict the <code>Rating</code> variable. This time, let’s try to use only demographic information as predictors.<a href="#fn59" class="footnote-ref" id="fnref59"><sup>59</sup></a> In particular, let’s focus on <code>Age</code> (numeric), <code>Gender</code> (categorical), and <code>Student</code> (categorical).</p>
<p>Let’s fit KNN models with these features, and various values of <span class="math inline">\(k\)</span>. To do so, we use the <code>knnreg()</code> function from the <code>caret</code> package.<a href="#fn60" class="footnote-ref" id="fnref60"><sup>60</sup></a> Use <code>?knnreg</code> for documentation and details.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="nonparametric-regression.html#cb109-1"></a>crdt_knn_<span class="dv">01</span> =<span class="st"> </span><span class="kw">knnreg</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est, <span class="dt">k =</span> <span class="dv">1</span>)</span>
<span id="cb109-2"><a href="nonparametric-regression.html#cb109-2"></a>crdt_knn_<span class="dv">10</span> =<span class="st"> </span><span class="kw">knnreg</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est, <span class="dt">k =</span> <span class="dv">10</span>)</span>
<span id="cb109-3"><a href="nonparametric-regression.html#cb109-3"></a>crdt_knn_<span class="dv">25</span> =<span class="st"> </span><span class="kw">knnreg</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est, <span class="dt">k =</span> <span class="dv">25</span>)</span></code></pre></div>
<p>Here, we fit three models to the estimation data. We supply the variables that will be used as features as we would with <code>lm()</code>. We also specify how many neighbors to consider via the <code>k</code> argument.</p>
<p>But wait a second, what is the distance from non-student to student? From male to female? In other words, how does KNN handle categorical variables? It doesn’t! Like <code>lm()</code> it creates dummy variables under the hood.</p>
<p><strong>Note:</strong> To this point, and until we specify otherwise, we will always coerce categorical variables to be factor variables in R. We will then let modeling functions such as <code>lm()</code> or <code>knnreg()</code> deal with the creation of dummy variables internally.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="nonparametric-regression.html#cb110-1"></a><span class="kw">head</span>(crdt_knn_<span class="dv">10</span><span class="op">$</span>learn<span class="op">$</span>X)</span></code></pre></div>
<pre><code>##   Age GenderFemale StudentYes
## 1  30            0          0
## 2  25            0          0
## 3  44            0          0
## 4  73            1          0
## 5  44            0          1
## 6  71            0          0</code></pre>
<p>Once these dummy variables have been created, we have a numeric <span class="math inline">\(X\)</span> matrix, which makes distance calculations easy.<a href="#fn61" class="footnote-ref" id="fnref61"><sup>61</sup></a> For example, the distance between the 3rd and 4th observation here is 29.017.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="nonparametric-regression.html#cb112-1"></a><span class="kw">dist</span>(<span class="kw">head</span>(crdt_knn_<span class="dv">10</span><span class="op">$</span>learn<span class="op">$</span>X))</span></code></pre></div>
<pre><code>##           1         2         3         4         5
## 2  5.000000                                        
## 3 14.000000 19.000000                              
## 4 43.011626 48.010416 29.017236                    
## 5 14.035669 19.026298  1.000000 29.034462          
## 6 41.000000 46.000000 27.000000  2.236068 27.018512</code></pre>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="nonparametric-regression.html#cb114-1"></a><span class="kw">sqrt</span>(<span class="kw">sum</span>((crdt_knn_<span class="dv">10</span><span class="op">$</span>learn<span class="op">$</span>X[<span class="dv">3</span>, ] <span class="op">-</span><span class="st"> </span>crdt_knn_<span class="dv">10</span><span class="op">$</span>learn<span class="op">$</span>X[<span class="dv">4</span>, ]) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 29.01724</code></pre>
<p>What about interactions? Basically, you’d have to create them the same way as you do for linear models. We only mention this to contrast with trees in a bit.</p>
<p>OK, so of these three models, which one performs best? (Where for now, “best” is obtaining the lowest validation RMSE.)</p>
<p>First, note that we return to the <code>predict()</code> function as we did with <code>lm()</code>.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="nonparametric-regression.html#cb116-1"></a><span class="kw">predict</span>(crdt_knn_<span class="dv">10</span>, crdt_val[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, ])</span></code></pre></div>
<pre><code>## [1] 337.7857 356.0000 295.7692 360.8182 306.8000</code></pre>
<p>This uses the 10-NN (10 nearest neighbors) model to make predictions (estimate the regression function) given the first five observations of the validation data. <strong>Note:</strong> We did not name the second argument to <code>predict()</code>. Again, you’ve been warned.</p>
<p>Now that we know how to use the <code>predict()</code> function, let’s calculate the validation RMSE for each of these models.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="nonparametric-regression.html#cb118-1"></a>knn_mod_list =<span class="st"> </span><span class="kw">list</span>(</span>
<span id="cb118-2"><a href="nonparametric-regression.html#cb118-2"></a>  <span class="dt">crdt_knn_01 =</span> <span class="kw">knnreg</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est, <span class="dt">k =</span> <span class="dv">1</span>),</span>
<span id="cb118-3"><a href="nonparametric-regression.html#cb118-3"></a>  <span class="dt">crdt_knn_10 =</span> <span class="kw">knnreg</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est, <span class="dt">k =</span> <span class="dv">10</span>),</span>
<span id="cb118-4"><a href="nonparametric-regression.html#cb118-4"></a>  <span class="dt">crdt_knn_25 =</span> <span class="kw">knnreg</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est, <span class="dt">k =</span> <span class="dv">25</span>)</span>
<span id="cb118-5"><a href="nonparametric-regression.html#cb118-5"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="nonparametric-regression.html#cb119-1"></a>knn_val_pred =<span class="st"> </span><span class="kw">lapply</span>(knn_mod_list, predict, crdt_val)</span></code></pre></div>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="nonparametric-regression.html#cb120-1"></a>calc_rmse =<span class="st"> </span><span class="cf">function</span>(actual, predicted) {</span>
<span id="cb120-2"><a href="nonparametric-regression.html#cb120-2"></a>  <span class="kw">sqrt</span>(<span class="kw">mean</span>((actual <span class="op">-</span><span class="st"> </span>predicted) <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</span>
<span id="cb120-3"><a href="nonparametric-regression.html#cb120-3"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="nonparametric-regression.html#cb121-1"></a><span class="kw">sapply</span>(knn_val_pred, calc_rmse, crdt_val<span class="op">$</span>Rating)</span></code></pre></div>
<pre><code>## crdt_knn_01 crdt_knn_10 crdt_knn_25 
##    182.3469    149.2172    138.6527</code></pre>
<p>So, of these three values of <span class="math inline">\(k\)</span>, the model with <span class="math inline">\(k = 25\)</span> achieves the lowest validation RMSE.</p>
<p>This process, fitting a number of models with different values of the <em>tuning parameter</em>, in this case <span class="math inline">\(k\)</span>, and then finding the “best” tuning parameter value based on performance on the validation data is called <strong>tuning</strong>. In practice, we would likely consider more values of <span class="math inline">\(k\)</span>, but this should illustrate the point.</p>
<p>In the next chapter, we will discuss the details of model flexibility and model tuning, and how these concepts are tied together. However, even though we will present some theory behind this relationship, in practice, <strong>you must tune and validate your models</strong>. There is no theory that will inform you ahead of tuning and validation which model will be the best. By teaching you <em>how</em> to fit KNN models in R and how to calculate validation RMSE, you already have all a set of tools you can use to find a good model.</p>
<p>Let’s turn to decision trees which we will fit with the <code>rpart()</code> function from the <code>rpart</code> package. Use <code>?rpart</code> and <code>?rpart.control</code> for documentation and details. In particular, <code>?rpart.control</code> will detail the many tuning parameters of this implementation of decision tree models in R.</p>
<p>We’ll start by using default tuning parameters.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="nonparametric-regression.html#cb123-1"></a>crdt_tree =<span class="st"> </span><span class="kw">rpart</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est)</span></code></pre></div>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="nonparametric-regression.html#cb124-1"></a>crdt_tree</span></code></pre></div>
<pre><code>## n= 256 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 256 6667400.0 357.0781  
##    2) Age&lt; 82.5 242 5865419.0 349.3719  
##      4) Age&gt;=69.5 52 1040678.0 313.0385 *
##      5) Age&lt; 69.5 190 4737307.0 359.3158  
##       10) Age&lt; 38.5 55  700013.2 326.6000 *
##       11) Age&gt;=38.5 135 3954443.0 372.6444  
##         22) Student=Yes 14  180764.4 297.7857 *
##         23) Student=No 121 3686148.0 381.3058  
##           46) Age&gt;=50.5 64 1881299.0 359.2344  
##             92) Age&lt; 53.5 9   48528.0 278.3333 *
##             93) Age&gt;=53.5 55 1764228.0 372.4727 *
##           47) Age&lt; 50.5 57 1738665.0 406.0877 *
##    3) Age&gt;=82.5 14  539190.9 490.2857 *</code></pre>
<p>Above we see the resulting tree printed, however, this is difficult to read. Instead, we use the <code>rpart.plot()</code> function from the <code>rpart.plot</code> package to better visualize the tree.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="nonparametric-regression.html#cb126-1"></a><span class="kw">rpart.plot</span>(crdt_tree)</span></code></pre></div>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-41-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>At each split, the variable used to split is listed together with a condition. If the condition is true for a data point, send it to the left neighborhood. Although the <code>Gender</code> available for creating splits, we only see splits based on <code>Age</code> and <code>Student</code>. This hints at the relative importance of these variables for prediction. More on this much later.</p>
<p>Categorical variables are split based on potential categories! This is <em>excellent</em>. This means that trees naturally handle categorical features without needing to convert to numeric under the hood. We see a split that puts students into one neighborhood, and non-students into another.</p>
<p>Notice that the splits happen in order. So for example, the third terminal node (with an average rating of 298) is based on splits of:</p>
<ul>
<li><code>Age &lt; 83</code></li>
<li><code>Age &lt; 70</code></li>
<li><code>Age &gt; 39</code></li>
<li><code>Student = Yes</code></li>
</ul>
<p>In other words, individuals in this terminal node are students who are between the ages of 39 and 70. (Only 5% of the data is represented here.) This is basically an interaction between <code>Age</code> and <code>Student</code> without any need to directly specify it! What a great feature of trees.</p>
<p>To recap:</p>
<ul>
<li>Trees do not make assumptions about the form of the regression function.</li>
<li>Trees automatically handle categorical features.</li>
<li>Trees naturally incorporate interaction.</li>
</ul>
<p>Now let’s fit another tree that is more flexible by relaxing some tuning parameters. Recall that by default, <code>cp = 0.1</code> and <code>minsplit = 20</code>.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="nonparametric-regression.html#cb127-1"></a>crdt_tree_big =<span class="st"> </span><span class="kw">rpart</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est, </span>
<span id="cb127-2"><a href="nonparametric-regression.html#cb127-2"></a>                      <span class="dt">cp =</span> <span class="fl">0.0</span>, <span class="dt">minsplit =</span> <span class="dv">20</span>)</span></code></pre></div>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="nonparametric-regression.html#cb128-1"></a><span class="kw">rpart.plot</span>(crdt_tree_big)</span></code></pre></div>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-43-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>To make the tree even bigger, we could reduce <code>minsplit</code>, but in practice we mostly consider the <code>cp</code> parameter.<a href="#fn62" class="footnote-ref" id="fnref62"><sup>62</sup></a> Since <code>minsplit</code> has been kept the same, but <code>cp</code> was reduced, we see the same splits as the smaller tree, but many additional splits.</p>
<p>Now let’s fit a bunch of trees, with different values of <code>cp</code>, for tuning.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="nonparametric-regression.html#cb129-1"></a>tree_mod_list =<span class="st"> </span><span class="kw">list</span>(</span>
<span id="cb129-2"><a href="nonparametric-regression.html#cb129-2"></a>  <span class="dt">crdt_tree_0000 =</span> <span class="kw">rpart</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est, <span class="dt">cp =</span> <span class="fl">0.000</span>),</span>
<span id="cb129-3"><a href="nonparametric-regression.html#cb129-3"></a>  <span class="dt">crdt_tree_0001 =</span> <span class="kw">rpart</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est, <span class="dt">cp =</span> <span class="fl">0.001</span>),</span>
<span id="cb129-4"><a href="nonparametric-regression.html#cb129-4"></a>  <span class="dt">crdt_tree_0010 =</span> <span class="kw">rpart</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est, <span class="dt">cp =</span> <span class="fl">0.010</span>),</span>
<span id="cb129-5"><a href="nonparametric-regression.html#cb129-5"></a>  <span class="dt">crdt_tree_0100 =</span> <span class="kw">rpart</span>(Rating <span class="op">~</span><span class="st"> </span>Age <span class="op">+</span><span class="st"> </span>Gender <span class="op">+</span><span class="st"> </span>Student, <span class="dt">data =</span> crdt_est, <span class="dt">cp =</span> <span class="fl">0.100</span>)</span>
<span id="cb129-6"><a href="nonparametric-regression.html#cb129-6"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="nonparametric-regression.html#cb130-1"></a>tree_val_pred =<span class="st"> </span><span class="kw">lapply</span>(tree_mod_list, predict, crdt_val)</span></code></pre></div>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="nonparametric-regression.html#cb131-1"></a><span class="kw">sapply</span>(tree_val_pred, calc_rmse, crdt_val<span class="op">$</span>Rating)</span></code></pre></div>
<pre><code>## crdt_tree_0000 crdt_tree_0001 crdt_tree_0010 crdt_tree_0100 
##       156.3527       155.4262       151.9081       140.0806</code></pre>
<p>Here we see the least flexible model, with <code>cp = 0.100</code>, performs best.</p>
<p>Note that by only using these three features, we are severely limiting our models performance. Let’s quickly assess using all available predictors.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="nonparametric-regression.html#cb133-1"></a>crdt_tree_all =<span class="st"> </span><span class="kw">rpart</span>(Rating <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> crdt_est)</span></code></pre></div>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="nonparametric-regression.html#cb134-1"></a><span class="kw">rpart.plot</span>(crdt_tree_all)</span></code></pre></div>
<p><img src="nonparametric-regression_files/figure-html/unnamed-chunk-48-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Notice that this model <strong>only</strong> splits based on <code>Limit</code> despite using all features. This should be a big hint about which variables are useful for prediction.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="nonparametric-regression.html#cb135-1"></a><span class="kw">calc_rmse</span>(</span>
<span id="cb135-2"><a href="nonparametric-regression.html#cb135-2"></a>  <span class="dt">actual =</span> crdt_val<span class="op">$</span>Rating,</span>
<span id="cb135-3"><a href="nonparametric-regression.html#cb135-3"></a>  <span class="dt">predicted =</span> <span class="kw">predict</span>(crdt_tree_all, crdt_val)</span>
<span id="cb135-4"><a href="nonparametric-regression.html#cb135-4"></a>)</span></code></pre></div>
<pre><code>## [1] 28.8498</code></pre>
<p>This model performs much better. You should try something similar with the KNN models above. Also, consider comparing this result to results from last chapter using linear models.</p>
<p>Notice that we’ve been using that trusty <code>predict()</code> function here again.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="nonparametric-regression.html#cb137-1"></a><span class="kw">predict</span>(crdt_tree_all, crdt_val[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, ])</span></code></pre></div>
<pre><code>##        1        2        3        4        5 
## 292.8182 467.5152 467.5152 467.5152 772.4000</code></pre>
<p>What does this code do? It estimates the mean <code>Rating</code> given the feature information (the “x” values) from the first five observations from the validation data using a decision tree model with default tuning parameters. Hopefully a theme is emerging.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="51">
<li id="fn51"><p>We chose to start with linear regression because most students in STAT 432 should already be familiar.<a href="nonparametric-regression.html#fnref51" class="footnote-back">↩︎</a></p></li>
<li id="fn52"><p>The usual distance when you hear distance. That is, unless you drive a <a href="https://en.wikipedia.org/wiki/Taxicab_geometry">taxicab</a>.<a href="nonparametric-regression.html#fnref52" class="footnote-back">↩︎</a></p></li>
<li id="fn53"><p>For this reason, KNN is often not used in practice, but it is very useful learning tool.<a href="nonparametric-regression.html#fnref53" class="footnote-back">↩︎</a></p></li>
<li id="fn54"><p>Many texts use the term complex instead of flexible. We feel this is confusing as complex is often associated with difficult. KNN with <span class="math inline">\(k = 1\)</span> is actually a very simple model to understand, but it is very flexible as defined here.<a href="nonparametric-regression.html#fnref54" class="footnote-back">↩︎</a></p></li>
<li id="fn55"><p>To exhaust all possible splits of a variable, we would need to consider the midpoint between each of the order statistics of the variable. To exhaust all possible splits, we would need to do this for each of the feature variables.<a href="nonparametric-regression.html#fnref55" class="footnote-back">↩︎</a></p></li>
<li id="fn56"><p>It’s really an upside tree isn’t it?<a href="nonparametric-regression.html#fnref56" class="footnote-back">↩︎</a></p></li>
<li id="fn57"><p>Flexibility parameter would be a better name.<a href="nonparametric-regression.html#fnref57" class="footnote-back">↩︎</a></p></li>
<li id="fn58"><p>The <code>rpart</code> function in R would allow us to use others, but we will always just leave their values as the default values.<a href="nonparametric-regression.html#fnref58" class="footnote-back">↩︎</a></p></li>
<li id="fn59"><p>There is a question of whether or not we <em>should</em> use these variables. For example, should men and women be given different ratings when all other variables are the same? Using the <code>Gender</code> variable allows for this to happen. Also, you might think, just don’t use the <code>Gender</code> variable. Unfortunately, it’s not that easy. There is an increasingly popular field of study centered around these ideas called <a href="https://en.wikipedia.org/wiki/Fairness_(machine_learning)">machine learning <strong>fairness</strong></a>.<a href="nonparametric-regression.html#fnref59" class="footnote-back">↩︎</a></p></li>
<li id="fn60"><p>There are many other KNN functions in R. However, the operation and syntax of <code>knnreg()</code> better matches other functions we will use in this course.<a href="nonparametric-regression.html#fnref60" class="footnote-back">↩︎</a></p></li>
<li id="fn61"><p>Wait. Doesn’t this sort of create an arbitrary distance between the categories? Why <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> and not <span class="math inline">\(-42\)</span> and <span class="math inline">\(51\)</span>? Good question. This hints at the notion of pre-processing. We’re going to hold off on this for now, but, often when performing k-nearest neighbors, you should try scaling all of the features to have mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1\)</span>.<a href="nonparametric-regression.html#fnref61" class="footnote-back">↩︎</a></p></li>
<li id="fn62"><p>If you are taking STAT 432, we will occasionally modify the <code>minsplit</code> parameter on quizzes.<a href="nonparametric-regression.html#fnref62" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bias-variance-tradeoff.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/daviddalpiaz/bsl/edit/master/nonparametric-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
